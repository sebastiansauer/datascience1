@Book{xie2015,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  note = {ISBN 978-1498716963},
  url = {http://yihui.name/knitr/},
}




@book{rhys,
	address = {Shelter Island, {NY}},
	title = {Machine Learning with R, the tidyverse, and mlr},
	publisher = {Manning publications},
	author = {Hefin Rhys},
	year = {2020},
	note = {{OCLC}: on1121083327},
	keywords = {Artificial intelligence, Machine learning, R (Computer program language)}
}



@book{silge_tidy_2022,
	title = {Tidy {Modeling} with {R}},
	url = {https://www.tmwr.org/},
	abstract = {The tidymodels framework is a collection of R packages for modeling and machine learning using tidyverse principles. This book provides a thorough introduction to how to use tidymodels, and an outline of good methodology and statistical practice for phases of the modeling process.},
	urldate = {2022-02-11},
	author = {Silge, Julia and Kuhn, Max},
	year = {2022},
	file = {Snapshot:/Users/sebastiansaueruser/Zotero/storage/598T9IJV/www.tmwr.org.html:text/html}
}





@book{islr,
	location = {New York},
	edition = {Second edition},
	title = {An introduction to statistical learning: with applications in R},
	isbn = {978-1-07-161418-1 978-1-07-161417-4},
	url = {https://link.springer.com/book/10.1007/978-1-0716-1418-1},
	series = {Springer texts in statistics},
	shorttitle = {An introduction to statistical learning},
	pagetotal = {607},
	publisher = {Springer},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	date = {2021}
}



@book{baumer_modern_2017,
	location = {Boca Raton, Florida},
	title = {Modern Data Science with R (Chapman \& Hall/{CRC} Texts in Statistical Science)},
	isbn = {1-4987-2448-5},
	publisher = {Chapman and Hall/{CRC}},
	author = {Baumer, Benjamin S. and Kaplan, Daniel T. and Horton, Nicholas J.},
	date = {2017}
}



@book{spurzem_vw_2017,
	title = {{VW} 1303 von Wiking in 1:87},
	rights = {{CC} {BY}-{SA} 2.0 de},
	url = {https://de.wikipedia.org/wiki/Modellautomobil#/media/File:Wiking-Modell_VW_1303_(um_1975).JPG},
	abstract = {https://commons.wikimedia.org/wiki/File:Wiking-Modell\_VW\_1303\_(um\_1975).{JPG}},
	author = {Spurzem, Lothar},
	urldate = {2017-03-20},
	date = {2017-03}
}



@book{modar,
	location = {Wiesbaden},
	edition = {1. Auflage 2019},
	title = {Moderne Datenanalyse mit R: Daten einlesen, aufbereiten, visualisieren und modellieren},
	rights = {All rights reserved},
	isbn = {978-3-658-21587-3 978-3-658-21586-6},
	url = {https://www.springer.com/de/book/9783658215866},
	series = {{FOM}-Edition},
	shorttitle = {Moderne Datenanalyse mit R},
	publisher = {Springer},
	author = {Sauer, Sebastian},
	date = {2019},
	keywords = {modar}
}




@book{islrtidy,
	title = {{ISLR} tidymodels Labs},
	rights = {{MIT}},
	url = {https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/index.html},
	author = {Hvitfeldt, Emil},
	date = {2022}
}



@book{r4ds,
	title = {R for Data Science: Visualize, Model, Transform, Tidy, and Import Data},
	isbn = {1-4919-1039-9},
	url = {https://r4ds.had.co.nz/index.html},
	publisher = {O'Reilly Media},
	author = {Wickham, Hadley and Grolemund, Garrett},
	date = {2016}
}



@book{timbers_data_2022,
	location = {Boca Raton},
	edition = {First edition},
	title = {Data science: an introduction},
	isbn = {978-0-367-53217-8 978-0-367-52468-5},
	series = {Statistics},
	shorttitle = {Data science},
	abstract = {"Data Science: An Introduction focuses on using the R programming language in Jupyter notebooks to perform basic data manipulation and cleaning, create effective visualizations, and extract insights from data using supervised predictive models. Based on sound educational research and active learning principles, the book uses a modern approach to the R programming language and accompanying sheets for self-directed learning this book will leave students well-prepared for data science projects. Data Science: An Introduction focuses on workflows and communication strategies that are clear, reproducible, and shareable. Aimed at first year undergraduates with only minimal prior knowledge of mathematics and programming this book is suitable for students across many disciplines. All source code is available online as a {GitHub} repository, demonstrating the use of good reproducible and clear project workflows and is also accompanied by autograded Jupyter worksheets, providing the reader with guided interactive instruction"--},
	publisher = {{CRC} Press},
	author = {Timbers, Tiffany-Anne and Campbell, Trevor and Lee, Melissa},
	date = {2022},
	keywords = {Data processing, Mathematical statistics, Quantitative research, R (Computer program language), Textbooks}
}


@book{fattails,
	title = {The statistical consequences of fat tails, papers and commentaries},
	url = {https://nassimtaleb.org/2020/01/final-version-fat-tails/},
	publisher = {Monograph},
	author = {Taleb, Nassim Nicholas},
	date = {2019}
}




@book{kuhn,
	title = {Applied predictive modeling},
	volume = {26},
	publisher = {Springer},
	author = {Kuhn, Max and Johnson, Kjell},
	date = {2013}
}




@inproceedings{chen_xgboost_2016,
	location = {New York, {NY}, {USA}},
	title = {{XGBoost}: A Scalable Tree Boosting System},
	isbn = {978-1-4503-4232-2},
	url = {https://doi.org/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	series = {{KDD} '16},
	shorttitle = {{XGBoost}},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called {XGBoost}, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, {XGBoost} scales beyond billions of examples using far fewer resources than existing systems.},
	pages = {785--794},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {Association for Computing Machinery},
	author = {Chen, Tianqi and Guestrin, Carlos},
	urldate = {2022-05-12},
	date = {2016-08-13},
	keywords = {large-scale machine learning},
	file = {Chen&Guestrin_(2016)XGBoost - A Scalable Tree Boosting System.pdf:/Users/sebastiansaueruser/Literatur/Chen&Guestrin_(2016)XGBoost - A Scalable Tree Boosting System.pdf:application/pdf}
}




@article{friedman_greedy_2001,
	title = {Greedy function approximation: A gradient boosting machine.},
	doi = {10.1214/AOS/1013203451},
	shorttitle = {Greedy function approximation},
	abstract = {A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion, and specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such {TreeBoost} models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
	author = {Friedman, J.},
	date = {2001},
	file = {Friedman_(2001)Greedy function approximation - A gradient boosting machine.pdf:/Users/sebastiansaueruser/Literatur/Friedman_(2001)Greedy function approximation - A gradient boosting machine.pdf:application/pdf;Volltext:/Users/sebastiansaueruser/Zotero/storage/RHAN54TZ/Friedman - 2001 - Greedy function approximation A gradient boosting.pdf:application/pdf}
}

