# Ensemble Lerner




```{r global-knitr-options, include=FALSE}
  knitr::opts_chunk$set(
  fig.pos = 'H',
  fig.asp = 0.618,
  fig.align='center',
  fig.width = 5,
  out.width = "100%",
  fig.cap = "", 
  dpi = 300,
  # tidy = TRUE,
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  fig.show = "hold")
```




## Lernsteuerung

```{r chapter-start-sections, echo = FALSE, results = "asis"}
source("funs/chapter-start-sections.R")
chapter_start_sections(title = "Ensemble-Lerner")
```








## Vorbereitung


In diesem Kapitel werden folgende R-Pakete ben√∂tigt:

```{r echo = TRUE}
library(tidymodels)
library(tictoc)  # Zeitmessung
```


## Hinweise zur Literatur


Die folgenden Ausf√ºhrungen basieren prim√§r auf @rhys, aber auch auf @islr und (weniger) @kuhn.



## Wir brauchen einen Wald

Ein Pluspunkt von Entscheidungsb√§umen ist ihre gute Interpretierbarkeit.
Man k√∂nnte behaupten, dass B√§ume eine typische Art des menschlichen Entscheidungsverhalten
nachahmen: "Wenn A, dann tue B, ansonsten tue C" (etc.).
Allerdings: Einzelne Entscheidungsb√§ume haben oft keine so gute Prognosegenauigkeit. 
Der oder zumindest ein Grund ist, dass sie (zwar wenig Bias aber) viel Varianz aufweisen.
Das sieht man z.B. daran, dass die Vorhersagegenauigkeit stark schwankt,
w√§hlt man eine andere Aufteilung von Train- vs. Test-Sample.
Anders gesagt: B√§ume overfitten ziemlich schnell.
Und obwohl das No-Free-Lunch-Theorem zu den Grundfesten des maschinellen Lernens
(oder zu allem wissenschaftlichen Wissen) geh√∂rt,
kann man festhalten, dass sog. *Ensemble-Lernen* fast immer besser sind 
als einzelne Baummodelle.
Kurz gesagt: Wir brauchen einen Wald: üå≥üå≥üå≥^[√úbrigens geh√∂rt zu den weiteren Vorteilen von B√§umen, dass sie die Temperatur absenken; zu Zeiten von Hitzewellen k√∂nnte das praktisch sein. Ansonsten erzeugen sie aber nur Luft und haben auch sonst kaum erkennbaren Nutzen.]




## Was ist ein Ensemble-Lerner?

Ensemble-Lerner kombinieren mehrere schwache Lerner zu einem starken Lerner.
Das Paradebeispiel sind baumbasierte Modelle; 
darauf wird sich die folgende Ausf√ºhrung auch begrenzen.
Aber theoretisch kann man jede Art von Lerner kombinieren.
Bei numerischer Pr√§diktion wird bei Ensemble-Lerner zumeist der Mittelwert als Optmierungskriterium
herangezogen; bei Klassifikation (nominaler Pr√§diktion) hingegen die modale Klasse (also die h√§ufigste).
Warum hilft es, mehrere Modelle (Lerner) zu einem zu aggregieren?
Die Antwort lautet, dass die Streuung der Mittelwerte sinkt,
wenn die Stichprobengr√∂√üe steigt.
Zieht man Stichproben der Gr√∂√üe 1, werden die Mittelwerte stark variieren,
aber bei gr√∂√üeren Stichproben (z.B. Gr√∂√üe 100) deutlich weniger^[bei Fat-Tails-Variablen muss man diese Aussage einschr√§nken].
Die Streuung der Mittelwerte in den Stichproben nennt man bekanntlich *Standardefehler* (se).
Den se des Mittelwerts ($se_M$) f√ºr eine normalverteilte Variable $X \sim \mathcal{N}(\mu, \sigma)$ gilt: 
$se_{M} = \sigma / \sqrt(n)$, wobei $\sigma$ die SD der Verteilung und $\mu$ den Erwartungswert ("Mittelwert") meint,
und $n$ ist die Stichprobengr√∂√üe.


:::: {.infobox .quote}
Je gr√∂√üer die Stichprobe, desto kleiner die Varianz des Sch√§tzers (ceteris paribus).
Anders gesagt: Gr√∂√üere Stichproben sch√§tzen genauer als kleine Stichproben.
:::


Aus diesem Grund bietet es sich an,
schwache Lerner mit viel Varianz zu kombinieren,
da die Varianz so verringert wird.


## Bagging

### Bootstrapping

Das erste baumbasierte Modell, was vorgestellt werden soll,
basiert auf sog. *Bootstrapping*, ein Standardverfahren in der Statistik [@islr].

Bootstrapping ist eine Nachahmung f√ºr folgende Idee:
H√§tte man viele Stichproben aus der relevanten Verteilung,
so k√∂nnte man z.B. die Genauigkeit eines Modells $\hat{f}_{\bar{X}}$ zur Sch√§tzung des Erwartungswertes $\mu$ einfach dadurch bestimmen,
indem man *se* berechnet, also die Streuung der Mitterwerte $\bar{X}$ berechnet.
Au√üerdem gilt, dass die Pr√§zision der Sch√§tzung des Erwartungswerts steigt mit steigendem Stichprobenumfang $n$.
Wir k√∂nnten also f√ºr jede der $B$ Stichproben, $b=1,\ldots, B$, ein (Baum-)Modell berechnen
und dann deren Vorhersagen aggregieren (zum Mittelwert oder Modalwert).
Das kann man formal so darstellen [@islr]:


$$\hat{f}_{\bar{X}} = \frac{1}{B}\sum_{b=1}^{B}\hat{f}^b$$


Mit diesem Vorgehen kann die Varianz des Modells $\hat{f}_{\bar{X}}$ verringert werden;
die Vorhersagegenauigkeit steigt.

Leider haben wir in der Regel nicht viele ($B$) Datens√§tze.

Daher "bauen" wir uns aus dem einzelnen Datensatz, der uns zur Verf√ºgung steht,
viele Datens√§tze.
Das h√∂rt sich nach "too good to be true" an^[Wenn es einen No-Free-Lunch-Satz gibt, m√ºsste es auch einen Too-Good-to-be-True-Satz geben, den wir hiermit postulieren.]
Weil es sich unglaubw√ºrdig anh√∂rt, nennt man das entsprechende Verfahren (gleich kommt es!) auch "M√ºnchhausen-Methode",
nach dem ber√ºhmten L√ºbgenbaron.
Die Amerikaner ziehen sich √ºbrigens nicht am Schopf aus dem Sumpf, sondern
mit den Stiefelschlaufen (die Cowboys wieder),
daher spricht man im Amerikanischen auch von der "Boostrapping-Methode".

Diese "Pseudo-Stichproben" oder "Bootstrapping-Stichproben" sind aber recht einfach zu gewinnen..
Gegeben sei Stichprobe der Gr√∂√üe $n$:


1. Ziehe mit Zur√ºcklegen (ZmZ) aus der Stichprobe $n$ Beobachtungen
2. Fertig ist die Bootstrapping-Stichprobe.


Abb. \@ref(fig:zmz) verdeutlicht das Prinzip des ZMZ, d.h. des Bootstrappings.
Wie man sieht, sind die Bootstrap-Stichproben (rechts) vom gleichen Umfang $n$ 
wie die Originalstichprobe (links).
Allerdins kommen nicht alle F√§lle (in der Regel) in den "Boostrap-Beutel" (in bag),
sondern einige F√§lle werden oft mehrfach gezogen, so dass
einige F√§lle nicht gezogen werden (out of bag).

```{r zmz, fig.cap = "Bootstrapping: Der Topf links symbolisiert die Original-Stichprobe, aus der wir hier mehrere ZMZ-Stichproben ziehen (Rechts), dargestellt mit 'in bag'"}
knitr::include_graphics("img/zmz.png")
```


Man kann zeigen, dass ca. 2/3 der F√§lle gezogen werden,
bzw. ca. 1/3 nicht gezogen werden. Die nicht gezogenen F√§lle nennt man auch *out of bag* (OOB).

F√ºr die Entwicklung des Bootstrapping wurde der Autor, Bradley Efron, im Jahr 2018
mit dem internationalen Preis f√ºr Statistik [ausgezeichnet](https://www.amstat.org/news-listing/2021/10/08/international-prize-in-statistics-awarded-to-bradley-efron);


>   ‚ÄúWhile statistics offers no magic pill for quantitative scientific investigations, the bootstrap is the best statistical pain reliever ever produced,‚Äù says Xiao-Li Meng, Whipple V. N. Jones Professor of Statistics at Harvard University.‚Äú


## Bagging-Algorithmus

Bagging, die Kurzform f√ºr *B*ootstrap-*Agg*regation ist wenig mehr als die Umsetzung des Boostrappings.





Der Algorithmus von Bagging kann so beschrieben werden:


1. W√§hle $B$, die Anzahl der Boostrap-Stichproben und damit auch Anzahl der Submodelle (Lerner)
2. Ziehe $B$ Boostrap-Stichproben
3. Berechne das Modell $\hat{f}^{*b}$ f√ºr jede der $B$ Stichproben (typischerweise ein einfacher Baum)
4. Schicke die Test-Daten durch jedes Sub-Modell
5. Aggregiere ihre Vorhersage zu einem Wert (Modus bzw. Mittelwert) pro Fall aus dem Test-Sample, zu $\hat{f}_{\text{bag}}$


Anders gesagt:

$$\hat{f}_{\text{bag}} = \frac{1}{B}\sum_{b=1}^{B}\hat{f}^{*b}$$




Die Anzahl der B√§ume (allgemeiner: Submodelle) $B$ ist h√§ufig im oberen drei- oder niedrigem vierstelligen 
Bereich, z.B. $B=1000$. 
Eine gute Nachricht ist, dass Bagging nicht √ºberanpasst, wenn $B$ gro√ü wird.



### Variablenrelevanz


Man kann die Relevanz der Pr√§diktoren in einem Bagging-Modell auf mehrere Arten sch√§tzen.
Ein Weg (bei numerischer Pr√§diktion) ist, dass man die RSS-Verringerung, die durch Aufteilung anhand eines Pr√§diktors
erzeugt wird, mittelt √ºber alle beteiligten B√§ume (Modelle).
Bei Klassifikation kann man die analog die Reduktion des Gini-Wertes √ºber alle B√§ume mitteln
und als Sch√§tzwert f√ºr die Relevanz des Pr√§diktors heranziehen.



### Out of Bag Vorhersagen

Da nicht alle F√§lle der Stichprobe in das Modell einflie√üen (sondern nur ca. 2/3),
kann der Rest der F√§lle zur Vorhersage genutzt werden.
Bagging erzeugt sozusagen innerhalb der Stichprobe selbst√§ndig ein Train- und ein Test-Sample.
Man spricht von *Out-of-Bag-Sch√§tzung* (OOB-Sch√§tzung).
Der OOB-Fehler (z.B. MSE bei numerischen Modellen und Genauigkeit bei nominalen)
ist eine valide Sch√§tzung des typischen Test-Sample-Fehlers.

Hat man aber Tuningparameter, so wird man dennoch auf die typische Train-Test-Aufteilung
zur√ºckgreifen, um Overfitting durch das Ausprobieren der Tuning-Kandidaten zu vermeiden
 (was sonst zu Zufallstreffern f√ºhren w√ºrde bei gen√ºgend vielen Modellkandidaten).
 
 







<!-- ## Aufgaben und Vertiefung -->




```{r render-outline-vertiefung-aufgaben, results = "asis", echo = FALSE, message = FALSE}

  render_section(course_dates_file,
                 content_file,
                 i = NULL,
                 title = "Ensemble-Lerner",
                 name = "Aufgaben",
                 header_level = 1)


render_section(course_dates_file,
               content_file,
               i = NULL, 
               title = "Ensemble-Lerner",
               name = "Vertiefung",
               header_level = 1)
```
