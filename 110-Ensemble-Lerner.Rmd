# Ensemble Lerner




```{r global-knitr-options, include=FALSE}
  knitr::opts_chunk$set(
  fig.pos = 'H',
  fig.asp = 0.618,
  fig.align='center',
  fig.width = 5,
  out.width = "100%",
  fig.cap = "", 
  dpi = 300,
  # tidy = TRUE,
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  fig.show = "hold")
```




## Lernsteuerung

```{r chapter-start-sections, echo = FALSE, results = "asis"}
source("funs/chapter-start-sections.R")
chapter_start_sections(title = "Ensemble-Lerner")
```








## Vorbereitung


In diesem Kapitel werden folgende R-Pakete ben√∂tigt:

```{r echo = TRUE}
library(tidymodels)
library(tictoc)  # Zeitmessung
library(vip)  # Variable importance plot
```


## Hinweise zur Literatur


Die folgenden Ausf√ºhrungen basieren prim√§r auf @rhys, aber auch auf @islr und (weniger) @kuhn.



## Wir brauchen einen Wald

Ein Pluspunkt von Entscheidungsb√§umen ist ihre gute Interpretierbarkeit.
Man k√∂nnte behaupten, dass B√§ume eine typische Art des menschlichen Entscheidungsverhalten
nachahmen: "Wenn A, dann tue B, ansonsten tue C" (etc.).
Allerdings: Einzelne Entscheidungsb√§ume haben oft keine so gute Prognosegenauigkeit. 
Der oder zumindest ein Grund ist, dass sie (zwar wenig Bias aber) viel Varianz aufweisen.
Das sieht man z.B. daran, dass die Vorhersagegenauigkeit stark schwankt,
w√§hlt man eine andere Aufteilung von Train- vs. Test-Sample.
Anders gesagt: B√§ume overfitten ziemlich schnell.
Und obwohl das No-Free-Lunch-Theorem zu den Grundfesten des maschinellen Lernens
(oder zu allem wissenschaftlichen Wissen) geh√∂rt,
kann man festhalten, dass sog. *Ensemble-Lernen* fast immer besser sind 
als einzelne Baummodelle.
Kurz gesagt: Wir brauchen einen Wald: üå≥üå≥üå≥^[√úbrigens geh√∂rt zu den weiteren Vorteilen von B√§umen, dass sie die Temperatur absenken; zu Zeiten von Hitzewellen k√∂nnte das praktisch sein. Ansonsten erzeugen sie aber nur Luft und haben auch sonst kaum erkennbaren Nutzen.]




## Was ist ein Ensemble-Lerner?

Ensemble-Lerner kombinieren mehrere schwache Lerner zu einem starken Lerner.
Das Paradebeispiel sind baumbasierte Modelle; 
darauf wird sich die folgende Ausf√ºhrung auch begrenzen.
Aber theoretisch kann man jede Art von Lerner kombinieren.
Bei numerischer Pr√§diktion wird bei Ensemble-Lerner zumeist der Mittelwert als Optmierungskriterium
herangezogen; bei Klassifikation (nominaler Pr√§diktion) hingegen die modale Klasse (also die h√§ufigste).
Warum hilft es, mehrere Modelle (Lerner) zu einem zu aggregieren?
Die Antwort lautet, dass die Streuung der Mittelwerte sinkt,
wenn die Stichprobengr√∂√üe steigt.
Zieht man Stichproben der Gr√∂√üe 1, werden die Mittelwerte stark variieren,
aber bei gr√∂√üeren Stichproben (z.B. Gr√∂√üe 100) deutlich weniger^[bei Fat-Tails-Variablen muss man diese Aussage einschr√§nken].
Die Streuung der Mittelwerte in den Stichproben nennt man bekanntlich *Standardefehler* (se).
Den se des Mittelwerts ($se_M$) f√ºr eine normalverteilte Variable $X \sim \mathcal{N}(\mu, \sigma)$ gilt: 
$se_{M} = \sigma / \sqrt(n)$, wobei $\sigma$ die SD der Verteilung und $\mu$ den Erwartungswert ("Mittelwert") meint,
und $n$ ist die Stichprobengr√∂√üe.


:::: {.infobox .quote}
Je gr√∂√üer die Stichprobe, desto kleiner die Varianz des Sch√§tzers (ceteris paribus).
Anders gesagt: Gr√∂√üere Stichproben sch√§tzen genauer als kleine Stichproben.
:::


Aus diesem Grund bietet es sich an,
schwache Lerner mit viel Varianz zu kombinieren,
da die Varianz so verringert wird.


## Bagging

### Bootstrapping

Das erste baumbasierte Modell, was vorgestellt werden soll,
basiert auf sog. *Bootstrapping*, ein Standardverfahren in der Statistik [@islr].

Bootstrapping ist eine Nachahmung f√ºr folgende Idee:
H√§tte man viele Stichproben aus der relevanten Verteilung,
so k√∂nnte man z.B. die Genauigkeit eines Modells $\hat{f}_{\bar{X}}$ zur Sch√§tzung des Erwartungswertes $\mu$ einfach dadurch bestimmen,
indem man *se* berechnet, also die Streuung der Mitterwerte $\bar{X}$ berechnet.
Au√üerdem gilt, dass die Pr√§zision der Sch√§tzung des Erwartungswerts steigt mit steigendem Stichprobenumfang $n$.
Wir k√∂nnten also f√ºr jede der $B$ Stichproben, $b=1,\ldots, B$, ein (Baum-)Modell berechnen, $\hat{f}^b$,
und dann deren Vorhersagen aggregieren (zum Mittelwert oder Modalwert).
Das kann man formal so darstellen [@islr]:


$$\hat{f}_{\bar{X}} = \frac{1}{B}\sum_{b=1}^{B}\hat{f}^b$$


Mit diesem Vorgehen kann die Varianz des Modells $\hat{f}_{\bar{X}}$ verringert werden;
die Vorhersagegenauigkeit steigt.

Leider haben wir in der Regel nicht viele ($B$) Datens√§tze.

Daher "bauen" wir uns aus dem einzelnen Datensatz, der uns zur Verf√ºgung steht,
viele Datens√§tze.
Das h√∂rt sich nach "too good to be true" an^[Wenn es einen No-Free-Lunch-Satz gibt, m√ºsste es auch einen Too-Good-to-be-True-Satz geben, den wir hiermit postulieren.]
Weil es sich unglaubw√ºrdig anh√∂rt, nennt man das entsprechende Verfahren (gleich kommt es!) auch "M√ºnchhausen-Methode",
nach dem ber√ºhmten L√ºbgenbaron.
Die Amerikaner ziehen sich √ºbrigens nicht am Schopf aus dem Sumpf, sondern
mit den Stiefelschlaufen (die Cowboys wieder),
daher spricht man im Amerikanischen auch von der "Boostrapping-Methode".

Diese "Pseudo-Stichproben" oder "Bootstrapping-Stichproben" sind aber recht einfach zu gewinnen..
Gegeben sei Stichprobe der Gr√∂√üe $n$:


1. Ziehe mit Zur√ºcklegen (ZmZ) aus der Stichprobe $n$ Beobachtungen
2. Fertig ist die Bootstrapping-Stichprobe.


Abb. \@ref(fig:zmz) verdeutlicht das Prinzip des ZMZ, d.h. des Bootstrappings.
Wie man sieht, sind die Bootstrap-Stichproben (rechts) vom gleichen Umfang $n$ 
wie die Originalstichprobe (links).
Allerdins kommen nicht alle F√§lle (in der Regel) in den "Boostrap-Beutel" (in bag),
sondern einige F√§lle werden oft mehrfach gezogen, so dass
einige F√§lle nicht gezogen werden (out of bag).

```{r zmz, fig.cap = "Bootstrapping: Der Topf links symbolisiert die Original-Stichprobe, aus der wir hier mehrere ZMZ-Stichproben ziehen (Rechts), dargestellt mit 'in bag'"}
knitr::include_graphics("img/zmz.png")
```


Man kann zeigen, dass ca. 2/3 der F√§lle gezogen werden,
bzw. ca. 1/3 nicht gezogen werden. Die nicht gezogenen F√§lle nennt man auch *out of bag* (OOB).

F√ºr die Entwicklung des Bootstrapping wurde der Autor, Bradley Efron, im Jahr 2018
mit dem internationalen Preis f√ºr Statistik [ausgezeichnet](https://www.amstat.org/news-listing/2021/10/08/international-prize-in-statistics-awarded-to-bradley-efron);


>   ‚ÄúWhile statistics offers no magic pill for quantitative scientific investigations, the bootstrap is the best statistical pain reliever ever produced,‚Äù says Xiao-Li Meng, Whipple V. N. Jones Professor of Statistics at Harvard University.‚Äú


## Bagging-Algorithmus

Bagging, die Kurzform f√ºr *B*ootstrap-*Agg*regation ist wenig mehr als die Umsetzung des Boostrappings.





Der Algorithmus von Bagging kann so beschrieben werden:


1. W√§hle $B$, die Anzahl der Boostrap-Stichproben und damit auch Anzahl der Submodelle (Lerner)
2. Ziehe $B$ Boostrap-Stichproben
3. Berechne das Modell $\hat{f}^{*b}$ f√ºr jede der $B$ Stichproben (typischerweise ein einfacher Baum)
4. Schicke die Test-Daten durch jedes Sub-Modell
5. Aggregiere ihre Vorhersage zu einem Wert (Modus bzw. Mittelwert) pro Fall aus dem Test-Sample, zu $\hat{f}_{\text{bag}}$


Anders gesagt:

$$\hat{f}_{\text{bag}} = \frac{1}{B}\sum_{b=1}^{B}\hat{f}^{*b}$$



Der Bagging-Algorithmus ist in Abbildung \@ref(fig:bag) dargestellt.


```{r bag, fig.cap = "Bagging schematisch illustriert"}
nomnoml::nomnoml(
  "
  [<database> Datensatz] ->zmz [Baum 1]
[<database> Datensatz] ->zmz [Baum 2]
[<database> Datensatz] ->zmz [Baum ...]
[<database> Datensatz] ->zmz [Baum B]
[Baum 1] -> [Modus als Vorhersagewert]
[Baum 2] -> [Modus als Vorhersagewert]
[Baum ...] -> [Modus als Vorhersagewert]
[Baum B] -> [Modus als Vorhersagewert]
  ", 
height = 350
)

```



Die Anzahl der B√§ume (allgemeiner: Submodelle) $B$ ist h√§ufig im oberen drei- oder niedrigem vierstelligen 
Bereich, z.B. $B=1000$. 
Eine gute Nachricht ist, dass Bagging nicht √ºberanpasst, wenn $B$ gro√ü wird.



### Variablenrelevanz


Man kann die Relevanz der Pr√§diktoren in einem Bagging-Modell auf mehrere Arten sch√§tzen.
Ein Weg (bei numerischer Pr√§diktion) ist, dass man die RSS-Verringerung, die durch Aufteilung anhand eines Pr√§diktors
erzeugt wird, mittelt √ºber alle beteiligten B√§ume (Modelle).
Bei Klassifikation kann man die analog die Reduktion des Gini-Wertes √ºber alle B√§ume mitteln
und als Sch√§tzwert f√ºr die Relevanz des Pr√§diktors heranziehen.



### Out of Bag Vorhersagen

Da nicht alle F√§lle der Stichprobe in das Modell einflie√üen (sondern nur ca. 2/3),
kann der Rest der F√§lle zur Vorhersage genutzt werden.
Bagging erzeugt sozusagen innerhalb der Stichprobe selbst√§ndig ein Train- und ein Test-Sample.
Man spricht von *Out-of-Bag-Sch√§tzung* (OOB-Sch√§tzung).
Der OOB-Fehler (z.B. MSE bei numerischen Modellen und Genauigkeit bei nominalen)
ist eine valide Sch√§tzung des typischen Test-Sample-Fehlers.

Hat man aber Tuningparameter, so wird man dennoch auf die typische Train-Test-Aufteilung
zur√ºckgreifen, um Overfitting durch das Ausprobieren der Tuning-Kandidaten zu vermeiden
 (was sonst zu Zufallstreffern f√ºhren w√ºrde bei gen√ºgend vielen Modellkandidaten).
 
 


## Random Forests

Random Forests ("Zufallsw√§lder") sind eine Weiterentwicklung von Bagging-Modellen.
Sie *sind* Bagging-Modelle, aber haben noch ein Ass im √Ñrmel:
Und zwar wird an jedem Slit (Astgabel, Aufteilung) *nur eine Zufallsauswahl an $m$ Pr√§diktoren ber√ºcksichtigt*.
Das h√∂rt sich verr√ºckt an: "Wie, mit weniger Pr√§diktoren soll eine bessere Vorhersage erreicht werden?!"
Ja, genau so ist es!
Nehmen Sie an, es gibt im Datensatz einen sehr starken und ein paar mittelstarke Pr√§diktoren;
der Rest der Pr√§diktoren ist wenig relevant.
Wenn Sie jetzt viele "gebootstrapte"^[Schlimmes Denglisch] ziehen,
werden diese B√§ume sehr √§hnlich sein: Der st√§rkste Pr√§diktor steht vermutlich immer ob an der Wurzel,
dann kommen die mittelstarken Pr√§diktoren.
Jeder zus√§tzliche Baum tr√§gt dann wenig neue Information bei.
Anders gesagt: Die Vorhersagen der B√§ume sind dann sehr √§hnlich bzw. hoch korreliert.
Bildet man den Mittelwert von hoch korrelierten Variablen,
verringert sich leider die Varianzu nur *wenig* im Vergleich zu nicht oder gering korrelierten Variablen [@islr].
Dadurch dass Random Forests nur $m$ der $p$ Pr√§diktoren pro Split zulassen,
werden die B√§ume unterschiedlicher. Wir "dekorrelieren" die B√§ume.
Bildet man den Mittelwert von gering(er) korrelierten Variablen,
so ist die Varianzreduktion h√∂her - und die Vohersage genauer.
L√§sst man pro Split $m=p$ Pr√§diktoren zu,
so gleicht Bagging dem Random Forest.
Die Anzahl $m$ der erlaubten Pr√§diktoren werden als Zufallstichprobe aus den $p$
Pr√§diktoren des Datensatzes gezogen (ohne Zur√ºcklegen).
$m$ ist ein Tuningparameter; $m=\sqrt(p)$ ist ein beliebter Startwert.
In den meisten Implementationen wird $m$ mit `mtry` bezeichnet (so auch in Tidymodels).

Der Random-Forest-Algorithmus ist in Abb. \@ref(fig:rf1) illustriert.

```{r rf1, fig.cap = "Zufallsw√§lder durch Ziehen mit Zur√ºcklegen (zmz) und Ziehen ohne Zur√ºcklegen (ZoZ)", out.width = "100%"}
nomnoml::nomnoml("
#direction:down
                 [<database> Datensatz] ->zmz [Baum1
[ZoZ]->[ZOZ2]
[ZoZ]->[ZoZ3]
]

[<database> Datensatz] ->zmz [Baum2
[ZoZ]->[ZOZ2]
[ZoZ]->[ZoZ3]
]
[<database> Datensatz] ->zmz [Baum ...
[ZoZ]->[ZOZ2]
[ZoZ]->[ZoZ3]
]
[<database> Datensatz] ->zmz [Baum B
[ZoZ]->[ZOZ2]
[ZoZ]->[ZoZ3]
]
[Baum1
[ZoZ]->[ZOZ2]
[ZoZ]->[ZoZ3]
] -> [Modus als Vorhersagewert]
[Baum2
[ZoZ]->[ZOZ2]
[ZoZ]->[ZoZ3]
] -> [Modus als Vorhersagewert]
[Baum ...] -> [Modus als Vorhersagewert]
[Baum B] -> [Modus als Vorhersagewert]
                 ",
height = 700)
```



Abb. \@ref(fig:comp-trees) vergleicht die Test-Sample-Vorhersageg√ºte von Bagging- und Random-Forest-Algorithmen aus @islr.
In diesem Fall ist die Vorhersageg√ºte deutlich unter der OOB-G√ºte; laut @islr ist dies hier "Zufall".

```{r comp-trees, fig.cap  ="Test-Sample-Vorhersageg√ºte von Bagging- und Random-Forest-Algorithmen"}
knitr::include_graphics("img/8.8.png")
```


Den Effekt von $m$ (Anzahl der Pr√§diktoren pro Split) ist in Abb. \@ref(fig:mtry) dargestellt [@islr].
Man erkennt, dass der Zusatznutzen an zus√§tzlichen B√§umen, $B$, sich abschw√§cht.
$m=\sqrt{p}$ schneidet wie erwartet am besten ab.

```{r mtry, fig.cap = "Test-Sample-Vorhersageg√ºte von Bagging- und Random-Forest-Algorithmen"}
knitr::include_graphics("img/8.10.png")
```



## Boosting


Im Unterschied zu Bagging und Random-Forest-Modellen wird beim Boosting der "Wald"
*sequenziell* entwickelt, nicht gleichzeitig wie bei den anderen vorgestellten "Wald-Modellen".
Die zwei bekanntesten Implementierungen bzw. Algorithmus-Varianten sind *AdaBoost* und *XGBoost*.
Gerade XGBoost hat den Ruf, hervorragende Vorhersagen zu leisten.
Auf [Kaggle](https://en.wikipedia.org/wiki/Kaggle) gewinnt nach [einigen Berichten oft XGBoost](https://www.kaggle.com/code/msjgriffiths/r-what-algorithms-are-most-successful-on-kaggle/report).
Nur neuronale Netze schneiden besser ab.
Random-Forest-Modelle kommen nach diesem Bereich auf Platz 3.
Allerdings ben√∂tigen neuronale Netzen oft riesige Stichprobengr√∂√üen
und bei spielen ihre Nuanciertheit vor allem bei komplexen Daten wie Bildern oder Sprache aus.
F√ºr "rechteckige" Daten (also aus einfachen, normalen Tabellen) wird ein baumbasiertes Modell oft besser abschneiden.



Die Idee des Boosting ist es, anschaulich gesprochen, 
aus Fehlern zu lernen: Fitte einen Baum, schau welche F√§lle er schlecht vorhergesagt hat,
konzentriere dich beim n√§chsten Baum auf diese F√§lle und so weiter.

Wie andere Ensemble-Methoden auch kann Boosting theoretisch f√ºr beliebige Algorithmen eingesetzt werden.
Es macht aber Sinn, Boosting bei "schwachen Lernern" einzusetzen.
Typisches Beispiel ist ein einfacher Baum; "einfach" soll hei√üen, der Baum hat nur wenig Gabeln oder vielleicht sogar nur eine einzige.
Dann spricht man von einem *Stumpf*, was intuitiv gut passt.


### AdaBoost

Der AdaBoost-Algorithmus funktioniert, einfach dargestellt, wie folgt.
Zuerst hat jeder Fall $i$ im Datensatz des gleiche Gewicht.
Die erste (und alle weiteren) Stichprobe werden per Bootstrapping aus dem
Datensatz gezogen. Dabei ist die Wahrscheinlichkeit, gezogen zu werden,
proportional zum Gewicht des Falles, $w_i$. Da im ersten Durchgang die Gewichte identisch sind,
haben zun√§chst alle F√§lle die gleiche Wahrscheinlichkeit, in das Bootstrap-Sample gezogen zu werden.
Die B√§ume bei AdaBoost sind eigentlich nur "St√ºmpfe": Sie bestehen aus einem einzelnen Split, s. Abb. \@ref(fig:stump).


```{r stump, fig.cap = "Ein Baumstumpf bei AdaBoost"}
nomnoml::nomnoml(
  "#direction: topdown
  [root] -> [leaf1]
  [root] -> [leaf2]
  ", height = 200
)
```


Nach Berechnung des Baumes und der Vorhersagen werden die *richtig* klassifizierten F√§lle heruntergewichtet
und die falsch klassifizierten F√§lle hoch gewichtet, also st√§rker gewichtet (bleiben wir aus Gr√ºnden der Einfachheit zun√§chst bei der Klassifikation).
Dieses Vorgehen folgt dem Gedanken, dass man sich seine Fehler genauer anschauen muss,
die falsch klassifizierten F√§lle sozusagen mehr Aufmerksamkeit bed√ºrfen.
Das n√§chste (zweite) Modell zieht ein weiteres Bootstrap-Sample.
Jetzt sind allerdings die Gewichte schon angepasst,
so dass mehr F√§lle, die im vorherigen Modell falsch klassifiziert wurden, in den neuen (zweiten)
Baum gezogen werden.
Das neue Modell hat also bessere Chancen,
die Aspekte, die das Vorg√§nger-Modell √ºbersah zu korrigieren bzw. zu lernen.
Jetzt haben wir zwei Modelle. Die k√∂nnen wir aggregieren, genau
wie beim Bagging: Der Modus der Vorhersage √ºber alle (beide) B√§ume hinwig ist
dann die Vorhersage f√ºr einen bestimmten Fall ("Fall" und "Beobachtung" sind stets synonym f√ºr $y_i$ zu verstehen).
So wiederholt sich das Vorgehen f√ºr $B$ B√§ume:
Die Gewichte werden angepasst, das neue Modell wird berechnet, 
alle Modelle machen ihre Vorhersagen, per Mehrheitsbeschluss - mit gewichteten Modellen -  wird die Vorhersage bestimmt pro Fall.
Irgendwann erreichen wir die vorab definierte Maximalzahl an B√§umen, $B$, und das Modell kommt zu einem Ende.

Da das Modell die Fehler seiner Vorg√§nger reduziert,
wird der Bias im Gesamtmodell verringert.
Da wir gleichzeitig auch Bagging vornehmen,
wird aber die Varianz auch verringert.
Klingt schon wieder (fast) nach Too-Good-to-be-True!



Das Gewicht $w_i^b$ des $i$ten Falls im $b$ten Modell von $B$ berechnet sich wie folgt [@rhys]:

$$ w_i^b = \begin{cases}
w_i^{b-1} \cdot e^{-\text{model weight}} \qquad \text{wenn korrekt klassifiziert} \\
w_i^{b-1} \cdot e^{\text{model weight}} \qquad \text{wenn inkorrekt klassifiziert} \\
\end{cases}$$


Das *Modellgewicht* $mw$ berechnet sich dabei so [@rhys]:


$$mw_b = 0.5 \cdot log\left( \frac{1-p(\text{inkorrect})}{p(\text{korrekt})} \right) \propto \mathcal{L(p)} $$

$p(\cdot)$ ist der Anteil (Wahrscheinlichkeit) einer Vorhersage.

Das Modellgewicht ist ein Faktor, der schlechtere Modelle bestraft.
Das folgt dem Gedanken, 
dass schlechteren Modellen weniger Geh√∂rt geschenkt werden soll,
aber schlecht klassifizierten F√§llen mehr Geh√∂r.

Das Vorgehen von AdaBoost ist in Abb. \@ref(fig:ada) illustriert.

```{r ada, fig.cap = "AdaBoost illustriert"}
nomnoml::nomnoml(
  "
  [m1] -> [ensemble]
  [ensemble] -> [m2]
  [m2] -> [ensemble]
  [ensemble] -> [m3]
  [m3] -> [ensemble]
  [ensemble] -> [m4]
  [m4] -> [ensemble]
  [ensemble] -> [M ...]
  [M ...] -> [ensemble]
  [ensemble] -> [M B]
  ",
  height = 450
)
```





### XGBoost

XGBoost ist ein Gradientenverfahren,
eine Methode also, die die Richtung des parziellen Ableitungskoeffizienten als Optimierungskriterium heranzieht.
XGBoost ist √§hnlich zu AdaBoost,
nur dass *Residuen* modelliert werden, nicht $y$.
Die Vorhersagefehler von $\hat{f}^b$ werden die Zielvariable von $\hat{f}^{b+1}$.
Ein Residuum ist der Vorhersagefehler, bei metrischen Modellen etwa RMSE,
oder schlicht $r_i = y_i - \hat{y}_i$.
Details finden sich z.B. [hier](https://arxiv.org/pdf/1603.02754.pdf), dem Original XGBoost-Paper [@chen_xgboost_2016].


Die hohe Vorhersageg√ºte von Boosting-Modellen ist exemplarisch in Abb. \@ref(fig:boost) dargestellt [@islr, S. 358ff].
Allerdings verwenden die Autoren Friedmans [-@friedman_greedy_2001] *Gradient Boosting Machine*, eine weitere Variante des Boosting .


```{r boost, fig.cap = "Vorhersageg√ºte von Boosting und Random Forest"}
knitr::include_graphics("img/8.10.png")
```





## Tidymodels


### Datensatz Churn

Wir betrachten einen Datensatz zur Kundenabwanderung (Churn) aus [dieser Quelle](https://www.gmudatamining.com/lesson-13-r-tutorial.html).


```{r}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
churn_df <- read_rds('https://gmudatamining.com/data/churn_data.rds')
```

Ein Blick in die Daten:

```{r}
churn_df %>% 
  head() %>% 
  gt::gt()
```


### Data Splitting und CV

Das Kreuzvalidieren (CV) fassen wir auch unter diesen Punkt.



```{r}
churn_split <- initial_split(churn_df, prop = 0.75, 
                             strata = canceled_service)

churn_training <- churn_split %>% training()

churn_test <- churn_split %>% testing()

churn_folds <- vfold_cv(churn_training, v = 5)
```



### Feature Engineering


Hier definieren wir zwei Rezepte.
Gleichzeitig ver√§ndern wir die Pr√§diktoren (normalisieren, dummysieren, ...).
Das nennt man auch *Feature Engineering*.

```{r recipes}
churn_recipe1 <- recipe(canceled_service ~ ., data = churn_training) %>% 
                       step_normalize(all_numeric(), -all_outcomes()) %>% 
                       step_dummy(all_nominal(), -all_outcomes())

churn_recipe2 <- recipe(canceled_service ~ ., data = churn_training) %>% 
                       step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
                       step_normalize(all_numeric(), -all_outcomes()) %>% 
                       step_dummy(all_nominal(), -all_outcomes())
```


`step_YeoJohnson()` reduziert Schiefe in der Verteilung.



### Modelle


```{r def-models}
tree_model <- decision_tree(cost_complexity = tune(),
                            tree_depth = tune(),
                            min_n = tune()) %>% 
              set_engine('rpart') %>% 
              set_mode('classification')

rf_model <- rand_forest(mtry = tune(),
                        trees = tune(),
                        min_n = tune()) %>% 
            set_engine('ranger') %>% 
            set_mode('classification')


boost_model <- boost_tree(mtry = tune(),
                        min_n = tune(),
                        trees = tune()) %>% 
  set_engine("xgboost", nthreads = parallel::detectCores()) %>% 
  set_mode("classification")


glm_model <- logistic_reg()
```



### Workflows


Wir definieren ein Workflow-Set:


```{r}
preproc <- list(rec1 = churn_recipe1, rec2 = churn_recipe2)
models <- list(tree1 = tree_model, rf1 = rf_model, boost1 = boost_model, glm1 = glm_model)
 
 
all_workflows <- workflow_set(preproc, models)
```

Infos zu `workflow_set` bekommt man wie gewohnt mit `?workflow_set`.

Im Standard werden alle Rezepte und Modelle miteinander kombiniert (`cross = TRUE`),
also `preproc * models` Modelle gefittet.




### Modelle berechnen mit Tuning, einzeln


Wir k√∂nnten jetzt jedes Modell einzeln tunen, wenn wir wollen.


#### Baum

```{r tree-tune-grid, eval = TRUE}
tree_wf <-
  workflow() %>% 
  add_model(tree_model) %>% 
  add_recipe(churn_recipe1)


tic()
tree_fit <-
  tree_wf %>% 
  tune_grid(
    resamples = churn_folds,
    metrics =  metric_set(roc_auc, sens, yardstick::spec)
    )
toc()
```

Im Standard werden 10 Modellkandidaten getuned.


```{r}
tree_fit
```

Schauen wir uns das Objekt etwas n√§her an:

```{r}
tree_fit$.metrics[[1]]
```

30 Zeilen: 3 G√ºtemetriken (Sens, Spec, ROC AUC) mit je 10 Werten (Submodellen),
gibt 30 Koeffizienten.

F√ºr jeden der 5 Faltungen haben wir also 10 Submodelle.


Welches Modell ist das beste?

```{r}
show_best(tree_fit)
```

Aha, das sind die f√ºnf besten Modelle, bzw. ihre Tuningparameter,
ihre mittlere G√ºte zusammen mit dem Standardfehler.


```{r}
autoplot(tree_fit)
```


#### RF


Was f√ºr Tuningparameter hat den der Algorithmus bzw. seine Implementierung?

```{r}
show_model_info("rand_forest")
```


Da die Berechnung einiges an Zeit braucht,
kann man das (schon fr√ºher einmal berechnete) Ergebnisobjekt
von der Festplatte lesen (sofern es existiert).
Ansonsten berechnet man neu:

```{r rf-tune-grid, cache = TRUE, eval = TRUE}
if (file.exists("objects/rf_fit1.rds")){
  rf_fit1 <- read_rds("objects/rf_fit1.rds")
} else {
rf_wf1 <-
  workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(churn_recipe1)


tic()
rf_fit1 <-
  rf_wf1 %>% 
  tune_grid(
    resamples = churn_folds,
    metrics =  metric_set(roc_auc, sens, spec)
    )
toc()
}
```

So
kann man das berechnete Objekt abspeichern auf Festplatte,
um k√ºnftig Zeit zu sparen:

```{r eval = FALSE}
write_rds(rf_fit1, file = "objects/rf_fit1.rds")
```



```{r}
rf_fit1
```


```{r}
show_best(rf_fit1)
```


#### XGBoost




```{r boost-tune-grid, cache = TRUE, eval = FALSE}
boost_wf1 <-
  workflow() %>% 
  add_model(boost_model) %>% 
  add_recipe(churn_recipe1)


tic()
boost_fit1 <-
  boost_wf1 %>% 
  tune_grid(
    resamples = churn_folds,
    metrics =  metric_set(roc_auc, sens, spec)
    )
toc()
```


Wieder auf Festplatte speichern:

```{r eval = FALSE}
write_rds(boost_fit1, file = "objects/boost_fit1.rds")
```


Und so weiter.

### Workflow-Set tunen

```{r tune-workflow-map}
if (file.exists("objects/churn_model_set.rds")) {
  churn_model_set <- read_rds("objects/churn_model_set.rds")
} else {
  tic()
  churn_model_set <-
    all_workflows %>% 
    workflow_map(
      resamples = churn_folds,
      grid = 20,
      metrics = metric_set(roc_auc),
      seed = 42,  # reproducibility
      verbose = TRUE)
  toc()
}
```



Da die Berechnung schon etwas Zeit braucht,
macht es Sinn, das Modell (bzw. das Ergebnisobjekt) auf Festplatte zu speichern:

```{r eval = FALSE}
write_rds(churn_model_set, file = "objects/churn_model_set.rds")
```


Entsprechend kann man das Modellobjekt wieder importieren, wenn einmal abgespeichert:

```{r eval = FALSE}
churn_model_set <- read_rds(file = "objects/churn_model_set.rds")
```


### Ergebnisse im Train-Sest


Hier ist die Rangfolge der Modelle, geordnet nach mittlerem ROC AUC:

```{r}
rank_results(churn_model_set, rank_metric = "roc_auc")
```



```{r}
autoplot(churn_model_set, metric = "roc_auc")
```



### Bestes Modell

Und hier nur der beste Kandidat pro Algorithmus:


```{r}
autoplot(churn_model_set, metric = "roc_auc", select_best = "TRUE") +
  geom_text(aes(y = mean - .01, label = wflow_id), angle = 90, hjust = 1) +
  theme(legend.position = "none") +
  lims(y = c(0.85, 1))
```


Boosting hat  - knapp - am besten abgeschnitten. 
Allerdings sind Random Forest und die schlichte, einfache logistische Regression auch fast genau so gut.
Das w√§re ein Grund f√ºr das einfachste Modell, das GLM, zu votieren.
Zumal die Interpretierbarkeit am besten ist.
Alternativ k√∂nnte man sich f√ºr das Boosting-Modell aussprechen.


Man kann sich das beste Submodell auch von Tidymodels bestimmen lassen.
Das scheint aber (noch) nicht f√ºr ein Workflow-Set zu funktionieren,
sondern nur f√ºr das Ergebnisobjekt von `tune_grid`.



```{r error = TRUE}
select_best(churn_model_set, metric = "roc_auc")
```


`rf_fit1` haben wir mit `tune_grid()` berechnet;
mit diesem Modell kann `select_best()` arbeiten: 

```{r}
select_best(rf_fit1)
```


Aber wir k√∂nnen uns h√§ndisch behelfen.

Schauen wir uns mal die Metriken (Vorhersageg√ºte) an:

```{r}
churn_model_set %>% 
  collect_metrics() %>% 
  arrange(-mean)
```

`rec1_boost1` scheint das beste Modell zu sein.


```{r}
best_model_params <-
extract_workflow_set_result(churn_model_set, "rec1_boost1") %>% 
  select_best()

best_model_params
```



### Finalisisieren

Wir entscheiden uns mal f√ºr das Boosting-Modell, `rec1_boost1`.
Diesen Workflow, in finalisierter Form, 
brauchen wir f√ºr den "final Fit".
Finalisierte Form hei√üt:

- Schritt 1: Nimm den passenden Workflow, hier `rec1` und `boost1`; das hatte uns oben `rank_results()` verraten.
- Schritt 2: Update (Finalisiere) ihn mit den besten Tuningparameter-Werten



```{r}
# Schritt 1:
best_wf <- 
all_workflows %>% 
  extract_workflow("rec1_boost1")

best_wf
```







Jetzt finalisieren wir den Workflow,
d.h. wir setzen die Parameterwerte des besten Submodells ein:


```{r}
# Schritt 2:
best_wf_finalized <- 
  best_wf %>% 
  finalize_workflow(best_model_params)

best_wf_finalized
```





### Last Fit


```{r}
fit_final <-
  best_wf_finalized %>% 
  last_fit(churn_split)

fit_final
```



```{r}
collect_metrics(fit_final)
```



### Variablenrelevanz


Um die Variablenrelevanz zu plotten,
m√ºssen wir aus dem Tidymodels-Ergebnisobjekt
das eigentliche Ergebnisobjekt herausziehen,  von der R-Funktion, die die eigentliche
Berechnung durchf√ºhrt,
das w√§re `glm()` bei einer logistischen Regression oder `xgboost::xgb.train()` bei 
XGBoost:

```{r}
fit_final %>% 
  extract_fit_parsnip()
```

*Dieses* Objekt √ºbergeben wir dann an `{vip}`:

```{r}
fit_final %>% 
  extract_fit_parsnip() %>% 
  vip()
```




### ROC-Curve


Eine ROC-Kurve berechnet Sensitivit√§t und Spezifit√§t aus den Vorhersagen,
bzw. aus dem Vergleich von Vorhersagen und wahrem Wert (d.h. der beobachtete Wert).


Ziehen wir also zuerst die Vorhersagen heraus:

```{r}
fit_final %>% 
  collect_predictions()
```

Praktischerweise werden die "wahren Werte" (also die beobachtaten Werte), `canceled_service`,
ausch angegeben.

Dann berechnen wir die `roc_curve` und `autoplot`ten sie.

```{r}
fit_final %>% 
  collect_predictions() %>% 
  roc_curve(canceled_service, .pred_yes) %>% 
  autoplot()
```



<!-- ## Aufgaben und Vertiefung -->




## Aufgaben

```{r child = "Aufgaben/Thema11-Loesungen1.markdown", results='asis'}

 
```






```{r render-outline-vertiefung-aufgaben, results = "asis", echo = FALSE, message = FALSE}

# render_section(course_dates_file,
#                  content_file,
#                  i = NULL,
#                  title = "Ensemble-Lerner",
#                  name = "Aufgaben",
#                  header_level = 1)


render_section(course_dates_file,
               content_file,
               i = NULL, 
               title = "Ensemble-Lerner",
               name = "Vertiefung",
               header_level = 1)
```
