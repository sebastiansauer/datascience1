
# Logistische Regression





```{r echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Lernsteuerung

### Vorbereitung

Frischen Sie Ihr Wissen zur logistischen Regression auf bzw. machen Sie sich mit den Grundlagen des Verfahrens vertraut.
    
### Lernziele

Sie verstehen den Zusammenhang von linearen und logistischen Modellen
Sie k√∂nnen die logistische Regression mit Methoden von tidymodels anwenden
 
###  Literatur

Rhys, Kap. 4


### Ben√∂tigte R-Pakete

```{r}
library(tidyverse)
library(tidymodels)
library(easystats)
```


`{easystats}` ist, wie Tidymodels und Tidyverse, ein Metapaket,
ein R-Paket also, das mehrere Pakete verwaltet und startet.
[Hier](https://easystats.github.io/easystats/) findet sich mehr Info zu Easystats.




Einen flotten Spruch bekommen wir von Easystats gratis dazu:

```{r}
easystats_zen()
```




```{r echo = FALSE}
library(gt)
library(nomnoml)
library(patchwork)
```




## Intuitive Erkl√§rung

Die *logistische Reression* ist ein Spezialfall des linearen Modells (lineare Regression),
der f√ºr *bin√§re* (dichotom) AV eingesetzt wird (es gibt auch eine Variante f√ºr multinominale AV). 
Es k√∂nnen eine oder mehrere UV in eine logistische Regression einflie√üen,
mit beliebigem Skalenniveau.

Beispiele f√ºr  Forschungsfragen, die mit der logistischen Regression modelliert werden sind:

- Welche Faktoren sind pr√§diktiv, um vorherzusagen, ob jemand einen Kredit zur√ºckzahlen kann oder nicht?
- Haben weibliche Passagiere aus der 1. Klasse eine h√∂here √úberlebenschance als andere Personen auf der Titanic?
- Welche Faktoren h√§ngen damit zusammen, ob ein Kunde eine Webseite verl√§sst, bevor er einen Kauf abschlie√üt?



Der Name stammt von der [logistischen Funktion](https://en.wikipedia.org/wiki/Logistic_function),
die man in der einfachsten Form so darstellen kann:

$$f(x) = \frac{x}{1+e^{-x}}$$

Da die AV als dichotom modelliert wird, spricht man von einer *Klassifikation*.


Allerdings ist das Modell reichhaltiger als eine blo√üe Klassifikation,
die (im bin√§ren Fall) nur 1 Bit Information liefert: "ja" vs. "nein" bzw. 0 vs. 1.

Das Modell liefert n√§mlich nicht nur eine Klassifikation zur√ºck,
sondern auch eine *Indikation der St√§rke* (epistemologisch) der Klassenzugeh√∂rigkeit.

Einfach gesagt hei√üt das, 
dass die logistische Regression eine Wahrscheinlichkeit der Klassenzugeh√∂rigkeit zur√ºckliefert.




```{mermaid}
%%| label: fig-klass
%%| fig-cap: Ablauf einer Klassifikation


flowchart LR
  Daten --> Modell --> Wskt --> Klasse

```

## Profil

Das Profil des Modells kann man wie folgt charakterisieren, vgl. Tab. @tbl-tabprof-logist.


```{r prof-logist, echo = FALSE}
#| label: tbl-tabprof-logist
#| tbl-cap: "Profil der logistischen Regression"
d <- read_csv("materials/profile-models.csv")
d %>% 
  select(1, 2) %>% 
  gt::gt()
```



## Warum nicht die lineare Regression verwenden?


*Forschungsfrage*: Kann man anhand des Spritverbrauchs vorhersagen, ob ein Auto eine Automatik- bzw. ein manuelle Schaltung hat? Anders gesagt: H√§ngen Spritverbrauch und Getriebeart, s. @fig-mtcars? (Datensatz `mtcars`)

```{r}
data(mtcars)
d <-
  mtcars %>% 
  mutate(mpg_z = standardize(mpg),
         iv = mpg_z,
         dv = am)

m81 <- lm(dv ~ iv, data = d)
coef(m81)
```


```{r echo = FALSE}
#| label: fig-mtcars
#| fig-cap: Klassifikation von am 
d %>% 
  ggplot(aes(x = iv, y = dv)) +
  geom_point() +
  geom_hline(yintercept = 0.5, color = "white", size = 2) +
  geom_abline(intercept = coef(m81)[1],
              slope = coef(m81)[2],
              color = "blue") 
```


$Pr(\text{am}=1|m91,\text{mpgz}=0) = 0.46$: 
Die Wahrscheinlichkeit einer manuelle Schaltung, 
gegeben einem durchschnittlichen Verbrauch (und dem Modell `m81`) liegt bei knapp 50%.



### Lineare Modelle running wild

Wie gro√ü ist die Wahrscheinlichkeit f√ºr eine manuelle Schaltung ...



- ... bei `mpg_z = -2`?

```{r echo = TRUE}
predict(m81, newdata = data.frame(iv = -2))
```

$Pr(\hat{y})<0$ macht keinen Sinn. ‚ö° 


- ... bei `mpg_z = +2`?

```{r echo = TRUE}
predict(m81, newdata = data.frame(iv = +2))
```





$Pr(\hat{y})>1$ macht keinen Sinn. ‚ö° 



Schauen Sie sich mal die Vorhersage an f√ºr `mpg_z=5` ü§Ø 


### Wir m√ºssen die Regressionsgerade umbiegen

... wenn der vorhergesagte Wert eine Wahrscheinlichkeit, $p_i$, ist, s. @fig-biegen.


```{r echo = FALSE}
#| label: fig-biegen
#| fig-cap: Wir biegen die Regressionsgeraden in eine S-Form
s_fun <- function(x) exp(x) / (1 + exp(x))

plot_logist_regr <- 
  ggplot(tibble(x=c(-2,2))) +
  aes(x=x) +
  geom_abline(slope = .25,
              intercept = 0.5) +
  labs(x = "x",
       y = "Pr(y=1)=p") +
  geom_hline(yintercept = c(0,1), linetype = "dotted") +
  scale_x_continuous(limits = c(-5,5)) +
  scale_y_continuous(limits = c(-0.5, 1.5)) +
  stat_function( fun = s_fun, color = "blue", size = 2) +
  geom_rect(xmin = -Inf,
            xmax = +Inf,
            ymin = 1,
            ymax = +Inf,
            fill = "Red",
            alpha = .2) +
  geom_rect(xmin = -Inf,
            xmax = +Inf,
            ymin = 0,
            ymax = -Inf,
            fill = "Red",
            alpha = .2)

plot_logist_regr
```


Die *schwarze* Gerade verl√§sst den Wertebereich der Wahrscheinlichkeit.
Die *blaue* Kurve, $\mathcal{f}$, bleibt im erlaubten Bereich, $Pr(y) \in [0,1]$.
Wir m√ºssen also die linke oder die rechte Seite des linearen Modells transformieren:
$p_i = f(\alpha + \beta \cdot x)$ bzw.:

$f(p) = \alpha + \beta \cdot x$

$\mathcal{f}$ nennt man eine *Link-Funktion*.





### Verallgemeinerte lineare Modelle zur Rettung

F√ºr metrische AV mit theoretisch unendlichen Grenzen des Wertebereichs haben wir bisher eine Normalverteilung verwendet:

$$y_i \sim \mathcal{N}(\mu_i, \sigma)$$

Dann ist die Normalverteilung eine voraussetzungsarme Wahl (maximiert die Entropie).

Aber wenn die AV *bin√§r* ist bzw. *H√§ufigkeiten* modelliert, 
braucht man eine Variable die nur positive Werte zul√§sst.

Diese Verallgemeinerung des linearen Modells bezeichnet man als *verallgemeinertes lineares Modell* (generalized linear model, GLM).

Im Falle einer bin√§ren (bzw. dichotomen) AV liegt eine bestimmte Form des GLM vor,
die man als *logistische Regression* bezeichnet.






## Der Logit-Link

Der *Logit-Link* wird auch $\mathcal{L}$, `logit`, Log-Odds oder Logit-Funktion genannt.

Er "biegt" die lineare Funktion in die richtige Form.

Der Logit-Link ordnet einen Parameter, der als Wahrscheinlichkeitsmasse definiert ist (und daher im Bereich von 0 bis 1 liegt), einem linearen Modell zu (das jeden beliebigen reellen Wert annehmen kann):

$$
\begin{align}
    \text{logit}(p_i) &= \alpha + \beta x_i
\end{align}
$$





- Die Logit-Funktion $\mathcal{L}$ ist definiert als der (nat√ºrliche) Logarithmus des Verh√§ltnisses der Wahrscheinlichkeit zu Gegenwahrscheinlichkeit:

$$\mathcal{L} = \text{log} \frac{p_i}{1-p_i}$$

- Das Verh√§ltnis der Wahrscheinlichkeit zu Gegenwahrscheinlichkeit nennt man auch *Odds*.

- Also:

$$\mathcal{L} = \text{log} \frac{p_i}{1-p_i} = \alpha + \beta x_i$$


## Aber warum?

*Forschungsfrage*: H√§ngt das √úberleben (statistisch) auf der Titanic vom Geschlecht ab?


Wie war eigentlich *insgesamt*, also ohne auf einen (oder mehrere) Pr√§diktoren zu bedingen, die √úberlebenswahrscheinlichkeit?

```{r m82}
data(titanic_train, package = "titanic")

m82 <- lm(Survived ~ 1, data = titanic_train)
coef(m82)
```

Die Wahrscheinlichkeit zu √úberleben $Pr(y=1)$ lag bei einem guten Drittel (0.38).

Das h√§tte man auch so ausrechnen:

```{r}
titanic_train %>% 
  count(Survived) %>% 
   mutate(prop = n/sum(n))
```


Anders gesagt: $p(y=1) = \frac{549}{549+342} \approx 0.38$


### tidymodels, m83


Berechnen wir jetzt ein lineares Modell f√ºr die AV `Survived` mit dem Geschlecht als P√§diktor:

```{r d}
d <-
  titanic_train %>% 
  filter(Fare > 0) %>% 
  mutate(iv = log(Fare),
         dv = factor(Survived))
```


Die Faktorstufen, genannt `levels` von `Survived` sind:

```{r}
levels(d$dv)
```

Und zwar genau in dieser Reihenfolge.


## lm83, glm

Die klassische Methoden in R, ein logistisches Modell zu berechnen, ist
mit der Funktion `glm()`.
Tidymodels greift intern auf diese Funktion zur√ºck.
Daher sind die Ergebnisse numerisch identisch.

```{r m83-glm}
lm83 <- glm(dv ~ iv, data = d, family = "binomial")
coef(lm83)
```


- AV: √úberleben (bin√§r/Faktor)
- UV: Ticketpreis



Mit `{easystats}` kann man sich `model_parameter()` einfach ausgeben lassen:


```{r}
library(easystats)


model_parameters(lm83)
```


Und auch visualisieren lassen:

```{r}
plot(model_parameters(lm83))
```


## m83, tidymodels


*Achtung*! Bei tidymodels muss bei einer Klassifikation die AV vom Type `factor` sein.
Au√üerdem wird bei `tidymodels`, im Gegensatz zu `(g)lm` nicht die zweite,
sondern die *erste* als Ereignis modelliert wird.



Daher wechseln wir die *ref*erenzkategorie, wir "re-leveln", mit `relevel()`:

```{r}
d2 <-
  d %>% 
  mutate(dv = relevel(dv, ref = "1"))
```


Check:

```{r}
levels(d2$dv)
```


Passt.



Die erste Stufe ist jetzt `1`, also √úberleben.


Jetzt berechnen wir das Modell in gewohnter Weise mit `tidymodels`.

```{r}
m83_mod <-
  logistic_reg()

m83_rec <-
  recipe(dv ~ iv, data = d2)

m83_wf <-
  workflow() %>% 
  add_model(m83_mod) %>% 
  add_recipe(m83_rec)

m83_fit <-
  fit(m83_wf, data = d2)
```


Hier sind die Koeffizienten, die kann man sich aus `m83_fit` herausziehen:

```{r m83-coefs, echo = FALSE}
m83_fit %>% 
  tidy() %>% 
  gt() %>% 
  fmt_number(where(is.numeric))

coefm83 <-
  m83_fit %>% 
  tidy() %>% 
  pull(2)
coefm83
```


```{r eval = TRUE, echo = FALSE}
m83_fit %>% 
  tidy() 
```


Die Koeffizienten werden in Logits angegeben.


In @fig-m83-plot ist das Modell und die Daten visualisiert.


```{r m83-plot, echo = FALSE, fig.cap = "Modell m83 und die Titanic-Daten"}
#| label: fig-m83-plot
d2 %>% 
  ggplot(aes(x = iv, y = Survived)) +
  geom_hline(yintercept = 0.5, color = "white", size = 2) +
  geom_jitter(alpha = .5, height = 0) +
  geom_abline(intercept = coefm83[1],
              slope = coefm83[2],
              color = "blue") +
  scale_x_continuous(limits = c(0, 10)) +
  labs(y = "Wahrscheinlichkeit (√úberleben)",
       x = "Ticketpreis")
```



Definieren wir als $y=1$ das zu modellierende Ereignis, hier "√úberleben auf der Titanic" (hat also √ºberlebt). 


Wie wir oben schon gesehen haben, funktioniert die lineare Regression nicht einwandfrei bei bin√§ren (oder dichotomen) AV.




### Wahrscheinlichkeit in Odds

Probieren wir Folgendes: Rechnen wir die Wahrscheinlichkeit zu √úberlegen f√ºr $y$, kurz $p$, in *Odds* (Chancen) um.

$odds = \frac{p}{1-p}$

In R:

```{r}
odds <- 0.38 / 0.62
odds
```

Bildlich gesprochen sagen die Odds: f√ºr 38 Menschen, die √ºberlebt haben, kommen (ca.) 62 Menschen, die nicht √ºberlebt haben, s. @fig-odds1.


```{r odds1, fig.cap = "Odds: 38 zu 62", echo = FALSE}
#| label: fig-odds1
d2_odds <-
  tibble(x = c(rep("0", 38), rep("1", 62)),
         y = c(rep(1, 38), rep(1, 62)))

d2_odds %>% 
  ggplot(aes(x, y)) +
  geom_jitter(width = .2, alpha = .7)
```


Plotten wir die Odds als Funktion der UV, s. @fig-odds2.

```{r eval = FALSE, echo = FALSE}
# d3 <-
#   d2 %>% 
#   bind_cols(predict(m83_fit, new_data = tibble(iv = iv)))
```





```{r odds2, echo = FALSE, fig.cap = "Odds als Funktion der UV"}
#| label: fig-odds2

d_pred <-
  tibble(iv = seq(0, 10, by = .1))
         
d_pred <-
  d_pred %>% 
  mutate(logit = predict(m83_fit, new_data = d_pred, type = "raw")) %>% 
  mutate(odds = exp(logit))

d_pred %>% 
  ggplot(aes(x = iv, y = odds)) +
  geom_jitter(alpha = .5) +
  labs(y = "Odds (Sterben)")
```


Wir sind noch nicht am Ziel;
die Variable ist noch nicht "richtig gebogen".


### Von Odds zu Log-Odds

Wenn wir jetzt den Logarithmus (der Odds) berechnen bekommen wir eine "brav gebogenen" Funktion, die Log-Odds, $\mathcal{L}$, als Funktion der UV, s. @fig-logit2.


$$\mathcal{L} = log (odds) = log \left(\frac{p}{1-p}\right)$$

```{r logit2, echo = FALSE, fig.cap = "Logit als Funktion der UV"}
#| label: fig-logit2
d_pred %>% 
  ggplot(aes(x = iv, y = logit)) +
  geom_jitter(alpha = .5) +
  labs(y = "Logit")
```

Linear!


Es gilt also:

$$\text {log-odds} = b_0 + b_1x$$




Log-Odds (Log-Odds) bezeichnet man auch als *Logits*.



## Inverser Logit

Um nach $p$ aufzul√∂sen, m√ºssen wir einige Algebra bem√ºhen:

$$
\begin{align}
\text{log} \frac{p}{1-p} &= \alpha + \beta x & & \text{Exponentieren}\\
\frac{p}{1-p} &= e^{\alpha + \beta x} \\
p_i &= e^{\alpha + \beta x_i} (1-p) & & \text{Zur Vereinfachung: } x := e^{\alpha + \beta x_i} \\
p_i &= x (1-p) \\
&= x - xp \\
p + px &= x \\
p(1+x) &= x \\
p &= \frac{x} {1+x} & & \text{L√∂sen wir x wieder auf.} \\
p &= \frac{e^{\alpha + \beta x_i}}{1 + e^{\alpha + \beta x_i}} = \mathcal{L}^{-1}
\end{align}
$$



Diese Funktion nennt man auch *inverser Logit*, $\text{logit}^{-1}, \mathcal{L}^{-1}$.


Zum Gl√ºck macht das alles die Rechenmaschine f√ºr uns üòÑ.



### Vom Logit zur Klasse


Praktisch k√∂nnen wir uns die Logits und ihre zugeh√∂rige Wahrscheinlichkeit einfach ausgeben lassen mit R. Und die vorhergesagte Klasse (`.pred_class`) auch:


```{r d3}
d3 <-
  d2 %>% 
  bind_cols(predict(m83_fit, new_data = d2, type = "prob")) %>% 
  bind_cols(predict(m83_fit, new_data = d2)) %>%  # Klasse
  bind_cols(logits = predict(m83_fit, new_data = d2, type = "raw"))  # Logits
  
d3 %>% 
  slice_head(n = 3) %>% 
  select(Name, last_col())
```


### Grenzwert wechseln

Im Standard wird 50% als Grenzwert f√ºr die vorhergesagte Klasse $c$ genommen:

- wenn $p <= .5 \rightarrow c = 0$
- wenn $p > .5 \rightarrow c = 1$

Man kann aber den Grenzwert beliebig w√§hlen, um Kosten-Nutzen-Abw√§gungen zu optimieren;
mehr dazu findet sich z.B. [hier](https://probably.tidymodels.org/articles/where-to-use.html).


## Logit und Inverser Logit



### Logit
$(0,1) \rightarrow (-\infty, +\infty)$

@fig-logit-invlogit zeigt die Ver√§nderung des Wertebereichs bei Umrechnung von Wahrscheinlichkeit zu Logit.

```{r dpi = 300, echo = FALSE}
#| label: fig-logit-invlogit
#| fig-cap: "Der Wertebereich der Wahrscheinlichkeit ist [0,1]; der Wertebereich des Logits [-Inf,+Inf]."
ggplot() +
  annotate("segment", 
           x = 0,
           xend = 0,
           y = 0,
           yend = 1) +
  scale_x_continuous(breaks = c(0,1), labels = c("p", "Logit")) +
  scale_y_continuous(breaks = NULL) +
  annotate("label",
           x = c(0, 0),
             y = c(0, 1),
             label = c("0", "1")) +
  annotate("segment", 
           x = 1,
           xend = 1,
           y = -1,
           yend = 2) +
  annotate("segment", 
           x = 1,
           xend = 1,
           y = -2,
           yend = 3,
           linetype = "dotted") +
  annotate("label",
           x = c(1, 1),
             y = c(-2, 3),
             label = c("-‚àû", "+‚àû")) +
  annotate("segment",
           x = 0.02,
           xend = 0.98,
           y = 0,
           yend = -2,
           #linetype = "dotted",
           size = 0.1,
           arrow = arrow()) +
  annotate("segment",
           x = 0.02,
           xend = 0.98,
           y = 1,
           yend = 3,
           #linetype = "dotted",
           size = 0.1,
           arrow = arrow()) +
  labs(x = "",
       y = "") +
  annotate("label",
           x = c(1.01),
             y = c(.5),
             label = c("0"),
           size = 4) +
  labs(x = "",
       y = "") +
  annotate("label",
           x = c(-0.01),
             y = c(.5),
             label = c("0.5"),
           size = 4) +
  annotate("segment",
           x = 0.02,
           xend = 0.98,
           yend = .5,
           y = .5,
           #linetype = "dotted",
           size = 0.1,
           arrow = arrow())
```




Praktisch, um Wahrscheinlichkeit zu modellieren.

$$p \rightarrow \fbox{logit} \rightarrow \alpha + \beta x$$


### Inv-Logit

Beim Inversen Logit (Inv-Logit) ist es genau umgekehrt wie beim Logit.
@fig-inv-logit-prob zeigt die Ver√§nderung des Wertebereichs des Inv-Logits.

$(-\infty, +\infty) \rightarrow (0,1)$

```{r inv-logit-plot, echo = FALSE}
#| label: fig-inv-logit-prob
#| fig-cap: Ver√§nderung der Wertebereichs durch die Inv-Logit-Umrechnung
ggplot() +
  annotate("segment", 
           x = 0,
           xend = 0,
           y = 0,
           yend = 1) +
  scale_x_continuous(breaks = c(0,1), labels = c("p", "Logit")) +
  scale_y_continuous(breaks = NULL) +
  annotate("label",
           x = c(0, 0),
             y = c(0, 1),
             label = c("0", "1")) +
  annotate("segment", 
           x = 1,
           xend = 1,
           y = -1,
           yend = 2) +
  annotate("segment", 
           x = 1,
           xend = 1,
           y = -2,
           yend = 3,
           linetype = "dotted") +
  annotate("label",
           x = c(1, 1),
             y = c(-2, 3),
             label = c("-‚àû", "+‚àû")) +
  annotate("segment",
           xend = 0.02,
           x = 0.98,
           yend = 0,
           y = -2,
           #linetype = "dotted",
           size = 0.1,
           arrow = arrow()) +
  annotate("segment",
           xend = 0.02,
           x = 0.98,
           yend = 1,
           y = 3,
           #linetype = "dotted",
           size = 0.1,
           arrow = arrow()) +
  annotate("label",
           x = c(1.01),
             y = c(.5),
             label = c("0"),
           size = 3) +
  labs(x = "",
       y = "") +
  annotate("label",
           x = c(-0.01),
             y = c(.5),
             label = c("0.5"),
           size = 3) +
  annotate("segment",
           xend = 0.02,
           x = 0.98,
           yend = .5,
           y = .5,
           #linetype = "dotted",
           size = 0.1,
           arrow = arrow())
```

Praktisch, um in Wahrscheinlichkeiten umzurechnen.

$$p \leftarrow \fbox{inv-logit} \leftarrow \alpha + \beta x$$


## Logistische Regression im √úberblick

- Eine Regression mit binomial verteilter AV und Logit-Link nennt man *logistische Regression*.

- Man verwendet die logistische Regression um binomial verteilte AV zu modellieren, z.B.
    - Wie hoch ist die Wahrscheinlichkeit, dass ein Kunde das Produkt kauft?
    - Wie hoch ist die Wahrscheinlichkeit, dass ein Mitarbeiter k√ºndigt?
    - Wie hoch ist die Wahrscheinlichkeit, die Klausur zu bestehen?
    
- Die logistische Regression ist eine normale, lineare Regression f√ºr den Logit von $Pr(y=1)$, wobei $y$ (AV) binomialvereteilt mit $n=1$ angenommen wird:


$$
\begin{align}
y_i &\sim \mathcal{B}(1, p_i) \\
\text{logit}(p_i) &= \alpha + \beta x_i
\end{align}
$$



- Da es sich um eine normale, lineare Regression handelt, sind alle bekannten Methoden und Techniken der linearen Regression zul√§ssig.

- Da Logits nicht einfach zu interpretieren sind, rechnet man nach der Berechnung des Modells den Logit h√§ufig in Wahrscheinlichkeiten um.



### Die Koeffizienten sind schwer zu interpretieren

Puhhh, s. @fig-logit-convert-plot

```{r logit-convert-plot, fig.asp=0.4, echo = FALSE}
#| label: fig-logit-convert-plot
#| fig-cap: "Die Koeffizienten der logistischen Regression sind nicht normal - im additiven Sinne - zu interpretieren."

s_fun <- function(x) exp(x) / (1 + exp(x))
inv_logist <- function(x) x

p_logist1 <- 
  ggplot(tibble(x=c(-2,2))) +
  aes(x=x) +
  labs(x = "x",
       title = "Logistische Regression",
       y = "Pr(y=1)=p") +
  scale_x_continuous(limits = c(-3,3)) +
  scale_y_continuous(limits = c(-0.5, 1.5)) +
  stat_function( fun = plogis) 

p_logist2 <- 
  ggplot(tibble(x=c(-2,2))) +
  aes(x=x) +
  labs(x = "x",
       title = "Lineare Regression",
       y = "Logit") +
  scale_x_continuous(limits = c(-3,3)) +
  scale_y_continuous(limits = c(-5, 5)) +
  stat_function(fun = inv_logist)  

p_logist1 + p_logist2
```




- In der logistischen Regression gilt *nicht* mehr, dass eine konstante Ver√§nderung in der UV mit einer konstanten Ver√§nderung in der AV einhergeht.
- Stattdessen geht eine konstante Ver√§nderung in der UV mit einer konstanten Ver√§nderung im *Logit* der AV einher.
- Beim logistischen Modell hier gilt, dass in der N√§he von $x=0$ die gr√∂√üte Ver√§nderung in $p$ von statten geht; je weiter weg von $x=0$, desto geringer ist die Ver√§nderung in $p$.


### Logits vs. Wahrscheinlichkeiten 

```{r logit-konvert, echo = TRUE}
#| echo: false
#| label: tbl-logit-konvert
#| tbl-cvap: "Tabelle zur Umrechnung von Logit zu Wahrscheinlichkeit (p)"
konvert_logits <-
  tibble(
    logit = c( -10, -3, 
              -2, -1, -0.5, -.25, 
              0, 
              .25, .5, 1, 2, 
              3, 10),
    p = rstanarm::invlogit(logit)
  )  %>% 
  gt() %>% 
  fmt_number(everything(), decimals = 2)

konvert_logits
```








## Aufgaben


- [Fallstudien zu Studiengeb√ºhren](https://juliasilge.com/blog/tuition-resampling/)
- [1. Modell der Fallstudie Hotel Bookings](https://www.tidymodels.org/start/case-study/)
- [Aufgaben zur logistischen Regression, PDF](https://github.com/sebastiansauer/datascience1/blob/main/Aufgaben/Thema8-Loesungen1.pdf)


##  Vertiefung

[Fallstudie Diabetes mit logististischer Regression](https://medium.com/the-researchers-guide/modelling-binary-logistic-regression-using-tidymodels-library-in-r-part-1-c1bdce0ac055)
    



