# Ensemble Lerner









## Lernsteuerung


### Lernziele 

- Sie können Algorithmen für Ensemble-Lernen erklären, d.i. Bagging, AdaBoost, XGBoost, Random Forest
- Sie wissen, anhand welche Tuningparamter man Overfitting bei diesen Algorithmen begrenzen kann
- Sie können diese Verfahren in R berechnen



### Literatur 

- Rhys, Kap. 8








## Vorbereitung


In diesem Kapitel werden folgende R-Pakete benötigt:


```r
library(tidymodels)
library(tictoc)  # Zeitmessung
```













<!-- ## Aufgaben und Vertiefung -->




## Aufgaben 

- [Fallstudie Oregon Schools](https://bcullen.rbind.io/post/2020-06-02-tidymodels-decision-tree-learning-in-r/)
- [Fallstudie Churn](https://www.gmudatamining.com/lesson-13-r-tutorial.html)
- [Fallstudie Ikea](https://juliasilge.com/blog/ikea-prices/)
- [Fallstudie Wasserquellen in Sierra Leone](https://juliasilge.com/blog/water-sources/)
- [Fallstudie Bäume in San Francisco](https://dev.to/juliasilge/tuning-random-forest-hyperparameters-in-r-with-tidytuesday-trees-data-4ilh)
- [Fallstudie Vulkanausbrüche](https://juliasilge.com/blog/multinomial-volcano-eruptions/)
- [Fallstudie Brettspiele mit XGBoost](https://juliasilge.com/blog/board-games/)
