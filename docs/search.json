[{"path":"index.html","id":"zu-diesem-buch","chapter":"Zu diesem Buch","heading":"Zu diesem Buch","text":"","code":""},{"path":"index.html","id":"was-sie-hier-lernen-und-wozu-das-gut-ist","chapter":"Zu diesem Buch","heading":"0.1 Was Sie hier lernen und wozu das gut ist","text":"Alle Welt spricht von Big Data, aber ohne die Analyse sind die großen Daten nur großes Rauschen. letztlich interessiert, sind die Erkenntnisse, die Einblicke, nicht die Daten sich.\nDabei ist es egal, ob die Daten groß oder klein sind.\nNatürlich erlauben die heutigen Datenmengen im Verbund mit leistungsfähigen Rechnern und neuen Analysemethoden ein Verständnis,\ndas vor Kurzem noch nicht möglich war.\nUnd wir stehen erst Anfang dieser Entwicklung.\nVielleicht handelt es sich bei diesem Feld um eines der dynamischsten Fachgebiete der heutigen Zeit.\nSie sind dabei: Sie lernen einiges Handwerkszeugs des “Datenwissenschaftlers”.\nWir konzentrieren uns auf das vielleicht bekannteste Teilgebiet:\nEreignisse vorhersagen auf Basis von hoch strukturierten Daten\nund geeigneter Algorithmen und Verfahren.\nNach diesem Kurs sollten Sie der Lage sein,\ntypisches Gebabbel des Fachgebiet mit Lässigkeit mitzumachen.\nAch ja, und mit einigem Erfolg Vorhersagemodelle entwickeln.","code":""},{"path":"index.html","id":"zitation","chapter":"Zu diesem Buch","heading":"0.2 Zitation","text":"Nutzen Sie diese DOI, um dieses Buch zu zitieren: ","code":""},{"path":"index.html","id":"technische-details","chapter":"Zu diesem Buch","heading":"0.3 Technische Details","text":"Diese Version des Buches wurde erstellt : 2022-06-03 15:46:10Diese Version des Buches wurde erstellt : 2022-06-03 15:46:10Die URL zu diesem Buch lautet https://sebastiansauer.github.io/datascience1/ und ist bei GitHub Pages gehostet.Die URL zu diesem Buch lautet https://sebastiansauer.github.io/datascience1/ und ist bei GitHub Pages gehostet.Lesen Sie sich die folgenden Informationen bitte gut durch: HinweiseLesen Sie sich die folgenden Informationen bitte gut durch: HinweiseDen Quellcode finden Sie diesem Github-Repo.Den Quellcode finden Sie diesem Github-Repo.Sie haben Feedback, Fehlerhinweise oder Wünsche zur Weiterentwicklung? besten stellen Sie hier einen Issue ein.Sie haben Feedback, Fehlerhinweise oder Wünsche zur Weiterentwicklung? besten stellen Sie hier einen Issue ein.Dieses Projekt steht unter der MIT-Lizenz.Dieses Projekt steht unter der MIT-Lizenz.Dieses Buch wurde RStudio mit Hilfe von bookdown geschrieben.Dieses Buch wurde RStudio mit Hilfe von bookdown geschrieben.Diese Version des Buches wurd mit der R-Version R version 4.1.3 (2022-03-10) und den folgenden Paketen erstellt:Diese Version des Buches wurd mit der R-Version R version 4.1.3 (2022-03-10) und den folgenden Paketen erstellt:","code":""},{"path":"hinweise.html","id":"hinweise","chapter":"Kapitel 1 Hinweise","heading":"Kapitel 1 Hinweise","text":"","code":""},{"path":"hinweise.html","id":"lernziele","chapter":"Kapitel 1 Hinweise","heading":"1.1 Lernziele","text":"Nach diesem Kurs sollten Siegrundlegende Konzepte des statistischen Lernens verstehen und mit R anwenden könnengängige Prognose-Algorithmen kennen, Grundzügen verstehen und mit R anwenden könnendie Güte und Grenze von Prognosemodellen einschätzen können","code":""},{"path":"hinweise.html","id":"voraussetzungen","chapter":"Kapitel 1 Hinweise","heading":"1.2 Voraussetzungen","text":"Um von diesem Kurs besten zu profitieren,\nsollten Sie folgendes Wissen mitbringen:grundlegende Kenntnisse im Umgang mit R, möglichst auch mit dem tidyversegrundlegende Kenntnisse der deskriptiven Statistikgrundlegende Kenntnis der Regressionsanalyse","code":""},{"path":"hinweise.html","id":"lernhilfen","chapter":"Kapitel 1 Hinweise","heading":"1.3 Lernhilfen","text":"","code":""},{"path":"hinweise.html","id":"software","chapter":"Kapitel 1 Hinweise","heading":"1.3.1 Software","text":"Installieren Sie R und seine Freunde.Installieren Sie die folgende R-Pakete:\ntidyverse\ntidymodels\nweitere Pakete werden im Unterricht bekannt gegeben (es schadet aber nichts, jetzt schon Pakete nach eigenem Ermessen zu installieren)\ntidyversetidymodelsweitere Pakete werden im Unterricht bekannt gegeben (es schadet aber nichts, jetzt schon Pakete nach eigenem Ermessen zu installieren)R Syntax aus dem Unterricht findet sich im Github-Repo bzw. Ordner zum jeweiligen Semester.","code":""},{"path":"hinweise.html","id":"videos","chapter":"Kapitel 1 Hinweise","heading":"1.3.2 Videos","text":"Playlist zu den ThemenAuf dem YouTube-Kanal des Autors finden sich eine Reihe von Videos mit Bezug zum Inhalt dieses Buches.","code":""},{"path":"hinweise.html","id":"online-zusammenarbeit","chapter":"Kapitel 1 Hinweise","heading":"1.3.3 Online-Zusammenarbeit","text":"Hier finden Sie einige Werkzeuge,\ndie das Online-Zusammenarbeiten vereinfachen:Frag-Jetzt-Raum zum anonymen Fragen stellen während des Unterrichts. Der Keycode wird Ihnen vom Dozenten bereitgestellt.Padlet zum einfachen (und anonymen) Hochladen von Arbeitsergebnissen der Studentis im Unterricht. Wir nutzen es als eine Art Pinwand zum Sammeln von Arbeitsbeiträgen. Die Zugangsdaten stellt Ihnen der Dozent bereit.","code":""},{"path":"hinweise.html","id":"modulzeitplan","chapter":"Kapitel 1 Hinweise","heading":"1.4 Modulzeitplan","text":"","code":""},{"path":"hinweise.html","id":"literatur","chapter":"Kapitel 1 Hinweise","heading":"1.5 Literatur","text":"Zentrale Kursliteratur für die theoretischen Konzepte ist Rhys (2020).\nBitte prüfen Sie, ob das Buch einer Bibliothek verfügbar ist.\nDie praktische Umsetzung R basiert auf Silge Kuhn (2022) (dem “Tidymodels-Konzept”);\ndas Buch ist frei online verfügbar.Eine gute Ergänzung ist das Lehrbuch von Timbers, Campbell, Lee (2022),\nwelches grundlegende Data-Science-Konzepte erläutert und mit tidymodels umsetzt.James et al. (2021) haben ein weithin renommiertes und sehr bekanntes Buch verfasst.\nEs ist allerdings etwas anspruchsvoller aus Rhys (2020),\ndaher steht es nicht im Fokus dieses Kurses,\naber einige Schwenker zu Inhalten von James et al. (2021) gibt es. Schauen Sie mal rein,\ndas Buch ist gut!einigen Punkten ist weiterhin Sauer (2019) hilfreich;\ndas Buch ist über SpringerLink Ihrer Hochschul-Bibliothek verfügbar. Eine gute Ergänzung ist das “Lab-Buch” von Hvitfeldt (2022).\ndem Buch wird das Lehrbuch James et al. (2021) Tidymodels-Konzepte übersetzt; durchaus nett!","code":""},{"path":"hinweise.html","id":"faq","chapter":"Kapitel 1 Hinweise","heading":"1.6 FAQ","text":"Folien\nFrage: Gibt es ein Folienskript?\nAntwort: Wo es einfache, gute Literatur gibt, gibt es kein Skript. Wo es keine gute oder keine einfach zugängliche Literatur gibt, dort gibt es ein Skript.\nFrage: Gibt es ein Folienskript?Antwort: Wo es einfache, gute Literatur gibt, gibt es kein Skript. Wo es keine gute oder keine einfach zugängliche Literatur gibt, dort gibt es ein Skript.Englisch\nIst die Literatur auf Englisch?\nJa. Allerdings ist die Literatur gut zugänglich. Das Englisch ist nicht schwer. Bedenken Sie: Englisch ist die lingua franca Wissenschaft und Wirtschaft. Ein solides Verständnis englischer (geschriebener) Sprache ist für eine gute Ausbildung unerlässlich. Zu dem sollte die Kursliteratur fachlich passende und gute Bücher umfassen; oft sind das englische Titel.\nIst die Literatur auf Englisch?Ja. Allerdings ist die Literatur gut zugänglich. Das Englisch ist nicht schwer. Bedenken Sie: Englisch ist die lingua franca Wissenschaft und Wirtschaft. Ein solides Verständnis englischer (geschriebener) Sprache ist für eine gute Ausbildung unerlässlich. Zu dem sollte die Kursliteratur fachlich passende und gute Bücher umfassen; oft sind das englische Titel.Anstrengend\nIst der Kurs sehr anstrengend, aufwändig?\nDer Kurs hat ein mittleres Anspruchsniveau.\nIst der Kurs sehr anstrengend, aufwändig?Der Kurs hat ein mittleres Anspruchsniveau.Mathe\nMuss man ein Mathe-Crack sein, um eine gute Note zu erreichen?\nNein. Mathe steht nicht im Vordergrund. Schauen Sie sich die Literatur , sie werden wenig Mathe darin finden.\nMuss man ein Mathe-Crack sein, um eine gute Note zu erreichen?Nein. Mathe steht nicht im Vordergrund. Schauen Sie sich die Literatur , sie werden wenig Mathe darin finden.Prüfungsliteratur\nWelche Literatur ist prüfungsrelevant?\nDie Prüfung ist angewandt, z.B. ein Prognosewettbewerb. Es wird keine Klausur geben, der reines Wissen abgefragt wird.\nWelche Literatur ist prüfungsrelevant?Die Prüfung ist angewandt, z.B. ein Prognosewettbewerb. Es wird keine Klausur geben, der reines Wissen abgefragt wird.Nur R?\nWird nur R dem Kurs gelehrt? Andere Programmiersprachen sind doch auch wichtig.\nder Datenanalyse gibt es zwei zentrale Programmiersprachen, R und Python. Beide sind gut und beide werden viel verwendet. einer Grundausbildung sollte man sich auf eine Sprache begrenzen, da sonst den Sprachen zu viel Zeit eingeräumt werden muss. Wichtiger als eine zweite Programmiersprache zu lernen, mit der man nicht viel mehr kann als mit der ersten, ist es, die Inhalte des Fachs zu lernen.\nWird nur R dem Kurs gelehrt? Andere Programmiersprachen sind doch auch wichtig.der Datenanalyse gibt es zwei zentrale Programmiersprachen, R und Python. Beide sind gut und beide werden viel verwendet. einer Grundausbildung sollte man sich auf eine Sprache begrenzen, da sonst den Sprachen zu viel Zeit eingeräumt werden muss. Wichtiger als eine zweite Programmiersprache zu lernen, mit der man nicht viel mehr kann als mit der ersten, ist es, die Inhalte des Fachs zu lernen.","code":""},{"path":"prüfung.html","id":"prüfung","chapter":"Kapitel 2 Prüfung","heading":"Kapitel 2 Prüfung","text":"","code":""},{"path":"prüfung.html","id":"prüfungleistung","chapter":"Kapitel 2 Prüfung","heading":"2.1 Prüfungleistung","text":"Die Prüfungsleistung besteht aus einem Prognosewettbewerb.","code":""},{"path":"prüfung.html","id":"tldr-zusammenfassung","chapter":"Kapitel 2 Prüfung","heading":"2.2 tl;dr: Zusammenfassung","text":"Vorhersagen sind eine praktische Sache, zumindest wenn Sie stimmen.\nWenn Sie den DAX-Stand von morgen genau vorhersagen können,\nrufen Sie mich bitte sofort .\nGenau das ist Ihre Aufgabe dieser Prüfungsleistung:\nSie sollen Werte vorhersagen.Etwas konkreter: Stellen Sie sich ein paar Studentis vor.\nVon allen wissen Sie, wie lange die Person für die Statistikklausur gelernt hat.\nAußerdem wissen Sie die Motivation jeder Person und vielleicht noch ein paar noten-relevante Infos.\nUnd Sie wissen die Note jeder Person der Statistikklausur.\nAuf dieser Basis fragt sie ein Student (Alois),\nder im kommenden Semester die Prüfung Statistik schreiben muss :\n“Sag mal, wenn ich 100 Stunden lerne und mittel motiviert bin (bestenfalls), welche Note kann ich dann erwarten?”.\nMit Hilfe Ihrer Analyse können Sie diese Frage (und andere) beantworten.\nNatürlich könnten Sie es sich leicht machen und antworten:\n“Mei, der Notendurchschnitt war beim letzten Mal 2.7.\nAlso ist 2.7 kein ganz doofer Tipp für deine Note.”\nJa, das ist keine doofe Antwort, aber man genauere Prognose machen,\nwenn man es geschickt anstellt.\nDa hilft Ihnen die Statistik (doch, wirklich).Kurz gesagt gehen Sie vor:\nImportieren Sie die Daten R, starten Sie die nötigen R-Pakete und\nschauen Sie sich die Daten unter verschiedenen Blickwinkeln .\nDann nehmen Sie die vielversprechendsten Prädiktoren ein Regressionsmodell und schauen sich ,\nwie gut die Vorhersage ist.\nWiederholen Sie das ein paar Mal, bis Sie ein Modell haben, das Sie brauchbar finden.\nMit diesem Modell sagen Sie dann die Noten der neuen Studis (Alois und Co.) vorher.\nJe genauer Ihre Vorhersage, desto besser ist Ihr Prüfungsergebnis.","code":""},{"path":"prüfung.html","id":"vorhersage","chapter":"Kapitel 2 Prüfung","heading":"2.3 Vorhersage","text":"Neben der erklärenden, rückwärtsgerichteten Modellierung spielt insbesondere der Praxis die vorhersageorientierte Modellierung eine wichtige Rolle:\nZiel ist es, bei gegebenen, neuen Beobachtungen die noch unbekannten Werte der Zielvariablen \\(y\\) vorherzusagen, z.B. für neue Kunden auf Basis von soziodemographischen Daten den Kundenwert – möglichst genau – zu prognostizieren.\nDies geschieht auf Basis der vorhandenen Daten der Bestandskunden,\nd.h. inklusive des für diese Kunden bekannten Kundenwertes.Ihnen werden zwei Teildatenmengen zur Verfügung gestellt:\nZum einen gibt es die Trainingsdaten (auch Lerndaten genannt) und zum anderen gibt es Anwendungsdaten (auch Testdaten genannt), auf die man das Modell anwendet.Bei den Trainingsdaten (Train-Sample) liegen sowohl die erklärenden Variablen \\({\\bf{x}} = (x_1, x_2, \\ldots, x_n)\\) als auch die Zielvariable \\(y\\) vor.\nAuf diesen Trainingsdaten wird das Modell \\(y=f({x})+\\epsilon = f(x_1, x_2, \\ldots, x_n)+\\epsilon\\) gebildet und durch \\(\\hat{f}(\\cdot)\\) geschätzt. Es ist also die Variable \\(y\\) vorherszusagen.Bei den Trainingsdaten (Train-Sample) liegen sowohl die erklärenden Variablen \\({\\bf{x}} = (x_1, x_2, \\ldots, x_n)\\) als auch die Zielvariable \\(y\\) vor.\nAuf diesen Trainingsdaten wird das Modell \\(y=f({x})+\\epsilon = f(x_1, x_2, \\ldots, x_n)+\\epsilon\\) gebildet und durch \\(\\hat{f}(\\cdot)\\) geschätzt. Es ist also die Variable \\(y\\) vorherszusagen.Dieses geschätzte Modell (\\(\\hat{f}(\\cdot)\\)) wird auf die Anwendungsdaten \\(\\bf{x}_0\\), für die (Ihnen) die Werte der Zielvariable \\(y\\) unbekannt sind,\nangewendet, d.h., es wird \\(\\hat{y}_0 :=\\hat{f}({\\bf{x}}_0)\\) berechnet.\nDer unbekannte Wert \\(y_0\\) der Zielvariable \\(y\\) wird durch \\(\\hat{y}_0\\) prognostiziert.Dieses geschätzte Modell (\\(\\hat{f}(\\cdot)\\)) wird auf die Anwendungsdaten \\(\\bf{x}_0\\), für die (Ihnen) die Werte der Zielvariable \\(y\\) unbekannt sind,\nangewendet, d.h., es wird \\(\\hat{y}_0 :=\\hat{f}({\\bf{x}}_0)\\) berechnet.\nDer unbekannte Wert \\(y_0\\) der Zielvariable \\(y\\) wird durch \\(\\hat{y}_0\\) prognostiziert.Liegt zu einem noch späteren Zeitpunkt der eingetroffene Wert \\(y_0\\) der Zielvariable \\(y\\) vor,\nkann die eigene Vorhersage \\(\\hat{y}_0\\) evaluiert werden,\nd.h. z.B. kann der Fehler \\(e=y_0-\\hat{y}_0\\) zwischen prognostiziertem Wert \\(\\hat{y}_0\\) und wahrem Wert \\(y_0\\) analysiert werden.der praktischen Anwendung können zeitlich drei aufeinanderfolgende Schritte unterschieden werden (vergleiche oben):die Trainingsphase, d.h., die Phase für die sowohl erklärende (\\({\\bf{x}}\\)) als auch die erklärte Variable (\\(y\\)) bekannt sind.\nHier wird das Modell geschätzt (gelernt): \\(\\hat{f}(\\bf{x})\\). Dafür wird der Trainingsdatensatz genutzt.die Trainingsphase, d.h., die Phase für die sowohl erklärende (\\({\\bf{x}}\\)) als auch die erklärte Variable (\\(y\\)) bekannt sind.\nHier wird das Modell geschätzt (gelernt): \\(\\hat{f}(\\bf{x})\\). Dafür wird der Trainingsdatensatz genutzt.der folgenden Anwendungsphase sind nur die erklärenden Variablen (\\({\\bf{x_0}}\\)) bekannt, nicht \\(y_0\\).\nAuf Basis der Ergebnisses aus dem 1. Schritt wird \\(\\hat{y}_0 :=\\hat{f}({\\bf{x}}_0)\\) prognostiziert.der folgenden Anwendungsphase sind nur die erklärenden Variablen (\\({\\bf{x_0}}\\)) bekannt, nicht \\(y_0\\).\nAuf Basis der Ergebnisses aus dem 1. Schritt wird \\(\\hat{y}_0 :=\\hat{f}({\\bf{x}}_0)\\) prognostiziert.Evt. gibt es später noch die Evaluierungsphase, für die dann auch die Zielvariable (\\(y_0\\)) bekannt ist, dass die Vorhersagegüte des Modells überprüft werden kann.Evt. gibt es später noch die Evaluierungsphase, für die dann auch die Zielvariable (\\(y_0\\)) bekannt ist, dass die Vorhersagegüte des Modells überprüft werden kann.Im Computer kann man dieses Anwendungsszenario simulieren:\nman teilt die Datenmenge zufällig eine Lern- bzw. Trainingsstichprobe (Trainingsdaten; \\((\\bf{x},y)\\)) und eine Teststichprobe (Anwendungsdaten, \\((\\bf{x_0})\\)) auf:\nDie Modellierung erfolgt auf den Trainingsdaten.\nDas Modell wird angewendet auf die Testdaten (Anwendungsdaten).\nDa man hier aber auch die Zielvariable (\\(y_0\\)) kennt, kann damit das Modell evaluiert werden.","code":""},{"path":"prüfung.html","id":"hauptziel-genaue-prognose","chapter":"Kapitel 2 Prüfung","heading":"2.4 Hauptziel: Genaue Prognose","text":"Ihre Aufgabe ist: Spielen Sie den Data-Scientist!\nKonstruieren Sie ein Modell auf Basis der Trainingsdaten \\((\\bf{x},y\\))\nund sagen Sie für die Anwendungsdaten (\\(\\bf{x_0}\\)) die Zielvariable möglichst genau voraus (\\(\\hat{y}_0\\)).Ihr(e) Dozent*kennt den Wert der Zielvariable (\\(y_0\\)). Sie nicht.Von zwei Prognosemodellen zum gleichen Datensatz ist dasjenige Modell besser,\ndas weniger Vorhersagefehler aufweist (im Test-Datensatz), also genauer vorhersagt.\nKurz gesagt: Genauer ist besser.","code":""},{"path":"prüfung.html","id":"zum-aufbau-ihrer-prognosedatei-im-csv-format","chapter":"Kapitel 2 Prüfung","heading":"2.5 Zum Aufbau Ihrer Prognosedatei im CSV-Format","text":"Die CSV-Datei muss aus genau zwei Spalten mit (exakt) folgenden Spaltennamen bestehen:id: Den ID-Wert jedes vorhergesagten Wertespred: Der vorhergesagte Wert.Umlaute sind zu ersetzen (also Süß wird Suess etc.).Umlaute sind zu ersetzen (also Süß wird Suess etc.).Die CSV-Datei muss als Spaltentrennzeichen ein Komma verwenden und als Dezimaltrennzeichen einen Punkt (d.h. also die Standardformatierung einer CSV-Datei; nicht die deutsche Formatierung).Die CSV-Datei muss als Spaltentrennzeichen ein Komma verwenden und als Dezimaltrennzeichen einen Punkt (d.h. also die Standardformatierung einer CSV-Datei; nicht die deutsche Formatierung).Die CSV-Datei muss genau die Anzahl Zeilen aufweisen, die der Zeilenlänge im Test-Datensatz entspricht.Die CSV-Datei muss genau die Anzahl Zeilen aufweisen, die der Zeilenlänge im Test-Datensatz entspricht.Prüfen Sie, dass Ihre CSV-Datei sich problemlos lesen lässt.\nFalls keine (funktionstüchtige) CSV-Datei eingereicht (hochgeladen) wurde, ist die Prüfung nicht bestanden.\nTipp: Öffnen Sie die CSV-Datei mit einem Texteditor und schauen Sie sich , ob alles vernünftig aussieht.\nAchtung: Öffnen Sie die CSV-Datei besser nicht mit Excel, da Excel einen Bug hat,\nder CSV-Dateien verfälschen kann auch ohne dass man die Datei speichert.Prüfen Sie, dass Ihre CSV-Datei sich problemlos lesen lässt.\nFalls keine (funktionstüchtige) CSV-Datei eingereicht (hochgeladen) wurde, ist die Prüfung nicht bestanden.\nTipp: Öffnen Sie die CSV-Datei mit einem Texteditor und schauen Sie sich , ob alles vernünftig aussieht.\nAchtung: Öffnen Sie die CSV-Datei besser nicht mit Excel, da Excel einen Bug hat,\nder CSV-Dateien verfälschen kann auch ohne dass man die Datei speichert.","code":""},{"path":"prüfung.html","id":"einzureichende-dateien","chapter":"Kapitel 2 Prüfung","heading":"2.6 Einzureichende Dateien","text":"Folgende* Dateiarten* sind einzureichen:\nPrognose: Ihre Prognose-Datei (CSV-Datei)\nAnalyse: Ihr Analyseskript (R-, Rmd- oder Rmd-Notebook-Datei)\nFolgende* Dateiarten* sind einzureichen:Prognose: Ihre Prognose-Datei (CSV-Datei)Analyse: Ihr Analyseskript (R-, Rmd- oder Rmd-Notebook-Datei)Weitere Dateien sind nicht einzureichen.Weitere Dateien sind nicht einzureichen.Komprimieren Sie die Dateien nicht (z.B. via zip).Komprimieren Sie die Dateien nicht (z.B. via zip).Der Name jeder eingereichnte Datei muss wie folgt lauten: Nachname_Vorname_Matrikelnummer_Dateiart.Endung. Beispiel: Sauer_Sebastian_0123456_Prognose.csv bzw. Sauer_Sebastian_0123456_Analyse.Rmd.Der Name jeder eingereichnte Datei muss wie folgt lauten: Nachname_Vorname_Matrikelnummer_Dateiart.Endung. Beispiel: Sauer_Sebastian_0123456_Prognose.csv bzw. Sauer_Sebastian_0123456_Analyse.Rmd.","code":""},{"path":"prüfung.html","id":"gliederung-ihrer-analyse","chapter":"Kapitel 2 Prüfung","heading":"2.7 Gliederung Ihrer Analyse","text":"Ihr Analysedokument stellt alle Ihre Schritte vor, die Sie im Rahmen der Bearbeitung der Prüfungsaufgabe unternommen haben,\nzumindest die Analyse der Daten betrifft.Das Dokument mischt drei Textarten: R-Syntax, R-Ausgaben sowie Prosa (normale Sprache).\nAlle drei Aspekte sind gleichermaßen wichtig und unabdingbar für diese Analyse.Wenn Sie das Dokument als R-Markdown-Datei (Rmd-Datei) anlegen,\nmüssen Sie R-Code einem “R-Chunk” auszeichnen. Prosa wird Rmd-Datei als Standard gesehen,\nsie brauchen ihn nicht extra auszuzeichnen (für R-Notebook-Dateien gilt das Gleiche).\nR-Skript-Dateien ist es umgekehrt: Sie müssen R-Code nicht extra auszeichnen,\nda R-Skripten R als “Standard-Text” gesehen wird. Hingegen müssen Sie Prosa als Kommentar einfügen.\nEs bleibt Ihnen überlassen, für welche Variante (R-, Rmd- oder R-Notebook) Sie sich entscheiden.\nKeine Option wird als besser oder schlechter gewertet (vermutlich ist Rmd für Sie einfachsten).Sie können Ihr Analysedokument z.B. gliedern:Die Gliederung ist kein Muss; andere Gliederung sind auch möglich.\nEntscheidend ist die fachliche Angemessenheit und die Reproduzierbarkiet.","code":"1. Forschungsfrage und Hintergrund (Beschreiben Sie kurz, worum es geht)\n2. Vorbereitung (Pakete laden, Daten importieren, etc.)\n3. Explorative Datenanalyse (Untersuchen Sie den Datensatz nach Auffälligkeiten, die Sie dann beim Modellieren nutzen)\n4. Modellieren (Mehrere Prognosemodelle, z.B. via `lm(av ~ uv)`)\n5. Vorhersagen (Vorhersage der Test-Daten anhand des besten Vorhersagemodells und Einreichen)"},{"path":"prüfung.html","id":"abschnitt-forschungsfrage-und-hintergrund","chapter":"Kapitel 2 Prüfung","heading":"2.7.1 Abschnitt Forschungsfrage und Hintergrund","text":"diesem Abschnitt passiert noch keine Statistik bzw. keine Analyse.\nStattdessen stellen Sie “normaler Sprache”,\nalso ohne intensiven Gebrauch vom (statistischem) Fachvokabular dar,\nZiel und Hintergrund der Analyse ist.\nSie können als Ziel bzw. Hintergrund den formalen Aspekt der Prüfung anführen,\nwichiger sind aber inhaltliche bzw. fachliche Überlegungen: Worum geht es der Analyse?\nWarum ist die Frage wichtig?\nwird untersucht? Anhand welcher Methodik wird die Frage untersucht?","code":""},{"path":"prüfung.html","id":"vorbereitung","chapter":"Kapitel 2 Prüfung","heading":"2.7.2 Vorbereitung","text":"diesem Abschnitt Ihres Analysedokuments führen Sie die technische Vorbereitung durch.\nDas betrifft vor allem das Importieren der Daten und das Starten aller R-Pakete,\ndie der Analyse verwendet werden.","code":""},{"path":"prüfung.html","id":"explorative-datenanalyse","chapter":"Kapitel 2 Prüfung","heading":"2.7.3 Explorative Datenanalyse","text":"Die explorative Datenanalyse (EDA) meint sowohl die deskriptive Statistik als auch die Datenvisualisierung.\nTypische Schritte sind: das Bearbeiten (oder Entfernen) von Extremwerten und fehlenden Werten,\ndie Untersuchung von Verteilungsformen oder das Suchen nach Mustern (Korrelationen, Gruppenunterschieden).\nEin nützliches Ergebnis ist z.B. zu erkennen, welche Variablen sich als Prädiktoren eignen (für den nächsten Abschnitt der Modellierung).\nZiel ist, dass Sie den folgenden Schritt vorbereiten,\nalso Schritte unternehmen, damit Sie die AV möglichst gut vorhersagen können.","code":""},{"path":"prüfung.html","id":"modellierung","chapter":"Kapitel 2 Prüfung","heading":"2.7.4 Modellierung","text":"diesem Schritt berechnen Sie Prognosemodelle. Das sind oft lineare Modelle, also etwa lm(av ~ uv).\nEs empfiehlt sich, mehrere Modelle zu berechnen und zu schauen,\nwelches dieser Kandidaten besten ist.\nDie Güte eines Prognosemodells bemisst sich letztlich nur der Präzision der Vorhersage neuer Daten,\nalso des Test-Datensatzes.\nWie gut Ihre Vorhersagen also wirklich sind, erfahren Sie erst mit der Notenbekanntgabe.\nAllerdings können Sie die Trainingsdaten nutzen, um die Güte Ihrere Modell abzuschätzen.","code":""},{"path":"prüfung.html","id":"vorhersagen","chapter":"Kapitel 2 Prüfung","heading":"2.7.5 Vorhersagen","text":"Schließlich entscheiden Sie sich für einen Modellkandidaten.\nDiesen Modellkandidaten nehmen Sie , um die (Ihenn unbekannten) Werte der AV (Zielvariablen) vorherzusagen.\nDiese Vorhersagen - zusammen mit der ID für jede Vorhersagen - speichern Sie als (reguläre) CSV-Datei ab\nund reichen Sie als Ihre Prüfungsleistung ein, zusammen mit Ihrer Analysedatei.","code":""},{"path":"prüfung.html","id":"tipps","chapter":"Kapitel 2 Prüfung","heading":"2.8 Tipps","text":"","code":""},{"path":"prüfung.html","id":"tipps-für-eine-gute-prognose","chapter":"Kapitel 2 Prüfung","heading":"2.8.1 Tipps für eine gute Prognose","text":"Schauen Sie die Literatur.Schauen Sie die Literatur.Evtl. kann eine Datenvorverarbeitung (Variablentransformation, z.B. \\(\\log()\\) oder die Elimination von Ausreißern) helfen.Evtl. kann eine Datenvorverarbeitung (Variablentransformation, z.B. \\(\\log()\\) oder die Elimination von Ausreißern) helfen.Überlegen Sie sich Kriterien zur Modell- und/ oder Variablenauswahl. Auch hierfür gibt es Algorithmen und R-Funktionen.Überlegen Sie sich Kriterien zur Modell- und/ oder Variablenauswahl. Auch hierfür gibt es Algorithmen und R-Funktionen.Vermeiden Sie Über-Anpassung (Overfitting).Vermeiden Sie Über-Anpassung (Overfitting).Vermeiden Sie viele fehlende Werte bei Ihrer Prognose. Fehlende Werte werden bei der Benotung mit dem Mittelwert (der vorhandenen Prognosewerte Ihrer Einreichung) aufgefüllt.Vermeiden Sie viele fehlende Werte bei Ihrer Prognose. Fehlende Werte werden bei der Benotung mit dem Mittelwert (der vorhandenen Prognosewerte Ihrer Einreichung) aufgefüllt.Arbeiten Sie die bereitgestellten Fallstudien durch. Wenn Sie mehr tun möchten, finden Sie im Internet eine Fülle von weiteren Fallstudien.Arbeiten Sie die bereitgestellten Fallstudien durch. Wenn Sie mehr tun möchten, finden Sie im Internet eine Fülle von weiteren Fallstudien.","code":""},{"path":"prüfung.html","id":"tipps-zur-datenverarbeitung","chapter":"Kapitel 2 Prüfung","heading":"2.8.2 Tipps zur Datenverarbeitung","text":"Ein “deutsches” Excel kann Standard-CSV-Dateien nicht ohne Weiteres lesen. Online-Dienste wie Google Sheets können dies allerdings.","code":""},{"path":"prüfung.html","id":"tipps-zum-aufbau-des-analyseskripts","chapter":"Kapitel 2 Prüfung","heading":"2.8.3 Tipps zum Aufbau des Analyseskripts","text":"Zu Beginn des Skripts sollten alle verwendeten R-Pakete mittels library() gestartet werden.Zu Beginn des Skripts sollten alle verwendeten R-Pakete mittels library() gestartet werden.Zu Beginn des Skripts sollten die Daten von der vom Dozenten bereitgestellten URL importiert werden (nicht von der eigenen Festplatte, da das Skript sonst bei Dritten, wie Ihrem Prüfer, nicht lauffähig ist).Zu Beginn des Skripts sollten die Daten von der vom Dozenten bereitgestellten URL importiert werden (nicht von der eigenen Festplatte, da das Skript sonst bei Dritten, wie Ihrem Prüfer, nicht lauffähig ist).","code":""},{"path":"prüfung.html","id":"sonstiges","chapter":"Kapitel 2 Prüfung","heading":"2.8.4 Sonstiges","text":"Legen Sie regelmäßig Sicherheitskopien Ihrer Arbeit (ggf. auf einem anderen Datenträger).Legen Sie regelmäßig Sicherheitskopien Ihrer Arbeit (ggf. auf einem anderen Datenträger).Achten Sie darauf, dass Sie nicht durcheinander kommen, welcher Datei der aktuelle Stand Ihrer Arbeit liegt.Achten Sie darauf, dass Sie nicht durcheinander kommen, welcher Datei der aktuelle Stand Ihrer Arbeit liegt.","code":""},{"path":"prüfung.html","id":"bewertung","chapter":"Kapitel 2 Prüfung","heading":"2.9 Bewertung","text":"","code":""},{"path":"prüfung.html","id":"kriterien","chapter":"Kapitel 2 Prüfung","heading":"2.9.1 Kriterien","text":"Es gibt drei Bewertungskriterien:\nFormalia: u.. Reproduzierbarkeit der Analyse, Lesbarkeit der Syntax, Übersichtlichkeit der Analyse.\nMethode: u.. methodischer Anspruch und Korrektheit der Explorativen Datenanalyse, Datenvorverarbeitung, Variablenauswahl und Modellierungsmethode.\nInhalt: Vorhersagegüte.\nEs gibt drei Bewertungskriterien:Formalia: u.. Reproduzierbarkeit der Analyse, Lesbarkeit der Syntax, Übersichtlichkeit der Analyse.Formalia: u.. Reproduzierbarkeit der Analyse, Lesbarkeit der Syntax, Übersichtlichkeit der Analyse.Methode: u.. methodischer Anspruch und Korrektheit der Explorativen Datenanalyse, Datenvorverarbeitung, Variablenauswahl und Modellierungsmethode.Methode: u.. methodischer Anspruch und Korrektheit der Explorativen Datenanalyse, Datenvorverarbeitung, Variablenauswahl und Modellierungsmethode.Inhalt: Vorhersagegüte.Inhalt: Vorhersagegüte.Das zentrale Bewertungskriterium ist Inhalt; die übrigen beiden Kriterien fließen nur bei besonders guter oder schlechter Leistung die Gesamtnote ein.Das zentrale Bewertungskriterium ist Inhalt; die übrigen beiden Kriterien fließen nur bei besonders guter oder schlechter Leistung die Gesamtnote ein.Die quantitative Datenanalyse Durchführung und Interpretation ist der Schwerpunkt dieser Arbeit. Zufälliges identisches Vorgehen, z.B. im R Code, ist sehr unwahrscheinlich und kann als Plagiat bewertet werden.Die quantitative Datenanalyse Durchführung und Interpretation ist der Schwerpunkt dieser Arbeit. Zufälliges identisches Vorgehen, z.B. im R Code, ist sehr unwahrscheinlich und kann als Plagiat bewertet werden.Die Gesamtnote muss sich nicht als arithmetischer Mittelwert der Teilnoten ergeben.Die Gesamtnote muss sich nicht als arithmetischer Mittelwert der Teilnoten ergeben.Es werden keine Teilnoten vergeben, sondern nur eine Gesamtnote wird vergeben.Es werden keine Teilnoten vergeben, sondern nur eine Gesamtnote wird vergeben.Es werden keine Hinweise vergeben, stattdessen gibt es einen Überblick typischen Fehlern.Es werden keine Hinweise vergeben, stattdessen gibt es einen Überblick typischen Fehlern.Es wird keine Musterlösugn veröffentlicht, um nachfolgende Kohorten nicht zu bevorteilen bzw. die aktuelle Kohorte nicht zu benachteiligen.Es wird keine Musterlösugn veröffentlicht, um nachfolgende Kohorten nicht zu bevorteilen bzw. die aktuelle Kohorte nicht zu benachteiligen.","code":""},{"path":"prüfung.html","id":"kennzahl-der-modellgüte","chapter":"Kapitel 2 Prüfung","heading":"2.9.2 Kennzahl der Modellgüte","text":"Die Güte der Vorhersage wird anhand des mittleren Absolutfehlers (mae) bemessen:\\[\\text{mae} = \\frac{1}{n} \\sum_{=1}^n|(y_i - \\hat{y}_i)|\\]","code":""},{"path":"prüfung.html","id":"notenstufen","chapter":"Kapitel 2 Prüfung","heading":"2.9.3 Notenstufen","text":"Zur Vorhersagegüte: Die Vorhersagegüte eines einfachen Minimalmodells entspricht einer \\(4,0\\), die eines Referenzmodells des Dozenten einer \\(2,0\\).Ihre Bewertung erfolgt entsprechend Ihrer Vorhersagegüte, d.h., sind Sie besser als das Referenzmodell erhalten Sie hier diesem Teilaspekt eine bessere Note als \\(2,0\\)!","code":""},{"path":"prüfung.html","id":"bewertungsprozess","chapter":"Kapitel 2 Prüfung","heading":"2.9.4 Bewertungsprozess","text":"Der Gutachter legt im Nachgang der Prüfung alle Teilnehmis ihre jeweilige Wert der Kennzahl der Modellgüte offen.\nAußerdem werden die vorherzusagenden Daten veröffentlicht\nsowie die Grenzwerte für jede Notenstufe.\nAuf dieser Basis ist es allen Teilnehmis möglich,\ndie Korrektheit Ihrer Note zu überprüfen.","code":""},{"path":"prüfung.html","id":"hinweise-1","chapter":"Kapitel 2 Prüfung","heading":"2.10 Hinweise","text":"Sie haben freie Methodenwahl bei der Modellierung und Vorverarbeitung. Nutzen Sie den Stoff wie im Unterricht gelernt; Sie können aber auch auf weitere Inhalte, die nicht im Unterricht behandelt wurden, zugreifen.Eine Einführung verschiedene Methoden gibt es z.B. bei Sebastian Sauer (2019): Moderne Datenanalyse mit R1 aber auch bei Max Kuhn und Julia Silge (2021): Tidy Modeling R.2. Die Bücher beinhalten jeweils Beispiele und Anwendung mit R.Auch ist es Ihnen überlassen, welche Variablen Sie zur Modellierung heranziehen – und ob Sie diese eventuell vorverarbeiten, d.h., transformieren, zusammenfassen, Ausreißer bereinigen o.Ä.. Denken Sie nur daran, die Datentransformation, die Sie auf den Trainingsdaten durchführen, auch auf den Testdaten (Anwendungsdaten) durchzuführen.Hinweise zur Modellwahl usw. gibt es auch erwähnter Literatur, aber auch vielen Büchern zum Thema Data-Science.Alles, Sie tun, Datenvorverarbeitung, Modellierung und Anwenden, muss transparent und reproduzierbar sein. Im Übrigen lautet die Aufgabe:\nFinden Sie ein Modell, von dem Sie glauben, dass es die Testdaten gut vorhersagt. \\(\\hat{y}=42\\) ist zwar eine schöne Antwort,\ntrifft die Wirklichkeit aber leider nicht immer.\nEine gute Modellierung auf den Trainingsdaten (z.B. hohes \\(R^2\\)) bedeutet nicht zwangsläufig eine gute Vorhersage (Test-Set).","code":""},{"path":"prüfung.html","id":"formalia","chapter":"Kapitel 2 Prüfung","heading":"2.11 Formalia","text":"Es sind nur Einzelarbeiten zulässig.Es sind nur Einzelarbeiten zulässig.der Analyse muss als Ausgangspunkt der vom/von der Dozenten/bereitgestellten Datensatz genutzt werden.der Analyse muss als Ausgangspunkt der vom/von der Dozenten/bereitgestellten Datensatz genutzt werden.Alle Analyseschritte bzw. alle Veränderungen den Daten müssen im (eingereichten) Analyseskript nachvollziehbar (transparent und reproduzierbar) aufgeführt sein. Das Analyseskript ist als R-Skript, Rmd-Datei oder Rmd-Notebook-Datei abzugeben. Sie können die bereitgestellte Vorlage als Analyseskript nutzen (Template-Dokumentation-Vorhersagemodellierung.Rmd).Alle Analyseschritte bzw. alle Veränderungen den Daten müssen im (eingereichten) Analyseskript nachvollziehbar (transparent und reproduzierbar) aufgeführt sein. Das Analyseskript ist als R-Skript, Rmd-Datei oder Rmd-Notebook-Datei abzugeben. Sie können die bereitgestellte Vorlage als Analyseskript nutzen (Template-Dokumentation-Vorhersagemodellierung.Rmd).Das Analyseskript muss funktionstüchtig für den Prüfer sein: Alle Befehle müssen ohne Fehlermeldung durchlaufen (abgesehen von etwaiger Installation fehlender Pakete).Das Analyseskript muss funktionstüchtig für den Prüfer sein: Alle Befehle müssen ohne Fehlermeldung durchlaufen (abgesehen von etwaiger Installation fehlender Pakete).Es dürfen keine weiteren Informationen (Daten) als die vom Dozenten ausgegebenen verwendet werden. Sonstige Hilfe (z.B. von Dritten) ist ebenfalls unzulässig.Es dürfen keine weiteren Informationen (Daten) als die vom Dozenten ausgegebenen verwendet werden. Sonstige Hilfe (z.B. von Dritten) ist ebenfalls unzulässig.Nichtbeachtung der für dieses Modul formulierten Regeln kann zu Nichtbestehen oder Punkteabzug führen.Nichtbeachtung der für dieses Modul formulierten Regeln kann zu Nichtbestehen oder Punkteabzug führen.Der Schwerpunkt dieser Hausarbeit liegt auf der quantitativen Modellierung, der formale Anspruch liegt daher unter dem von anderen Hausarbeiten.Der Schwerpunkt dieser Hausarbeit liegt auf der quantitativen Modellierung, der formale Anspruch liegt daher unter dem von anderen Hausarbeiten.Es muss keine Literatur zitiert werden.Es muss keine Literatur zitiert werden.Ein ausgedrucktes Exemplar muss nicht abgegeben werden.Ein ausgedrucktes Exemplar muss nicht abgegeben werden.Während der Prüfungsphase werden keine inhaltlichen Fragen (“wie macht man nochmal eine Log-Transformation?”) und keine technischen Fragen (“wie installiert man nochmal ein R-Paket?”) beantwortet.Während der Prüfungsphase werden keine inhaltlichen Fragen (“wie macht man nochmal eine Log-Transformation?”) und keine technischen Fragen (“wie installiert man nochmal ein R-Paket?”) beantwortet.","code":""},{"path":"prüfung.html","id":"ich-brauche-hilfe","chapter":"Kapitel 2 Prüfung","heading":"2.12 Ich brauche Hilfe!","text":"","code":""},{"path":"prüfung.html","id":"wo-finde-ich-beispiele-und-vorlagen","chapter":"Kapitel 2 Prüfung","heading":"2.12.1 Wo finde ich Beispiele und Vorlagen?","text":"Im Rahmen des Unterrichts wurden mehrere Fallstudien erarbeitet bzw. bereitgestellt,\ndiese dienen Ihnen als ideale Vorlage.Eine Beispiel-Modellierung finden Sie der Datei Beispielanalyse-Prognose-Wettbewerb.Rmd.\nEine beispielhafte Vorlage (Template), die Sie als Richtschnur nutzen können, ist mit der Datei Template-Vorhersagemodellierung.Rmd hier bereitgestellt.Im Internet finden sich viele Fallstudien, von denen Sie sich inspirieren lassen können.","code":""},{"path":"prüfung.html","id":"materialsammlung","chapter":"Kapitel 2 Prüfung","heading":"2.12.2 Materialsammlung","text":"[diesem Ordner]((https://github.com/sebastiansauer/Lehre/tree/main/Hinweise/Prognosewettbewerb) finden Sie eine Materialsammlung zum Prognosewettbewerb.","code":""},{"path":"prüfung.html","id":"videos-1","chapter":"Kapitel 2 Prüfung","heading":"2.12.3 Videos","text":"Diese Playlist beinhaltet Videos,\ndie die Rahmenbedingungen der Prüfungsleistung vorstellt.","code":""},{"path":"prüfung.html","id":"plagiatskontrolle","chapter":"Kapitel 2 Prüfung","heading":"2.13 Plagiatskontrolle","text":"Die eingereichten Arbeiten können automatisiert auf Plagiate überprüft werden. Gibt es substanzielle Überschneidungen zwischen zwei (oder mehr) Arbeiten, werden alle betreffenden Arbeiten mit ungenügend bewertet oder es folgt eine Abwertung der Note.","code":""},{"path":"statistisches-lernen.html","id":"statistisches-lernen","chapter":"Kapitel 3 Statistisches Lernen","heading":"Kapitel 3 Statistisches Lernen","text":"","code":""},{"path":"statistisches-lernen.html","id":"lernsteuerung","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.1 Lernsteuerung","text":"","code":""},{"path":"statistisches-lernen.html","id":"vorbereitung-1","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.1.1 Vorbereitung","text":"Lesen Sie die Hinweise zum Modul.Installieren (oder Updaten) Sie die für dieses Modul angegeben Software.Lesen Sie die Literatur.","code":""},{"path":"statistisches-lernen.html","id":"lernziele-1","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.1.2 Lernziele","text":"Sie können erläutern, man unter statistischem Lernen versteht.Sie wissen, war Overfitting ist, wie es entsteht, und wie es vermieden werden kann.Sie kennen verschiedenen Arten von statistischem Lernen und können Algorithmen zu diesen Arten zuordnen.","code":""},{"path":"statistisches-lernen.html","id":"literatur-1","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.1.3 Literatur","text":"Rhys, Kap. 1evtl. Sauer, Kap. 15","code":""},{"path":"statistisches-lernen.html","id":"hinweise-2","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.1.4 Hinweise","text":"Bitte beachten Sie die Hinweise zum Präsenzunterricht und der Streamingoption.Bitte stellen Sie sicher, dass Sie einen einsatzbereiten Computer haben und dass die angegebene Software (aktueller Version) läuft.","code":""},{"path":"statistisches-lernen.html","id":"was-ist-data-science","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.2 Was ist Data Science?","text":"Es gibt mehrere Definitionen von Data Science, aber keinen kompletten Konsens.\nBaumer, Kaplan, Horton (2017) definieren Data Science wie folgt (S. 4):science extracting meaningful information dataAuf der anderen Seite entgegen viele Statistiker: “Hey, das machen wir doch schon immer!”.Eine Antwort auf diesen Einwand ist, dass Data Science nicht nur die Statistik eine Rolle spielt, sondern auch die Informatik sowie - zu einem geringen Teil - die Fachwissenschafte (“Domäne”), die sozusagen den Empfänger bzw. die Kunden oder den Rahmen stellt.\nDieser “Dreiklang” ist folgendem Venn-Diagramm dargestellt.","code":""},{"path":"statistisches-lernen.html","id":"was-ist-machine-learning","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.3 Was ist Machine Learning?","text":"Maschinelles Lernen (ML), oft auch (synonym) als statistisches Lernen (statistical learning) bezeichnet, ist ein Teilgebiet der künstlichen Intelligenz (KI; artificial intelligence, AI) (Rhys 2020). ML wird auch als data-based bezeichnet Abgrenzung von rule-based, auch als “klassische KI” bezeichnet wird, vgl. Abb. 3.1.\nFigure 3.1: KI und Maschinelles Lernen\nbeiden Fällen finden Algorithmen Verwendung.\nAlgorithmen sind nichts anderes als genaue Schritt-für-Schritt-Anleitungen, um etwas zu erledigen.\nEin Kochrezept ist ein klassisches Beispiel für einen Algorithmus.Hier findet sich ein Beispiel für einen einfachen Additionsalgorithmus.Es gibt viele ML-Algorithmen, vgl. Abb. 3.2.\nFigure 3.2: ML-Matroschka\n","code":""},{"path":"statistisches-lernen.html","id":"rule-based","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.3.1 Rule-based","text":"Klassische (ältere) KI implementiert Regeln “hartverdrahtet” ein Computersystem.\nNutzer füttern Daten dieses System. Das System leitet dann daraus Antworten ab.Regeln kann man prototypisch mit Wenn-Dann-Abfragen darstellen:Sicherlich könnte man das schlauer programmieren, vielleicht :","code":"\nlernzeit <- c(0, 10, 10, 20)\nschlauer_nebensitzer <- c(FALSE, FALSE, TRUE, TRUE)\n\nfor (i in 1:4) {\n  if (lernzeit[i] > 10) {\n    print(\"bestanden!\")\n  } else {\n    if (schlauer_nebensitzer[i] == TRUE) {\n      print(\"bestanden!\")\n    } else print(\"Durchgefallen!\")\n  }\n}## [1] \"Durchgefallen!\"\n## [1] \"Durchgefallen!\"\n## [1] \"bestanden!\"\n## [1] \"bestanden!\"\nd <- \n  tibble(\n  lernzeit = c(0, 10, 10, 20),\n  schlauer_nebensitzer = c(FALSE, FALSE, TRUE, TRUE)\n)\n\nd %>% \n  mutate(bestanden = ifelse(lernzeit > 10 | schlauer_nebensitzer == TRUE, TRUE, FALSE))## # A tibble: 4 × 3\n##   lernzeit schlauer_nebensitzer bestanden\n##      <dbl> <lgl>                <lgl>    \n## 1        0 FALSE                FALSE    \n## 2       10 FALSE                FALSE    \n## 3       10 TRUE                 TRUE     \n## 4       20 TRUE                 TRUE"},{"path":"statistisches-lernen.html","id":"data-based","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.3.2 Data-based","text":"ML hat zum Ziel, Regeln aus den Daten zu lernen. Man füttert Daten und Antworten das System, das System gibt Regeln zurück.James et al. (2021) definieren ML :\nNehmen wir , wir haben die abhängige Variable \\(Y\\) und \\(p\\) Prädiktoren, \\(X_1,X_2, \\ldots, X_p\\).\nWeiter nehmen wir , die Beziehung zwischen \\(Y\\) und \\(X = (X_1, X_2, \\ldots, X_p)\\) kann durch eine Funktion \\(f\\) beschrieben werden.\nDas kann man darstellen:\\[Y = f(X) + \\epsilon\\]ML kann man auffassen als eine Menge Verfahren, um \\(f\\) zu schätzen.Ein Beispiel ist Abb. 3.3 gezeigt (James et al. 2021).\nFigure 3.3: Vorhersage des Einkommens durch Ausbildungsjahre\nNatürlich kann \\(X\\) mehr als eine Variable beinhalten, vgl. Abb. 3.4 (James et al. 2021).\nFigure 3.4: Vorhersage des Einkommens als Funktion von Ausbildungsjahren und Dienstjahren\nAnders gesagt: traditionelle KI-Systeme werden mit Daten und Regeln gefüttert und liefern Antworten.\nML-Systeme werden mit Daten und Antworten gefüttert und liefern Regeln zurück, vgl. Abb. 3.5.\nFigure 3.5: Vergleich von klassischer KI und ML\n","code":""},{"path":"statistisches-lernen.html","id":"modell-vs.-algorithmus","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.4 Modell vs. Algorithmus","text":"","code":""},{"path":"statistisches-lernen.html","id":"modell","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.4.1 Modell","text":"Ein Modell, s. Abb. 3.6 (Spurzem 2017)!\nFigure 3.6: Ein Modell-Auto\nWie man sieht, ist ein Modell eine vereinfachte Repräsentation eines Gegenstands.Der Gegenstand definiert (gestaltet) das Modell. Das Modell ist eine Vereinfachung des Gegenstands, vgl. Abb. 3.7.\nFigure 3.7: Gegenstand und Modell\nIm maschinellen Lernen meint ein Modell, praktisch gesehen, die Regeln,\ndie aus den Daten gelernt wurden.","code":""},{"path":"statistisches-lernen.html","id":"beispiel-für-einen-ml-algorithmus","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.4.2 Beispiel für einen ML-Algorithmus","text":"Unter einem ML-Algorithmus versteht man das (mathematische oder statistische) Verfahren,\nanhand dessen die Beziehung zwischen \\(X\\) und \\(Y\\) “gelernt” wird. Bei Rhys (2020) (S. 9) findet sich dazu ein Beispiel, das kurz zusammengefasst etwa lautet:Beispiel eines RegressionsalgorithmusSetze Gerade die Daten mit \\(b_0 = \\hat{y}, b_1 = 0\\)Berechne \\(MSS = \\sum (y_i - \\hat{y_i})^2\\)“Drehe” die Gerade ein bisschen, d.h. erhöhe \\(b_1^{neu} = b_1^{alt} + 0.1\\)Wiederhole 2-3 solange, bis \\(MSS < \\text{Zielwert}\\)Diesen Algorithmus kann man “von Hand” z.B. mit dieser App durchspielen.","code":""},{"path":"statistisches-lernen.html","id":"taxonomie","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.5 Taxonomie","text":"Methoden des maschinellen Lernens lassen sich verschiedentlich gliedern.\nEine typische Gliederung unterscheidet supervidierte (geleitete) und nicht-supervidierte (ungeleitete) Algorithmen, s. Abb. 3.8.\nFigure 3.8: Taxonomie der Arten des maschinellen Lernens\n","code":""},{"path":"statistisches-lernen.html","id":"geleitetes-lernen","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.5.1 Geleitetes Lernen","text":"Die zwei Phasen des geleiteten Lernens sind Abb. 3.9 dargestellt.\nFigure 3.9: Geleitetes Lernen geschieht zwei Phasen\n","code":""},{"path":"statistisches-lernen.html","id":"regression-numerische-vorhersage","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.5.1.1 Regression: Numerische Vorhersage","text":"Die Modellgüte eines numerischen Vorhersagemodells wird oft mit (einem der) folgenden Gütekoeffizienten gemessen:Mean Squared Error (Mittlerer Quadratfehler):\\[MSE := \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2\\]Mean Absolute Error (Mittlerer Absolutfehler):\\[MAE :=  \\frac{1}{n} \\sum |(y_i - \\hat{y}_i)|\\]Wir sind nicht adaran interessiert die Vorhersagegenauigkeit den bekannten Daten einzuschätzen, sondern im Hinblick auf neue Daten, die der Lernphase dem Modell nicht bekannt waren.","code":""},{"path":"statistisches-lernen.html","id":"klassifikation-nominale-vorhersage","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.5.1.2 Klassifikation: Nominale Vorhersage","text":"Die Modellgüte eines numerischen Vorhersagemodells wird oft mit folgendem Gütekoeffizienten gemessen:Mittlerer Klassifikationfehler \\(e\\):\\[e := \\frac{1}{n} (y_i \\ne \\hat{y}_i) \\]Dabei ist \\(\\) eine Indikatorfunktion, die 1 zurückliefert,\nwenn tatsächlicher Wert und vorhergesagter Wert identisch sind.","code":""},{"path":"statistisches-lernen.html","id":"ungeleitetes-lernen","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.5.2 Ungeleitetes Lernen","text":"Die zwei Phasen des ungeleiteten Lernens sind Abb. 3.10 dargestellt.\nFigure 3.10: Die zwei Phasen des ungeleiteten Lernens\nUngeleitetes Lernen kann man wiederum zwei Arten unterteilen, vgl. Abb. 3.11:Fallreduzierendes Modellieren (Clustering)Dimensionsreduzierendes Modellieren (z.B. Faktorenanalyse)\nFigure 3.11: Zwei Arten von ungeleitetem Modellieren\n","code":""},{"path":"statistisches-lernen.html","id":"ziele-des-ml","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.6 Ziele des ML","text":"Man kann vier Ziele des ML unterscheiden, s. Abb. 3.12.\nFigure 3.12: Ziele des maschinellen Lernens\nVorhersage bezieht sich auf die Schätzung der Werte von Zielvariablen (sowie die damit verbundene Unsicherheit).\nErklärung meint die kausale Analyse von Zusammenhängen.\nBeschreibung ist praktisch gleichzusetzen mit der Verwendung von deskriptiven Statistiken.\nDimensionsreduktion ist ein Oberbegriff für Verfahren, die die Anzahl der Variablen (Spalten) oder der Beobachtungen (Zeilen) verringert.sWie “gut” ein Modell ist, quantifiziert man verschiedenen Kennzahlen; man spricht von Modellgüte oder model fit.\nJe schlechter die Modellgüte, desto höher der Modellfehler, vgl. Abb. 3.13.\nFigure 3.13: Wenig (links) vs. viel (rechts) Vorhersagefehler\nDie Modellgüte eines Modells ist nur relevant für neue Beobachtungen,\ndenen das Modell nicht trainiert wurde.","code":""},{"path":"statistisches-lernen.html","id":"über--vs.-unteranpassung","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.7 Über- vs. Unteranpassung","text":"Overfitting: Ein Modell sagt die Trainingsdaten zu genau vorher - es nimmt Rauschen als “bare Münze”, also fälschlich als Signal. Solche Modelle haben zu viel Varianz ihren Vorhersagen.Underfitting: Ein Modell ist zu simpel (ungenau, grobkörnig) - es unterschlägt Nuancen des tatsächlichen Musters. Solche Modelle haben zu viel Verzerrung (Bias) ihren Vorhersagen.Welches der folgenden Modelle (B,C,D) passt besten zu den Daten (), s. Abb. 3.14, vgl. (Sauer 2019), Kap. 15.\nFigure 3.14: - vs. Underfitting\nWelches Modell wird wohl neue Daten besten vorhersagen? meinen Sie?Modell D zeigt sehr gute Beschreibung (“Retrodiktion”) der Werte, anhand derer das Modell trainiert wurde (“Trainingsstichprobe”).\nWird es aber “ehrlich” getestet, d.h. anhand neuer Daten (“Test-Stichprobe”),\nwird es vermutlich nicht gut abschneiden.Es gilt, ein Modell mit “mittlerer” Komplexität zu finden, um Über- und Unteranpassung Grenzen zu halten.\nLeider ist es nicht möglich, vorab zu sagen, der richtige, “mittlere” Wert Komplexität eines Modells ist, vgl. Abb. 3.15 aus (Sauer 2019).\nFigure 3.15: Mittlere Modellkomplexität führt zur besten Vorhersagegüte\n","code":""},{"path":"statistisches-lernen.html","id":"no-free-lunch","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.8 No free lunch","text":"Wenn \\(f\\) (die Beziehung zwischen \\(Y\\) und \\(X\\), auch datengenerierender Prozess genannt) linear oder fast linear ist,\ndann wird ein lineare Modell gute Vorhersagen liefern, vgl. Abb. 3.16 aus James et al. (2021), dort zeigt die schwarze Linie den “wahren Zusammenhang”, also \\(f\\) . orange sieht man ein lineares Modell, grün ein hoch komplexes Modell,\ndas sich einer “wackligen” Funktion - also mit hoher Varianz -\nniederschlägt. Das grüne Modell könnte z.B. ein Polynom-Modell hohen Grades sein, z. B.\n\\(y = b_0 + b_1 x^{10} + b_2 x^9 + \\ldots + b_11 x^1 + \\epsilon\\).\nDas lineare Modell hat hingegen wenig Varianz und diesem Fall wenig Bias.\nDaher ist es für dieses \\(f\\) gut passend.\nDie grüne Funktion zeigt dagegen Überanpassung (overfitting),\nalso viel Modellfehler (für eine Test-Stichprobe).Die grüne Funktion Abb. 3.16 wird neue, beim Modelltraining unbekannte Beobachtungen (\\(y_0\\)) vergleichsweise schlecht vorhersagen. Abb. 3.17 ist es umgekehrt.\nFigure 3.16: Ein lineare Funktion verlangt ein lineares Modell; ein nichtlineares Modell wird einem höheren Vorhersagefehler (bei neuen Daten!) resultieren.\nBetrachten wir im Gegensatz dazu Abb. 3.17 aus James et al. (2021), die (schwarz) eine hochgradig nichtlineare Funktion \\(f\\) zeigt.\nEntsprechend wird das lineare Modell (orange) nur schlechte Vorhersagen erreichen - es hat zu viel Bias, da zu simpel.\nEin lineares Modell wird der Komplexität von \\(f\\) nicht gerecht,\nUnteranpassung (underfitting) liegt vor.\nFigure 3.17: Eine nichtlineare Funktion (schwarz) verlangt eine nichtlineares Modell. Ein lineares Modell (orange) ist unterangepasst und hat eine schlechte Vorhersageleistung.\n","code":""},{"path":"statistisches-lernen.html","id":"bias-varianz-abwägung","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.9 Bias-Varianz-Abwägung","text":"Der Gesamtfehler \\(E\\) des Modells ist die Summe dreier Terme:\\[E = (y - \\hat{y}) = \\text{Bias} + \\text{Varianz} + \\epsilon\\]Dabei meint \\(\\epsilon\\) den nicht reduzierbaren Fehler, z.B. weil dem Modell Informationen fehlen. kann man etwa auf der Motivation von Studentis keine perfekte Vorhersage ihrer Noten erreichen (lehrt die Erfahrung).Bias und Varianz sind Kontrahenten: Ein Modell, das wenig Bias hat, neigt tendenziell zu wenig Varianz und umgekehrt, vgl. Abb. 3.18 aus Sauer (2019).\nFigure 3.18: Abwängung von Bias vs. Varianz\n","code":""},{"path":"statistisches-lernen.html","id":"aufgaben","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.10 Aufgaben","text":"Machen Sie sich mit ‘Kaggle’ vertrautBearbeiten Sie die Fallstudie ‘TitaRnic’ auf KaggleMachen Sie sich mit dieser einfachen Fallstudie zur linearen Regression vertraut: Movie Data Base Revenue (Kaggle)","code":""},{"path":"statistisches-lernen.html","id":"vertiefung","chapter":"Kapitel 3 Statistisches Lernen","heading":"3.11 Vertiefung","text":"Verdienst einer deutschen Data ScientistinWeitere Fallstudie zum Thema Regression auf KaggleCrashkurs Data Science (Coursera, Johns Hopkins University) mit ‘Star-Dozenten’Arbeiten Sie diese Regressionsfallstudie (zum Thema Gehalt) auf Kaggle aufWerfen Sie einen Blick diese Fallstudie auf Kaggle zum Thema HauspreiseWiederholen Sie unser Vorgehen der Fallstudie zu den Flugverspätungen","code":""},{"path":"r-zweiter-blick.html","id":"r-zweiter-blick","chapter":"Kapitel 4 R, zweiter Blick","heading":"Kapitel 4 R, zweiter Blick","text":"","code":""},{"path":"r-zweiter-blick.html","id":"lernsteuerung-1","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.1 Lernsteuerung","text":"","code":""},{"path":"r-zweiter-blick.html","id":"vorbereitung-2","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.1.1 Vorbereitung","text":"Lesen Sie die Literatur.","code":""},{"path":"r-zweiter-blick.html","id":"lernziele-2","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.1.2 Lernziele","text":"Sie können Funktionen, R schreiben.Sie können Datensätze vom Lang- und Breit-Format wechseln.Sie können Wiederholungsstrukturen wie Mapping-Funktionen anwenden.Sie können eine dplyr-Funktion auf mehrere Spalten gleichzeitig anwenden.","code":""},{"path":"r-zweiter-blick.html","id":"literatur-2","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.1.3 Literatur","text":"Rhys, Kap. 2MODAR, Kap. 5","code":""},{"path":"r-zweiter-blick.html","id":"objekttypen-in-r","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.2 Objekttypen in R","text":"Näheres zu Objekttypen findet sich Sauer (2019), Kap. 5.2.","code":""},{"path":"r-zweiter-blick.html","id":"überblick","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.2.1 Überblick","text":"R ist praktisch alles ein Objekt.\nEin Objekt meint ein im Computerspeicher repräsentiertes Ding, etwa eine Tabelle.Vektoren und Dataframes (Tibbles) sind die vielleicht gängigsten Objektarten R (vgl. Abb. 4.1, aus Sauer (2019)).\nFigure 4.1: Zentrale Objektarten R\nEs gibt R keine (Objekte für) Skalare (einzelne Zahlen).\nStattdessen nutzt R Vektoren der Länge 1.Ein nützliches Schema stammt aus Wickham Grolemund (2016), s. Abb. 4.2.\nFigure 4.2: Objektarten hierarchisch gegliedert\n","code":""},{"path":"r-zweiter-blick.html","id":"taxonomie-1","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.2.2 Taxonomie","text":"Unter homogenen Objektiven verstehen wir Datenstrukturen,\ndie nur eine Art von Daten (wie Text oder Ganze Zahlen) fassen.\nSonstige Objekte nennen wir heterogen.Homogene Objekte\nVektoren\nMatrizen\nVektorenMatrizenHeterogen\nListe\nDataframes (Tibbles)\nListeDataframes (Tibbles)","code":""},{"path":"r-zweiter-blick.html","id":"vektoren","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.2.2.1 Vektoren","text":"Vektoren sind insofern zentral R,\nals dass die übrigen Datenstrukturen auf ihnen aufbauen, vgl. Abb. 4.3 aus Sauer (2019).Reine (atomare) Vektoren R sind eine geordnete Liste von Daten eines Typs.\nFigure 4.3: Vektoren stehen im Zentrum der Datenstrukturen R\nMit str() kann man sich die Struktur eines Objektsausgeben lassen:Vektoren können von folgenden Typen sein:Kommazahlen ( double) genanntGanzzahlig (integer, auch mit L für Long abgekürzt)Text (´character`, String)logische Ausdrücke (logical oder lgl) mit TRUE oder FALSEKommazahlen und Ganze Zahlen zusammen bilden den Typ numeric (numerisch) R.Den Typ eines Vektors kann man mit typeof() ausgeben lassen:","code":"\nein_vektor <- c(1, 2, 3)\nnoch_ein_vektor <- c(\"A\", \"B\", \"C\")\nlogischer_vektor <- c(TRUE, FALSE, TRUE)\nstr(ein_vektor)##  num [1:3] 1 2 3\nstr(noch_ein_vektor)##  chr [1:3] \"A\" \"B\" \"C\"\nstr(logischer_vektor)##  logi [1:3] TRUE FALSE TRUE## [1] \"double\""},{"path":"r-zweiter-blick.html","id":"faktoren","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.2.2.2 Faktoren","text":"Interessant:Vertiefende Informationen findet sich Wickham Grolemund (2016).","code":"\nsex <- factor(c(\"Mann\", \"Frau\", \"Frau\"))\nstr(sex)##  Factor w/ 2 levels \"Frau\",\"Mann\": 2 1 1"},{"path":"r-zweiter-blick.html","id":"listen","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.2.2.3 Listen","text":"","code":"\neine_liste <- list(titel = \"Einführung\",\n                   woche = 1,\n                   datum = c(\"2022-03-14\", \"2202-03-21\"),\n                   lernziele = c(\"dies\", \"jenes\", \"und noch mehr\"),\n                   lehre = c(TRUE, TRUE, TRUE)\n                   )\nstr(eine_liste)## List of 5\n##  $ titel    : chr \"Einführung\"\n##  $ woche    : num 1\n##  $ datum    : chr [1:2] \"2022-03-14\" \"2202-03-21\"\n##  $ lernziele: chr [1:3] \"dies\" \"jenes\" \"und noch mehr\"\n##  $ lehre    : logi [1:3] TRUE TRUE TRUE"},{"path":"r-zweiter-blick.html","id":"tibbles","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.2.2.4 Tibbles","text":"Für tibble() brauchen wir tidyverse:","code":"\nlibrary(tidyverse)\nstudentis <-\n  tibble(\n    name = c(\"Anna\", \"Berta\"),\n    motivation = c(10, 20),\n    noten = c(1.3, 1.7)\n  )\nstr(studentis)## tibble [2 × 3] (S3: tbl_df/tbl/data.frame)\n##  $ name      : chr [1:2] \"Anna\" \"Berta\"\n##  $ motivation: num [1:2] 10 20\n##  $ noten     : num [1:2] 1.3 1.7"},{"path":"r-zweiter-blick.html","id":"indizieren","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.2.3 Indizieren","text":"Einen Teil eines Objekts auszulesen, bezeichnen wir als Indizieren.","code":""},{"path":"r-zweiter-blick.html","id":"reine-vektoren","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.2.3.1 Reine Vektoren","text":"Zur Erinnerung:Aber nicht :Man darf Vektoren auch wie Listen ansprechen, also eine doppelte Eckklammer zum Indizieren verwendenDer Grund ist,\ndass Listen auch Vektoren sind, nur eben ein besonderer Fall eines Vektors:passiert, wenn man bei einem Vektor der Länge 3 das 4. Element indiziert?Ein schnödes NA ist die Antwort. Das ist interessant:\nWir bekommen keine Fehlermeldung, sondern den Hinweis,\ndas angesprochene Element sei leer bzw. nicht verfügbar.Sauer (2019), Kap. 5.3.1 findet man weitere Indizierungsmöglichkeiten für reine Vektoren.","code":"\nstr(ein_vektor)##  num [1:3] 1 2 3\nein_vektor[1]## [1] 1\nein_vektor[c(1,2)]## [1] 1 2\nein_vektor[1,2]## Error in ein_vektor[1, 2]: incorrect number of dimensions## [1] 2## [1] TRUE\nein_vektor[4]## [1] NA"},{"path":"r-zweiter-blick.html","id":"listen-1","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.2.3.2 Listen","text":"Listen können wie Vektoren, also mit [ ausgelesen werden.\nDann wird eine Liste zurückgegeben.Das hat den technischen Hintergrund,\ndass Listen als eine bestimmte Art von Vektoren implementiert sind.Mann kann auch die “doppelte Eckklammer”, [[ zum Auslesen verwenden;\ndann wird anstelle einer Liste die einfachere Struktur eines Vektors zurückgegeben:Man könnte sagen,\ndie “äußere Schicht” des Objekts, die Liste,\nwird abgeschält, und man bekommnt die “innere” Schicht,\nden Vektor.Mann die Elemente der Liste entweder mit ihrer Positionsnummer (1, 2, …) oder,\nsofern vorhanden, ihren Namen ansprechen:Dann gibt es noch den Dollar-Operator,\nmit dem Mann benannte Elemente von Listen ansprechen kann:Man kann auch tiefer eine Liste hinein indizieren.\nSagen wir, uns interessiert das 4. Element der Liste eine_liste -\nund davon das erste Element.Das geht dann :Eine einfachere Art des Indizierens von Listen bietet die Funktion pluck(), aus dem Paket purrr,\ndas Hilfen für den Umgang mit Listen bietet.Und jetzt aus dem 4. Element das 1. Element:Probieren Sie mal, aus einer Liste der Länge 5 das 6. Element auszulesen:Unser Versuch wird mit einer Fehlermeldung quittiert.Sprechen wir die Liste wie einen (atomaren) Vektor ,\nbekommen wir hingegen ein NA bzw. ein NULL:","code":"\neine_liste %>% str()## List of 5\n##  $ titel    : chr \"Einführung\"\n##  $ woche    : num 1\n##  $ datum    : chr [1:2] \"2022-03-14\" \"2202-03-21\"\n##  $ lernziele: chr [1:3] \"dies\" \"jenes\" \"und noch mehr\"\n##  $ lehre    : logi [1:3] TRUE TRUE TRUE\neine_liste[1]## $titel\n## [1] \"Einführung\"\neine_liste[2]## $woche\n## [1] 1\neine_liste[[1]]## [1] \"Einführung\"\neine_liste[[\"titel\"]]## [1] \"Einführung\"\neine_liste$titel## [1] \"Einführung\"\neine_liste[[4]][[1]] ## [1] \"dies\"\npluck(eine_liste, 4)## [1] \"dies\"          \"jenes\"         \"und noch mehr\"\npluck(eine_liste, 4, 1)## [1] \"dies\"\neine_liste %>% length()## [1] 5\neine_liste[[6]]## Error in eine_liste[[6]]: subscript out of bounds\neine_liste[6]## $<NA>\n## NULL"},{"path":"r-zweiter-blick.html","id":"tibbles-1","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.2.3.3 Tibbles","text":"Tibbles lassen sich sowohl wie ein Vektor als auch wie eine Liste indizieren.Die Indizierung eines Tibbles mit der einfachen Eckklammer liefert einen Tibble zurück.Mit doppelter Eckklammer bekommt man,\nanalog zur Liste,\neinen Vektor zurück:Beim Dollar-Operator kommt auch eine Liste zurück:","code":"\nstudentis[1]## # A tibble: 2 × 1\n##   name \n##   <chr>\n## 1 Anna \n## 2 Berta\nstudentis[\"name\"]## # A tibble: 2 × 1\n##   name \n##   <chr>\n## 1 Anna \n## 2 Berta\nstudentis[[\"name\"]]## [1] \"Anna\"  \"Berta\"\nstudentis$name## [1] \"Anna\"  \"Berta\""},{"path":"r-zweiter-blick.html","id":"weiterführende-hinweise","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.2.4 Weiterführende Hinweise","text":"Tutorial zum Themen Indizieren von Listen von Jenny BC.","code":""},{"path":"r-zweiter-blick.html","id":"indizieren-mit-dem-tidyverse","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.2.5 Indizieren mit dem Tidyverse","text":"Natürlich kann man auch die Tidyverse-Verben zum Indizieren verwenden.\nDas bietet sich , wenn zwei Bedingungen erfüllt sind:Wenn man einen Tibble als Input und als Output hatWenn man nicht programmieren möchte","code":""},{"path":"r-zweiter-blick.html","id":"datensätze-von-lang-nach-breit-umformatieren","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.3 Datensätze von lang nach breit umformatieren","text":"Manchmal findet man Datensätze im sog. langen Format vor,\nmanchmal im breiten.der Regel müssen die Daten “tidy” sein,\nmeist dem langen Format entspricht, vgl. Abb. 4.4 aus Sauer (2019).\nFigure 4.4: Von lang nach breit und zurück\neiner neueren Version des Tidyverse werden diese beiden Befehle umbenannt bzw. erweitert:gather() -> pivot_longer()spread() -> pivot_wider()Weitere Informationen findet sich Wickham Grolemund (2016), diesem Abschnitt, 12.3.","code":""},{"path":"r-zweiter-blick.html","id":"funktionen","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.4 Funktionen","text":"Eine Funktion kann man sich als analog zu einer Variable vorstellen.\nEs ist ein Objekt, das nicht Daten, sondern Syntax beinhaltet,\nvgl. Abb. 4.5 aus Sauer (2019).\nFigure 4.5: Sinnbild einer Funktion\nWeitere Informationen finden sich Kapitel 19 Wickham Grolemund (2016). Alternativ findet sich ein Abschnitt dazu (28.1) Sauer (2019).","code":"\nmittelwert <- function(x){\n  \n  summe <- sum(x, na.rm = TRUE)\n  mw <- summe/length(x)\n  return(mw)\n  \n}\nmittelwert(c(1, 2, 3))## [1] 2"},{"path":"r-zweiter-blick.html","id":"wiederholungen-programmieren","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.5 Wiederholungen programmieren","text":"Häufig möchte man eine Operation mehrfach ausführen.\nEin Beispiel wäre die Anzahl der fehlenden Werte pro Spalte auslesen.\nNatürlich kann man die Abfrage einfach häufig tippen, nervt aber irgendwann.\nDaher braucht’s Strukturen, die Wiederholungen beschreiben.Dafür gibt es verschiedene Ansätze.","code":""},{"path":"r-zweiter-blick.html","id":"across","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.5.1 across()","text":"Handelt es sich um Spalten von Tibbles, dann bietet sich die Funktion across(.col, .fns) .\nacross wendet eine oder mehrere Funktionen (mit .fns bezeichnet) auf die Spalten .col .Das erklärt sich besten mit einem Beispiel:Natürlich hätte man diesem Fall auch anders vorgehen können:Möchte man der Funktion .fns Parameter übergeben, nutzt man diese Syntax (“Purrr-Lambda”):Hier findet sich ein guter Überblick zu across().","code":"\nmtcars %>% \n  summarise(across(.cols = everything(),\n                   .fns = mean))##        mpg    cyl     disp       hp     drat      wt     qsec     vs      am\n## 1 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 0.4375 0.40625\n##     gear   carb\n## 1 3.6875 2.8125\nmtcars %>% \n  summarise(across(.cols = everything(),\n                   .fns = ~ mean(., na.rm = TRUE)))##        mpg    cyl     disp       hp     drat      wt     qsec     vs      am\n## 1 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 0.4375 0.40625\n##     gear   carb\n## 1 3.6875 2.8125"},{"path":"r-zweiter-blick.html","id":"map","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.5.2 map()","text":"map() ist eine Funktion aus dem R-Paket purrr und Teil des Tidyverse.map(x, f) wenden die Funktion f auf jedes Element von x .\nIst x ein Tibble, wird f demnach auf jede Spalte von x angewendet (“zugeordnet”, daher map), vgl. Abb. 4.6 aus Sauer (2019).\nFigure 4.6: Sinnbild für map\nHier ein Beispiel-Code:Möchte man der gemappten Funktion Parameter übergeben,\nnutzt man wieder die “Kringel-Schreibweise”:","code":"\ndata(mtcars)\n\nmap(mtcars, mean)## $mpg\n## [1] 20.09062\n## \n## $cyl\n## [1] 6.1875\n## \n## $disp\n## [1] 230.7219\n## \n## $hp\n## [1] 146.6875\n## \n## $drat\n## [1] 3.596563\n## \n## $wt\n## [1] 3.21725\n## \n## $qsec\n## [1] 17.84875\n## \n## $vs\n## [1] 0.4375\n## \n## $am\n## [1] 0.40625\n## \n## $gear\n## [1] 3.6875\n## \n## $carb\n## [1] 2.8125\nmap(mtcars, ~ mean(., na.rm = TRUE))## $mpg\n## [1] 20.09062\n## \n## $cyl\n## [1] 6.1875\n## \n## $disp\n## [1] 230.7219\n## \n## $hp\n## [1] 146.6875\n## \n## $drat\n## [1] 3.596563\n## \n## $wt\n## [1] 3.21725\n## \n## $qsec\n## [1] 17.84875\n## \n## $vs\n## [1] 0.4375\n## \n## $am\n## [1] 0.40625\n## \n## $gear\n## [1] 3.6875\n## \n## $carb\n## [1] 2.8125"},{"path":"r-zweiter-blick.html","id":"weiterführende-hinweise-1","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.5.3 Weiterführende Hinweise","text":"Weiteres zu map() findet sich z.B. Wickham Grolemund (2016), Kapitel 21.5 oder Sauer (2019), Kap. 28.2.Tutorial zu map() von Jenny BC.","code":""},{"path":"r-zweiter-blick.html","id":"listenspalten","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.6 Listenspalten","text":"","code":""},{"path":"r-zweiter-blick.html","id":"wozu-listenspalten","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.6.1 Wozu Listenspalten?","text":"Listenspalten sind immer dann sinnvoll,\nwenn eine einfache Tabelle nicht komplex genug für unsere Daten ist.Zwei Fälle stechen dabei ins Auge:Unsere Datenstruktur ist nicht rechteckigIn einer Zelle der Tabelle soll mehr als ein einzelner Wert stehen: vielleicht ein Vektor, eine Liste oder eine TabelleDer erstere Fall (nicht reckeckig) ließe sich noch einfach lösen,\ndem man mit NA auffüllt.Der zweite Fall verlangt schlichtweg nach komplexeren Datenstrukturen.Kap. 25.3 aus Wickham Grolemund (2016) bietet einen guten Einstieg das Konzept von Listenspalten (list-columns) R.","code":""},{"path":"r-zweiter-blick.html","id":"beispiele-für-listenspalten","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.6.2 Beispiele für Listenspalten","text":"","code":""},{"path":"r-zweiter-blick.html","id":"tidymodel","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.6.2.1 tidymodel","text":"Wenn wir mit tidymodels arbeiten,\nwerden wir mit Listenspalten zu tun haben.\nDaher ist es praktisch, sich schon mal damit zu beschäftigen.Hier ein Beispiel für eine \\(v=3\\)-fache Kreuzvalidierung:Betrachten wir das Objekt mtcars_cv näher.\nDie Musik spielt der 1. Spalte.Lesen wir den Inhalt der 1. Spalte, 1 Zeile aus (nennen wir das mal “Position 1,1”):dieser Zelle findet sich eine Aufteilung des Komplettdatensatzes den Analyseteil (Analysis sample) und den Assessmentteil (Assessment Sample).Schauen wir jetzt dieses Objekt näher .\nDas können wir mit str() tun.\nstr() zeigt uns die Strktur eines Objekts.Oh! pos11 ist eine Liste, und zwar eine durchaus komplexe.\nWir müssen erkennen,\ndass einer einzelnen Zelle dieses Dataframes viel mehr steht,\nals ein Skalar bzw. ein einzelnes, atomares Element.Damit handelt es sich bei Spalte 1 dieses Dataframes (mtcars_cv) also um eine Listenspalte.Üben wir uns noch etwas im Indizieren.Sprechen wir pos11 das erste Element (data) und davon das erste Element:Wir haben hier die doppelten Eckklammern benutzt,\num den “eigentlichen” oder “inneren” Vektor zu bekommen,\nnicht die “außen” herumgewickelte Liste.\nZur Erinnerung:\nEin Dataframe ist ein Spezialfall einer Liste,\nalso auch eine Liste, nur eine mit bestimmten Eigenschaften.Zum Vergleich indizieren wir mal mit einer einfachen Eckklammer:Mit pluck() bekommen wir das gleiche Ergebnis,\nnur etwas komfortabler,\nda wir keine Eckklammern tippen müssen:Wie man sieht, können wir beliebig tief das Objekt hineinindizieren.","code":"\nlibrary(tidymodels)\nmtcars_cv <-\n  vfold_cv(mtcars, v = 3)\n\nmtcars_cv## #  3-fold cross-validation \n## # A tibble: 3 × 2\n##   splits          id   \n##   <list>          <chr>\n## 1 <split [21/11]> Fold1\n## 2 <split [21/11]> Fold2\n## 3 <split [22/10]> Fold3\npos11 <- mtcars_cv[[1]][[1]]\npos11## <Analysis/Assess/Total>\n## <21/11/32>\nstr(pos11)## List of 4\n##  $ data  :'data.frame':  32 obs. of  11 variables:\n##   ..$ mpg : num [1:32] 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##   ..$ cyl : num [1:32] 6 6 4 6 8 6 8 4 4 6 ...\n##   ..$ disp: num [1:32] 160 160 108 258 360 ...\n##   ..$ hp  : num [1:32] 110 110 93 110 175 105 245 62 95 123 ...\n##   ..$ drat: num [1:32] 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##   ..$ wt  : num [1:32] 2.62 2.88 2.32 3.21 3.44 ...\n##   ..$ qsec: num [1:32] 16.5 17 18.6 19.4 17 ...\n##   ..$ vs  : num [1:32] 0 0 1 1 0 1 0 1 1 1 ...\n##   ..$ am  : num [1:32] 1 1 1 0 0 0 0 0 0 0 ...\n##   ..$ gear: num [1:32] 4 4 4 3 3 3 3 4 4 4 ...\n##   ..$ carb: num [1:32] 4 4 1 1 2 1 4 2 2 4 ...\n##  $ in_id : int [1:21] 2 5 8 9 10 11 12 13 15 17 ...\n##  $ out_id: logi NA\n##  $ id    : tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ id: chr \"Fold1\"\n##  - attr(*, \"class\")= chr [1:2] \"vfold_split\" \"rsplit\"\npos11[[\"data\"]][[1]]##  [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n## [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n## [31] 15.0 21.4\npos11[[\"data\"]][1] %>% \n  head()##                    mpg\n## Mazda RX4         21.0\n## Mazda RX4 Wag     21.0\n## Datsun 710        22.8\n## Hornet 4 Drive    21.4\n## Hornet Sportabout 18.7\n## Valiant           18.1\npluck(pos11, \"data\", 1, 1)## [1] 21"},{"path":"r-zweiter-blick.html","id":"kurs-datascience1","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.6.2.2 Kurs DataScience1","text":"Ein Kurs, wie dieser, kann anhand einer “Deskriptoren” wie Titel der Inhalte, Lernziele, Literatur und weiter zusammmengefasst werden.\nDiese Deskriptoren kann man wiederum jeder Kurswoche oder jedem Kursabschnitt zuordnen,\ndass eine zweidimensionale Struktur resultiert.\nEine Tabelle, einfach gesagt, etwa :Wie man sieht, entspricht jede Spalte einem Deskriptor des Kurses,\nund jede Zeile entspricht einem Thema (oder Woche oder Abschnitt) des Kurses.Jetzt ist es nur ,\ndass einzelne Zellen dieser Tabelle nicht aus nur einem Element bestehen.\nkönnte etwa “Aufgaben1” aus mehreren Aufgaben bestehen,\ndie jeweils wiederum aus mehreren (Text-)Elementen bestehen.\nOder “Literatur2” besteht vielleicht aus zwei Literaturquellen.Kurz gesagt, wir brauchen eine Tabelle,\ndie erlaubt, einer Zelle mehr als ein einzelnes Element zu packen.\nListenspalten erlauben das.Schauen wir uns die “Mastertabelle” dieses Kurses zur Illustration.Zunächst sourcen wir die nötigen Funktionen.Ihrem Environment sollten Sie jetzt die gesourcten Funktionen sehen.\nMit Klick auf den Funktionsnamen können Sie diese Funktionen auch betrachten.Die Deskriptoren des Kurses speisen sich aus zwei Textdateien, gespeichert im sog. YAML-Format, ein einfaches Textformat, und hier nicht weiter von Belang.Zum einen eine Datei mit den Datumsangaben:Zum anderen eine Datei mit den Deskriptoren,\ndie unabhängig vom Datum sind:Im Githup-Repo\ndieses Kurses können Sie die Dateien komfortabel betrachten.Die “Mastertabelle” kann man mit folgender Funktion erstellen:Betrachten Sie die Tabelle Ruhe!\nSie werden sehen, dass einige Spalten komplex sind,\nalso mehr als nur einen einzelnen Wert enthalten:Gerade haben wir aus dem Objekt mastertable, ein Dataframe,\ndie Spalte mit dem Namen Vorbereitung ausgelesen und aus dieser Spalte das erste erste Element.\nDieses erste Element ist ein Textvektor der Länge 3.Daraus könnten wir z.B. das zweite Element auslesen:würde passieren, wenn wir anstelle der doppelten Eckklammer einfache Eckklammern verwenden würden?Das macht noch keinen großen Unterschied,\naber schauen wir mal weiter.Wenn wir das erste Element der Spalte “Vorbereitung” mit doppelter Eckklammer ansprechen,\nbekommen wir einen Text-Vektor (der Länge drei) zurück.Jetzt können wir, wie oben getan,\ndiese einzelnen Elemente ansprechen.Aber: Wenn wir das erste Element der Spalte “Vorbereitung” mit einfacher Eckklammer ansprechen, bekommen wir eine Liste mit einem Element zurück.Wir können also nicht (ohne weiteres “Abschälen”)\nz.B. das zweite Element des Text-Vektors (“Installieren Sie…”) auslesen:Wenn Sie sich über pluck() wundern,\nSie hätten synonym auch schreiben können:Da die Liste nur aus einem Element besteht,\nkönnten wir z.B. nicht das zweite Element der Liste ansprechen:Haben wir aber die doppelte Eckklammer verwendet,\nbekommen wir einen Vektor der Länge drei zurück (vom Typ Text),\nund daher können wir die Elemente 1 bis 3 ansprechen:Dabei ist es egal, ob Sie einfache oder doppelte Eckklammern benutzen,\nda Listen auch Vektoren sind.","code":"\nsource(\"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/R-Code/render-course-sections.R\")\ndates_file <- \"https://raw.githubusercontent.com/sebastiansauer/datascience1/main/course-dates.yaml\"\ncourse_content_file <- \"https://raw.githubusercontent.com/sebastiansauer/datascience1/main/_modul-ueberblick.yaml\"\nmastertable <- build_master_course_table(course_dates_file = dates_file,\n                                         content_file = course_content_file)\nmastertable[[\"Vorbereitung\"]][[1]]## [1] \"Lesen Sie die Hinweise zum Modul.\"                                       \n## [2] \"Installieren (oder Updaten) Sie die für dieses Modul angegeben Software.\"\n## [3] \"Lesen Sie die Literatur.\"\nmastertable[[\"Vorbereitung\"]][[1]][2]## [1] \"Installieren (oder Updaten) Sie die für dieses Modul angegeben Software.\"\nmastertable[\"Vorbereitung\"] %>% class()## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\nmastertable[[\"Vorbereitung\"]] %>% class()## [1] \"list\"\nmastertable[[\"Vorbereitung\"]][[1]]## [1] \"Lesen Sie die Hinweise zum Modul.\"                                       \n## [2] \"Installieren (oder Updaten) Sie die für dieses Modul angegeben Software.\"\n## [3] \"Lesen Sie die Literatur.\"\nmastertable[[\"Vorbereitung\"]][1]## [[1]]\n## [1] \"Lesen Sie die Hinweise zum Modul.\"                                       \n## [2] \"Installieren (oder Updaten) Sie die für dieses Modul angegeben Software.\"\n## [3] \"Lesen Sie die Literatur.\"\nmastertable[[\"Vorbereitung\"]][1] %>% pluck(2)## NULL\nmastertable[[\"Vorbereitung\"]][1][2]## [[1]]\n## NULL\nmastertable[[\"Vorbereitung\"]][1][[2]]## Error in mastertable[[\"Vorbereitung\"]][1][[2]]: subscript out of bounds\nmastertable[[\"Vorbereitung\"]][[1]][2]## [1] \"Installieren (oder Updaten) Sie die für dieses Modul angegeben Software.\""},{"path":"r-zweiter-blick.html","id":"programmieren-mit-dem-tidyverse","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.6.3 Programmieren mit dem Tidyverse","text":"Das Programmieren mit dem Tidyvers ist nicht ganz einfach und hier nicht näher ausgeführt.\nEine Einführung findet sich z.B.Tidyeval fünf Minuten (Video)Kapiteln 17-21 Advanced R, 2nd EdEin Überblicksdiagramm findet sich hier Quelle.","code":""},{"path":"r-zweiter-blick.html","id":"aufgaben-1","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.7 Aufgaben","text":"Aufgabe\nSchreiben Sie eine Funktion, mit folgendem Algorithmus, wobei ein\nbeliebiger Vektor als Eingabe verlangt wird.\nZähle die Anzahl verschiedener Werte.\nWenn es nur zwei verschiedene Werte gibt, gebe TRUE zurück,\nansonsten FALSE.\nHinweise:\nWählen Sie einen treffenden Namen für Ihre Funktion (nutzen Sie\nbesten ein konsistentes Namensschema).\nWichtigster Tipp: Googeln :-)\nVerschiedene Werte eines Vektors gibt die Funktion unique()\nzurück.\nVermutlich gibt es schon viele Lösungen (Implementierungen) für\ndiese Funktion. Ist nur als Übung gedacht :-)\nLösung\nhas_two_levels <- function(vec){\n\n  # input: vector type\n  # value: number unique values (double)\n\n  tmp <- length(unique(vec))\n  tmp == 2\n}\nlevels ist ein Ausdruck, der nahelegt, dass es sich um\nverschiedene Werte (“Ausprägungen” oder “Stufen”) handelt.\nAlternativ könnte man die Funktion auch schreiben:\nhas_two_values <- function(vec){\n\n  # input: vector type\n  # value: number unique values (double)\n\n  step1 <- unique(vec)\n  step2 <- length(step1)\n  step3 <- (step2 == 2) {\n    <- TRUE\n  } else {\n    <- FALSE\n  }\n\n  <- step2\n  return()\n\n}\nDas ist expliziter, aber länger.\nWenn man es genau nimmt, heißt binär, dass es nur die Werte 0\nund 1 gibt. Das ist ein strengeres Kriterium, wie dass es zwei\nbeliebigen verschiedene Werte gibt (s. unten dazu ein Vorschlag).\nTesten wir unsere Funktion, Test 1:\nx <- c(1,2, 3, 3, 3, 1)\nx2 <- c(\"\", \"B\")\n\n\nhas_two_levels(x2)\n## [1] TRUE\nhas_two_levels(x)\n## [1] FALSE\nTest 2:\ndata(mtcars)\nWir wenden unsere neue Funktion mit Tidyverse-Methoden :\nmtcars %>% \n  summarise(am_has_two_values = has_two_levels(),\n            mpg_has_two_values = has_two_levels(mpg))\n##   am_has_two_values mpg_has_two_values\n## 1              TRUE              FALSE\nBonus!\nVerwenden Sie across() (dplyr), um alle Spalten von mtcars\nmit has_two_levels() zu überprüfen.\nmtcars %>% \n  summarise(across(everything(),\n                   has_two_levels))\n##     mpg   cyl  disp    hp  drat    wt\n## 1 FALSE FALSE FALSE FALSE FALSE FALSE\n##    qsec   vs    gear  carb\n## 1 FALSE TRUE TRUE FALSE FALSE\nKann man auch schreiben:\nmtcars %>% \n  summarise(across(everything(),\n                   ~ has_two_levels(.)))\n##     mpg   cyl  disp    hp  drat    wt\n## 1 FALSE FALSE FALSE FALSE FALSE FALSE\n##    qsec   vs    gear  carb\n## 1 FALSE TRUE TRUE FALSE FALSE\nBonus-Bonus:\nkönnte man eine Funktion schreiben, die prüft, ob die\nAusprägungen eines Vektors binär sind, d.h. nur 0 oder 1:\nis_binary <- function(var){\n  return((var %% c(0,1)))\n}AufgabeSchreiben Sie eine Funktion, mit folgendem Algorithmus, wobei ein\nbeliebiger Vektor als Eingabe verlangt wird.Zähle die Anzahl verschiedener Werte.Wenn es nur zwei verschiedene Werte gibt, gebe TRUE zurück,\nansonsten FALSE.Hinweise:Wählen Sie einen treffenden Namen für Ihre Funktion (nutzen Sie\nbesten ein konsistentes Namensschema).Wichtigster Tipp: Googeln :-)Verschiedene Werte eines Vektors gibt die Funktion unique()\nzurück.Vermutlich gibt es schon viele Lösungen (Implementierungen) für\ndiese Funktion. Ist nur als Übung gedacht :-)Lösunglevels ist ein Ausdruck, der nahelegt, dass es sich um\nverschiedene Werte (“Ausprägungen” oder “Stufen”) handelt.Alternativ könnte man die Funktion auch schreiben:Das ist expliziter, aber länger.Wenn man es genau nimmt, heißt binär, dass es nur die Werte 0\nund 1 gibt. Das ist ein strengeres Kriterium, wie dass es zwei\nbeliebigen verschiedene Werte gibt (s. unten dazu ein Vorschlag).Testen wir unsere Funktion, Test 1:Test 2:Wir wenden unsere neue Funktion mit Tidyverse-Methoden :Bonus!Verwenden Sie across() (dplyr), um alle Spalten von mtcars\nmit has_two_levels() zu überprüfen.Kann man auch schreiben:Bonus-Bonus:könnte man eine Funktion schreiben, die prüft, ob die\nAusprägungen eines Vektors binär sind, d.h. nur 0 oder 1:Aufgabe\nSchreiben Sie eine Funktion zur Berechnung der Anzahl der fehlenden\nWerte einem (numerischen) Vektor!\nHinweise:\nWählen Sie einen treffenden Namen für Ihre Funktion (nutzen Sie\nbesten ein konsistentes Namensschema).\nLösung\nna_n <- function(num_vec) {\n  # input: num. vector (int double)\n  # value: count missing values (double)\n\n  (!.numeric(num_vec)) stop(\"Numeric input needed!\")\n  <- sum(.na(num_vec))\n  return()\n\n}\nTest:\nx <- c(1,2, NA, NA)\nx2 <- c(\"\", \"B\", NA)\n\nna_n(x)\n## [1] 2\nBei x2 sollte ein Fehler aufgeworfen werden:\nna_n(x2)\n## Error na_n(x2): Numeric input needed!\nGut!\nTesten wir weiter, jetzt mit dem Datensatz mtcars:\nmtcars[1, c(1,2,3,4)] <- NA  # Zeilen/Spalte\n\nmtcars %>% \n  summarise(mpg_na_n = na_n(mpg))\n##   mpg_na_n\n## 1        1\nBONUS!\nVerwenden Sie across(), um die fehlenden Werte allen Spalten\nvon mtcars zu zählen.\nWer schnell ist, der nehme gerne nycflights13::flights (aus dem\nPaket nycflights13 oder per CSV-Datei aus geeigneter Stelel aus\ndem Internet. Meistens geht es schneller, die Daten aus einem Paket\nzu laden mit data(flights) nachdem man library(nycflights13)\ngeschrieben hat).\nmtcars %>% \n  summarise(across(everything(), na_n)) \n##   mpg cyl disp hp drat wt qsec vs \n## 1   1   1    1  1    0  0    0  0  0\n##   gear carb\n## 1    0    0\nMit pivot_longer() ist es häufig übersichtlicher bzw. besser für\nweitere Bearbeitungsschritte, wie das folgende Beispiel zeigt:\nmtcars %>% \n  summarise(across(everything(), na_n)) %>% \n  pivot_longer(everything()) %>% \n  filter(value > 0)\n## # tibble: 4 × 2\n##   name  value\n##   <chr> <int>\n## 1 mpg       1\n## 2 cyl       1\n## 3 disp      1\n## 4 hp        1\nflights:\ndata(flights, package = \"nycflights13\")\n\nflights %>% \n  select((.numeric)) %>% \n  summarise(across(everything(),\n                   na_n)) %>% \n  pivot_longer(everything()) %>% \n  arrange(-value) %>%  # Sortiere absteigend nach Anzahl der fehlenden Werte\n  filter(value > 0)  # Zeige nur Variablen mit fehlenden Werten\n## # tibble: 5 × 2\n##   name      value\n##   <chr>     <int>\n## 1 arr_delay  9430\n## 2 air_time   9430\n## 3 arr_time   8713\n## 4 dep_time   8255\n## 5 dep_delay  8255AufgabeSchreiben Sie eine Funktion zur Berechnung der Anzahl der fehlenden\nWerte einem (numerischen) Vektor!Hinweise:Wählen Sie einen treffenden Namen für Ihre Funktion (nutzen Sie\nbesten ein konsistentes Namensschema).LösungTest:Bei x2 sollte ein Fehler aufgeworfen werden:Gut!Testen wir weiter, jetzt mit dem Datensatz mtcars:BONUS!Verwenden Sie across(), um die fehlenden Werte allen Spalten\nvon mtcars zu zählen.Wer schnell ist, der nehme gerne nycflights13::flights (aus dem\nPaket nycflights13 oder per CSV-Datei aus geeigneter Stelel aus\ndem Internet. Meistens geht es schneller, die Daten aus einem Paket\nzu laden mit data(flights) nachdem man library(nycflights13)\ngeschrieben hat).Mit pivot_longer() ist es häufig übersichtlicher bzw. besser für\nweitere Bearbeitungsschritte, wie das folgende Beispiel zeigt:flights:Aufgabe\nErstellen Sie für jede Variable aus mtcars ein Histogramm!\nHinweise:\nNutzen Sie eine Wiederholungsstruktur. Schreiben Sie prägnante\nSyntax.\nOptional: Lassen Sie dichotome (zweiwertige) Variablen aus.\nHier\n(und vielen anderen Stellen im Netz) finden Sie Tipps.\nLösung\nhas_two_levels <- function(vec){\n\n  # input: vector type\n  # value: number unique values (double)\n\n  tmp <- length(unique(vec))\n  tmp == 2\n}\nmtcars_hist <- function(col){\n  mtcars %>% \n    ggplot(aes(x = col)) +\n    geom_histogram()\n}\nmtcars %>% \n  select((.numeric)) %>% \n  select((negate(has_two_levels))) %>%   # `!` zum Negieren geht leider nicht\n  map(mtcars_hist)\n## $mpg\n## \n## $cyl\n## \n## $disp\n## \n## $hp\n## \n## $drat\n## \n## $wt\n## \n## $qsec\n## \n## $gear\n## \n## $carb\nAufgabeErstellen Sie für jede Variable aus mtcars ein Histogramm!Hinweise:Nutzen Sie eine Wiederholungsstruktur. Schreiben Sie prägnante\nSyntax.Optional: Lassen Sie dichotome (zweiwertige) Variablen aus.Hier\n(und vielen anderen Stellen im Netz) finden Sie Tipps.Lösung","code":"has_two_levels <- function(vec){\n\n  # input: vector of any type\n  # value: number of unique values (double)\n\n  tmp <- length(unique(vec))\n  tmp == 2\n}has_two_values <- function(vec){\n\n  # input: vector of any type\n  # value: number of unique values (double)\n\n  step1 <- unique(vec)\n  step2 <- length(step1)\n  step3 <- if(step2 == 2) {\n    out <- TRUE\n  } else {\n    out <- FALSE\n  }\n\n  out <- step2\n  return(out)\n\n}x <- c(1,2, 3, 3, 3, 1)\nx2 <- c(\"A\", \"B\")\n\n\nhas_two_levels(x2)## [1] TRUEhas_two_levels(x)## [1] FALSEdata(mtcars)mtcars %>% \n  summarise(am_has_two_values = has_two_levels(am),\n            mpg_has_two_values = has_two_levels(mpg))##   am_has_two_values mpg_has_two_values\n## 1              TRUE              FALSEmtcars %>% \n  summarise(across(everything(),\n                   has_two_levels))##     mpg   cyl  disp    hp  drat    wt\n## 1 FALSE FALSE FALSE FALSE FALSE FALSE\n##    qsec   vs   am  gear  carb\n## 1 FALSE TRUE TRUE FALSE FALSEmtcars %>% \n  summarise(across(everything(),\n                   ~ has_two_levels(.)))##     mpg   cyl  disp    hp  drat    wt\n## 1 FALSE FALSE FALSE FALSE FALSE FALSE\n##    qsec   vs   am  gear  carb\n## 1 FALSE TRUE TRUE FALSE FALSEis_binary <- function(var){\n  return(all(var %in% c(0,1)))\n}na_n <- function(num_vec) {\n  # input: num. vector (int or double)\n  # value: count of missing values (double)\n\n  if (!is.numeric(num_vec)) stop(\"Numeric input is needed!\")\n  out <- sum(is.na(num_vec))\n  return(out)\n\n}x <- c(1,2, NA, NA)\nx2 <- c(\"A\", \"B\", NA)\n\nna_n(x)## [1] 2na_n(x2)## Error in na_n(x2): Numeric input is needed!mtcars[1, c(1,2,3,4)] <- NA  # Zeilen/Spalte\n\nmtcars %>% \n  summarise(mpg_na_n = na_n(mpg))##   mpg_na_n\n## 1        1mtcars %>% \n  summarise(across(everything(), na_n)) ##   mpg cyl disp hp drat wt qsec vs am\n## 1   1   1    1  1    0  0    0  0  0\n##   gear carb\n## 1    0    0mtcars %>% \n  summarise(across(everything(), na_n)) %>% \n  pivot_longer(everything()) %>% \n  filter(value > 0)## # A tibble: 4 × 2\n##   name  value\n##   <chr> <int>\n## 1 mpg       1\n## 2 cyl       1\n## 3 disp      1\n## 4 hp        1data(flights, package = \"nycflights13\")\n\nflights %>% \n  select(where(is.numeric)) %>% \n  summarise(across(everything(),\n                   na_n)) %>% \n  pivot_longer(everything()) %>% \n  arrange(-value) %>%  # Sortiere absteigend nach Anzahl der fehlenden Werte\n  filter(value > 0)  # Zeige nur Variablen mit fehlenden Werten## # A tibble: 5 × 2\n##   name      value\n##   <chr>     <int>\n## 1 arr_delay  9430\n## 2 air_time   9430\n## 3 arr_time   8713\n## 4 dep_time   8255\n## 5 dep_delay  8255has_two_levels <- function(vec){\n\n  # input: vector of any type\n  # value: number of unique values (double)\n\n  tmp <- length(unique(vec))\n  tmp == 2\n}mtcars_hist <- function(col){\n  mtcars %>% \n    ggplot(aes(x = col)) +\n    geom_histogram()\n}mtcars %>% \n  select(where(is.numeric)) %>% \n  select(where(negate(has_two_levels))) %>%   # `!` zum Negieren geht leider nicht\n  map(mtcars_hist)## $mpg\n## \n## $cyl\n## \n## $disp\n## \n## $hp\n## \n## $drat\n## \n## $wt\n## \n## $qsec\n## \n## $gear\n## \n## $carb"},{"path":"r-zweiter-blick.html","id":"vertiefung-1","chapter":"Kapitel 4 R, zweiter Blick","heading":"4.8 Vertiefung","text":"Funktionale Programmierung mit RLernen Sie Wiederholungsstrukturen mit ggplotFallstudie FlugverspätungenFallstudie Getreideernte","code":""},{"path":"tidymodels.html","id":"tidymodels","chapter":"Kapitel 5 tidymodels","heading":"Kapitel 5 tidymodels","text":"Benötigte R-Pakete für dieses Kapitel:","code":""},{"path":"tidymodels.html","id":"lernsteuerung-2","chapter":"Kapitel 5 tidymodels","heading":"5.1 Lernsteuerung","text":"","code":""},{"path":"tidymodels.html","id":"vorbereitung-3","chapter":"Kapitel 5 tidymodels","heading":"5.1.1 Vorbereitung","text":"Lesen Sie TMWR, Kapitel 1Lesen Sie übrige Literatur zu diesem Thema","code":""},{"path":"tidymodels.html","id":"lernziele-3","chapter":"Kapitel 5 tidymodels","heading":"5.1.2 Lernziele","text":"Sie sind der Lage, Regressionsmodelle mit dem tidymodels-Ansatz zu spezifizieren","code":""},{"path":"tidymodels.html","id":"literatur-3","chapter":"Kapitel 5 tidymodels","heading":"5.1.3 Literatur","text":"TMWR, Kap. 1, 5, 6, 7, 8, 9","code":""},{"path":"tidymodels.html","id":"daten","chapter":"Kapitel 5 tidymodels","heading":"5.2 Daten","text":"Dieser Abschnitt bezieht sich auf Kapitel 4 Silge Kuhn (2022).Wir benutzen den Datensatz zu Immobilienpreise aus dem Ames County Iowa, USA,\ngelegen im Zentrum des Landes.Hier wurde die AV log-transformiert.\nDas hat zwei (wichtige) Effekte:Die Verteilung ist symmetrischer, näher der Normalverteilung. Damit gibt es mehr Daten im Hauptbereich des Ranges von Sale_Price, die Vorhersagen stabiler machen dürfte.Logarithmiert man die Y-Variable, kommt dies einem multiplikativen Modell gleich, s. auch hier.","code":""},{"path":"tidymodels.html","id":"train--vs-test-datensatz-aufteilen","chapter":"Kapitel 5 tidymodels","heading":"5.3 Train- vs Test-Datensatz aufteilen","text":"Dieser Abschnitt bezieht sich auf Kapitel 5 Silge Kuhn (2022).Das Aufteilen Train- und Test-Datensatz ist einer der wesentlichen Grundsätze im maschinellen Lernen. Das Ziel ist, Overfitting abzuwenden.Im Train-Datensatz werden alle Modelle berechnet.\nDer Test-Datensatz wird nur einmal verwendet, und zwar zur Überprüfung der Modellgüte.Praktisch funktioniert das Silge Kuhn (2022) wie folgt.Wir laden die Daten und erstellen einen Index,\nder jeder Beobachtung die Zuteilung zu Train- bzw. zum Test-Datensatz zuweist:initial_split() speichert für spätere komfortable Verwendung auch die Daten.\nAber eben auch der Index, der bestimmt, welche Beobachtung im Train-Set landet:Praktisch ist auch,\ndass die AV-Verteilung beiden Datensätzen ähnlich gehalten wird (Stratifizierung),\ndas besorgt das Argument strata.Die eigentlich Aufteilung die zwei Datensätze geht dann :","code":"##  [1]  2 27 28 30 32 33 35 78 79 83## [1] 2342"},{"path":"tidymodels.html","id":"grundlagen-der-modellierung-mit-tidymodels","chapter":"Kapitel 5 tidymodels","heading":"5.4 Grundlagen der Modellierung mit tidymodels","text":"Dieser Abschnitt bezieht sich auf Kapitel 6 Silge Kuhn (2022).tidymodels ist eine Sammlung mehrerer, zusammengehöriger Pakete,\neben zum Thema statistische Modellieren.Das kann man analog zur Sammlung tidyverse verstehen,\nzu der z.B. das R-Paket dplyr gehört.Das R-Paket innerhalb von tidymodels, das zum “Fitten” von Modellen zuständig ist, heißt parsnip.Eine Liste der verfügbaren Modelltypen, Modellimplementierungen und Modellparameter, die Parsnip aktuell unterstützt werden, findet sich hier.","code":""},{"path":"tidymodels.html","id":"modelle-spezifizieren","chapter":"Kapitel 5 tidymodels","heading":"5.4.1 Modelle spezifizieren","text":"Ein (statistisches) Modell wird Tidymodels mit drei Elementen spezifiziert, vgl. Abb. 5.1.\nFigure 5.1: Definition eines Models tidymodels\nDie Definition eines Modells tidymodels folgt diesen Ideen:Das Modell sollte unabhängig von den Daten spezifiziert seinDas Modell sollte unabhängig von den Variablen (AV, UVs) spezifiziert seinDas Modell sollte unabhängig von etwaiger Vorverarbeitung (z.B. z-Transformation) spezifiziert seinDa bei einer linearen Regression nur der Modus “Regression” möglich ist,\nmuss der Modus diesem Fall nicht angegeben werden.\nTidymodels erkennt das automatisch.","code":""},{"path":"tidymodels.html","id":"modelle-berechnen","chapter":"Kapitel 5 tidymodels","heading":"5.4.2 Modelle berechnen","text":"Nach Rhys (2020) ist ein Modell sogar erst ein Modell,\nwenn die Koeffizienten berechnet sind.\nTidymodels kennt diese Unterscheidung nicht.\nStattdessen spricht man Tidymodels von einem “gefitteten” Modell,\nsobald es berechnet ist.\nÄhnlich fancy könnte man von einem “instantiierten” Modell sprechen.Für das Beispiel der einfachen linearen Regression heißt das,\ndas Modell ist gefittet,\nsobald die Steigung und der Achsenabschnitt (sowie die Residualstreuung)\nberechnet sind.","code":""},{"path":"tidymodels.html","id":"vorhersagen-1","chapter":"Kapitel 5 tidymodels","heading":"5.4.3 Vorhersagen","text":"Im maschinellen Lernen ist man primär den Vorhersagen interessiert,\nhäufig nur Punktschätzungen.\nSchauen wir uns also zunächst diese .Vorhersagen bekommt man recht einfach mit der predict() Methode:Die Syntax lautet predict(modell, daten_zum_vorhersagen).","code":"## # A tibble: 6 × 1\n##   .pred\n##   <dbl>\n## 1  5.28\n## 2  5.28\n## 3  5.28\n## 4  5.28\n## 5  5.28\n## 6  5.27"},{"path":"tidymodels.html","id":"vorhersagen-im-train-datensatz","chapter":"Kapitel 5 tidymodels","heading":"5.4.4 Vorhersagen im Train-Datensatz","text":"Vorhersagen im Train-Datensatz machen keinen Sinn,\nda sie nicht gegen Overfitting geschützt sind und daher deutlich zu optimistisch sein können.Bei einer linearen Regression ist diese Gefahr nicht hoch,\naber bei anderen, flexibleren Modellen, ist diese Gefahr absurd groß.","code":""},{"path":"tidymodels.html","id":"modellkoeffizienten-im-train-datensatz","chapter":"Kapitel 5 tidymodels","heading":"5.4.5 Modellkoeffizienten im Train-Datensatz","text":"Gibt man den Namen des Modellobjekts ein,\nwird ein Überblick relevanten Modellergebnissen Bildschirm gedruckt:Innerhalb des Ergebnisobjekts findet sich eine Liste namens fit,\nder die Koeffizienten (der “Fit”) abgelegt sind:Zum Herausholen dieser Infos kann man auch die Funktion extract_fit_engine() verwenden:Das extrahierte Objekt ist, diesem Fall,\ndas typische lm() Objekt.\nEntsprechend kann man daruaf coef() oder summary() anwenden.Schicker sind die Pendant-Befehle aus broom,\ndie jeweils einen Tibble zuückliefern:","code":"## parsnip model object\n## \n## \n## Call:\n## stats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n## \n## Coefficients:\n## (Intercept)    Longitude     Latitude  \n##    -311.111       -2.098        2.853## \n## Call:\n## stats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n## \n## Coefficients:\n## (Intercept)    Longitude     Latitude  \n##    -311.111       -2.098        2.853## \n## Call:\n## stats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n## \n## Coefficients:\n## (Intercept)    Longitude     Latitude  \n##    -311.111       -2.098        2.853## (Intercept)   Longitude    Latitude \n## -311.111432   -2.097629    2.852511## \n## Call:\n## stats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.02140 -0.09632 -0.01682  0.09907  0.57741 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -311.1114    14.7090  -21.15   <2e-16 ***\n## Longitude     -2.0976     0.1302  -16.11   <2e-16 ***\n## Latitude       2.8525     0.1828   15.61   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1615 on 2339 degrees of freedom\n## Multiple R-squared:  0.1705, Adjusted R-squared:  0.1698 \n## F-statistic: 240.4 on 2 and 2339 DF,  p-value: < 2.2e-16## # A tibble: 3 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)  -311.      14.7       -21.2 5.27e-91\n## 2 Longitude      -2.10     0.130     -16.1 1.83e-55\n## 3 Latitude        2.85     0.183      15.6 2.64e-52## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik    AIC    BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl>  <dbl>  <dbl>\n## 1     0.171         0.170 0.162      240. 1.10e-95     2   948. -1888. -1865.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>"},{"path":"tidymodels.html","id":"parsnip-rstudio-add-in","chapter":"Kapitel 5 tidymodels","heading":"5.4.6 Parsnip RStudio add-in","text":"Mit dem Add-von Parsnip kann man sich eine Modellspezifikation per Klick ausgeben lassen.\nNett!","code":""},{"path":"tidymodels.html","id":"workflows","chapter":"Kapitel 5 tidymodels","heading":"5.5 Workflows","text":"Dieser Abschnitt bezieht sich auf Kapitel 7 Silge Kuhn (2022).","code":""},{"path":"tidymodels.html","id":"konzept-des-workflows-in-tidymodels","chapter":"Kapitel 5 tidymodels","heading":"5.5.1 Konzept des Workflows in Tidymodels","text":"\nFigure 5.2: Definition eines Models tidymodels\n","code":""},{"path":"tidymodels.html","id":"einfaches-beispiel","chapter":"Kapitel 5 tidymodels","heading":"5.5.2 Einfaches Beispiel","text":"Wir initialisieren einen Workflow,\nverzichten auf Vorverarbeitung und fügen ein Modell hinzu:Werfen wir einen Blick das Workflow-Objekt:Wie man sieht,\ngehört die Modellformel (y ~ x) zur Vorverarbeitung\naus Sicht von Tidymodels.war nochmal im Objekt lm_model enthalten?Jetzt können wir das Modell berechnen (fitten):Natürlich kann man synonym auch schreiben:Schauen wir uns das Ergebnis :","code":"## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Formula\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## Sale_Price ~ Longitude + Latitude\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Formula\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## Sale_Price ~ Longitude + Latitude\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)    Longitude     Latitude  \n##    -311.111       -2.098        2.853"},{"path":"tidymodels.html","id":"vorhersage-mit-einem-workflow","chapter":"Kapitel 5 tidymodels","heading":"5.5.3 Vorhersage mit einem Workflow","text":"Die Vorhersage mit einem Tidymodels-Workflow ist einerseits komfortabel,\nda man einfach sagen kann:“Nimm die richtigen Koeffizienten des Modells aus dem Train-Set\nund wende sie auf das Test-Sample . Berechne mir\ndie Vorhersagen und die Modellgüte.”sieht das aus:Anderseits wird auch ein recht komplexes Objekt zurückgeliefert,\ndas man erst mal durchschauen muss.Wie man sieht, gibt es mehrere Listenspalten.\nBesonders interessant erscheinen natürlich die Listenspalten .metrics und .predictions.Schauen wir uns die Vorhersagen .Es gibt auch eine Funktion, die obige Zeile vereinfacht (also synonym ist):","code":"## # Resampling results\n## # Manual resampling \n## # A tibble: 1 × 6\n##   splits             id               .metrics .notes   .predictions .workflow \n##   <list>             <chr>            <list>   <list>   <list>       <list>    \n## 1 <split [2342/588]> train/test split <tibble> <tibble> <tibble>     <workflow>## # A tibble: 5 × 5\n##   id               .pred  .row Sale_Price .config             \n##   <chr>            <dbl> <int>      <dbl> <chr>               \n## 1 train/test split  5.28     7       5.33 Preprocessor1_Model1\n## 2 train/test split  5.28     8       5.28 Preprocessor1_Model1\n## 3 train/test split  5.28     9       5.37 Preprocessor1_Model1\n## 4 train/test split  5.28    10       5.28 Preprocessor1_Model1\n## 5 train/test split  5.28    11       5.25 Preprocessor1_Model1"},{"path":"tidymodels.html","id":"modellgüte","chapter":"Kapitel 5 tidymodels","heading":"5.5.4 Modellgüte","text":"Dieser Abschnitt bezieht sich auf Kapitel 9 Silge Kuhn (2022).Die Vorhersagen bilden die Basis für die Modellgüte (“Metriken”),\ndie schon fertig berechnet im Objekt final_lm_res liegen und mit\ncollect_metrics herausgenommen werden können:Man kann auch angeben,\nwelche Metriken der Modellgüte man bekommen möchte:","code":"## # A tibble: 3 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       0.159\n## 2 rsq     standard       0.180\n## 3 mae     standard       0.121"},{"path":"tidymodels.html","id":"vorhersage-von-hand","chapter":"Kapitel 5 tidymodels","heading":"5.5.5 Vorhersage von Hand","text":"Man kann sich die Metriken auch von Hand ausgeben lassen,\nwenn man direktere Kontrolle haben möchte als mit last_fit und collect_metrics.Jetzt binden wir die Spalten zusammen, also die “Wahrheit” (\\(y\\)) und die Vorhersagen:Andere Koeffizienten der Modellgüte können mit rmse oder mae abgerufen werden.","code":"## # A tibble: 5 × 1\n##   .pred\n##   <dbl>\n## 1  5.28\n## 2  5.28\n## 3  5.28\n## 4  5.28\n## 5  5.28## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rsq     standard     0.00687"},{"path":"tidymodels.html","id":"rezepte-zur-vorverarbeitung","chapter":"Kapitel 5 tidymodels","heading":"5.6 Rezepte zur Vorverarbeitung","text":"Dieser Abschnitt bezieht sich auf Kapitel 8 Silge Kuhn (2022).","code":""},{"path":"tidymodels.html","id":"was-ist-rezept-und-wozu-ist-es-gut","chapter":"Kapitel 5 tidymodels","heading":"5.6.1 Was ist Rezept und wozu ist es gut?","text":"könnte ein typischer Aufruf von lm() aussehen:Neben dem Fitten des Modells besorgt die Formel-Schreibweise noch einige zusätzliche nützliche Vorarbeitung:Definition von AV und AVLog-Transformation von Gr_Liv_AreaTransformation der nominalen Variablen Dummy-VariablenDas ist schön und nütlich, hat aber auch Nachteile:Das Modell wird nicht nur spezifiziert, sondern auch gleich berechnet. Das ist unpraktisch, weil man die Modellformel vielleicht anderen Modell wiederverwenden möchte. Außerdem kann das Berechnen lange dauern.Die Schritte sind ineinander vermengt, dass man nicht einfach und übersichtlich die einzelnen Schritte bearbeiten kann.Praktischer wäre also, die Schritte der Vorverarbeitung zu ent-flechten.\nDas geht mit einem “Rezept” aus Tidmoodels:Ein Rezept berechnet kein Modell. Es macht nichts außer die Vorverarbeitung des Modells zu spezizifieren (inklusive der Modellformel).","code":"## Recipe\n## \n## Inputs:\n## \n##       role #variables\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Log transformation on Gr_Liv_Area\n## Dummy variables from all_nominal_predictors()"},{"path":"tidymodels.html","id":"workflows-mit-rezepten","chapter":"Kapitel 5 tidymodels","heading":"5.6.2 Workflows mit Rezepten","text":"Jetzt definieren wir den Workflow nicht nur mit einer Modellformel,\nsondern mit einem Rezept:Sonst hat sich nichts geändert.Wie vorher, können wir jetzt das Modell berechnen.","code":"## # Resampling results\n## # Manual resampling \n## # A tibble: 1 × 6\n##   splits             id               .metrics .notes   .predictions .workflow \n##   <list>             <chr>            <list>   <list>   <list>       <list>    \n## 1 <split [2342/588]> train/test split <tibble> <tibble> <tibble>     <workflow>\n## \n## There were issues with some computations:\n## \n##   - Warning(s) x1: prediction from a rank-deficient fit may be misleading\n## \n## Use `collect_notes(object)` for more information.## # A tibble: 2 × 4\n##   .metric .estimator .estimate .config             \n##   <chr>   <chr>          <dbl> <chr>               \n## 1 rmse    standard      0.0819 Preprocessor1_Model1\n## 2 rsq     standard      0.785  Preprocessor1_Model1"},{"path":"tidymodels.html","id":"spaltenrollen","chapter":"Kapitel 5 tidymodels","heading":"5.6.3 Spaltenrollen","text":"Eine praktische Funktion ist es,\nbestimmte Spalten nicht als Prädiktor,\nsondern als ID-Variable zu nutzen.\nDas kann man Tidymodels komfortabel wie folgt angeben:","code":"## Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         id          1\n##    outcome          1\n##  predictor          3\n## \n## Operations:\n## \n## Log transformation on Gr_Liv_Area\n## Dummy variables from all_nominal_predictors()"},{"path":"tidymodels.html","id":"fazit","chapter":"Kapitel 5 tidymodels","heading":"5.6.4 Fazit","text":"Mehr zu Rezepten findet sich hier.\nEin Überblick zu allen Schritten der Vorverarbeitung findet sich hier.","code":""},{"path":"tidymodels.html","id":"aufgaben-2","chapter":"Kapitel 5 tidymodels","heading":"5.7 Aufgaben","text":"Fallstudie SeegurkenSehr einfache Fallstudie zur Modellierung einer Regression mit tidymodelsFallstudie zur linearen Regression mit Tidymodels","code":""},{"path":"knn.html","id":"knn","chapter":"Kapitel 6 kNN","heading":"Kapitel 6 kNN","text":"","code":""},{"path":"knn.html","id":"lernsteuerung-3","chapter":"Kapitel 6 kNN","heading":"6.1 Lernsteuerung","text":"","code":""},{"path":"knn.html","id":"lernziele-4","chapter":"Kapitel 6 kNN","heading":"6.1.1 Lernziele","text":"Sie sind der Lage, einfache Klassifikationsmodelle zu spezifizieren mit tidymodelsSie können den knn-Algorithmus erläuternSie können den knn-Algorithmus tidymodels anwendenSie können die Gütemetriken von Klassifikationsmodellen einschätzen","code":""},{"path":"knn.html","id":"literatur-4","chapter":"Kapitel 6 kNN","heading":"6.1.2 Literatur","text":"Rhys, Kap. 3Timbers et al., Kap. 5","code":""},{"path":"knn.html","id":"überblick-1","chapter":"Kapitel 6 kNN","heading":"6.2 Überblick","text":"diesem Kapitel geht es um das Verfahren KNN, K-Nächste-Nachbarn.Benötigte R-Pakete für dieses Kapitel:","code":""},{"path":"knn.html","id":"intuitive-erklärung","chapter":"Kapitel 6 kNN","heading":"6.3 Intuitive Erklärung","text":"K-Nächste-Nachbarn (k nearest neighbors, kNN) ist ein einfacher Algorithmus des maschinellen Lernens,\nder sowohl für Klassifikation als auch für numerische Vorhersage (Regression) genutzt werden kann.\nWir werden kNN als Beispiel für eine Klassifikation betrachten.Betrachen wir ein einführendes Beispiel von Rhys (2020), für das es eine Online-Quelle gibt.\nStellen Sie sich vor, wir laufen durch englische Landschaft,\nvielleicht die Grafschaft Kent, und sehen ein kleines Tier durch das Gras huschen.\nEine Schlange?!\nEngland gibt es (laut Rhys (2020)) nur eine giftige Schlange,\ndie Otter (Adder).\nEine andere Schlange, die Grass Snake ist nicht giftig,\nund dann kommt noch der Slow Worm Frage,\nder gar nicht zur Familie der Schlangen gehört.\nPrimär interessiert uns die Frage, haben wir jetzt eine Otter gesehen?\nOder für ein Tier war es?Zum Glück wissen wir einiges über Schlangen bzw. schlangenähnliche Tiere Englands.\nNämlich können wir die betreffenden Tierarten Größe und Aggressivität einschätzen,\ndas ist Abbildung 6.1 dargestellt.\nFigure 6.1: Haben wir gerade eine Otter gesehen?\nDer Algorithmus von kNN sieht einfach gesagt vor,\ndass wir schauen, welcher Tierarten Tiere mit ähnlicher Aggressivität und Größe angehören.\nDie Tierart die bei diesen “Nachbarn” hinsichtlich Ähnlichkeit relevanter Merkmale häufigsten vertreten ist, ordnen wir die bisher unklassifizierte Beobachtung zu.Etwas zugespitzt:Wenn es quakt wie eine Ente, läuft wie eine Ente und aussieht wie eine Ente, dann ist es eine Ente.Die Anzahl \\(k\\) der nächsten Nachbarn können wir frei wählen;\nder Wert wird nicht vom Algorithmuss bestimmt.\nSolche vom Nutzi zu bestimmenden Größen nennt man auch Tuningparameter.","code":""},{"path":"knn.html","id":"krebsdiagnostik","chapter":"Kapitel 6 kNN","heading":"6.4 Krebsdiagnostik","text":"Betrachten wir ein Beispiel von Timbers, Campbell, Lee (2022),\ndas hier frei eingesehen werden kann.Die Daten sind zu beziehen:diesem Beispiel versuchen wir Tumore der Brust zu klassifizieren,\nob sie einen schweren Verlauf (maligne, engl. malignant) oder einen weniger schweren Verlauf (benigne, engl. benign) erwarten lassen.\nDer Datensatz ist hier näher erläutert.Wie Abb. 6.2 ersichtlich,\nsteht eine Tumordiagnose (malignant vs. benign) Abhängigkeit\nvon Umfang (engl. perimeter) und Konkavität,\ndie “Gekrümmtheit nach innen”.\nFigure 6.2: Streudiagramm zur Einschätzung von Tumordiagnosen\nWichtig ist, dass die Merkmale standardisiert sind, also eine identische Skalierung aufweisen,\nda sonst das Merkmal mit kleinerer Skala weniger\ndie Berechnung der Nähe (bzw. Abstand) eingeht.Für einen neuen, bisher unklassifizierten Fall suchen nur nun nach einer Diagnose,\nalso nach der besten passenden Diagnose (maligne oder benigne),\ns. Abb. 6.3, wieder aus Timbers, Campbell, Lee (2022).\nIhr Quellcode für dieses Diagramm (und das ganze Kapitel) findet sich hier.\nFigure 6.3: Ein neuer Fall, bisher unklassifiziert\nWir können zunächst den (im euklidischen Koordinatensystem) nächst gelegenen Fall (der “nächste Nachbar”) betrachten,\nund vereinbaren,\ndass wir dessen Klasse als Schätzwert für den unklassiffizierten Fall übernehmen,\ns. Abb. 6.4.\nFigure 6.4: Ein nächster Nachbar\nBetrachten wir einen anderen zu klassifizierenden Fall, s. Abb 6.5.\nOb hier die Klassifikation von “benign” korrekt ist?\nWomöglich nicht, denn viele andere Nachbarn,\ndie etwas weiter weg gelegen sind, gehören zur anderen Diagnose, malign.\nFigure 6.5: Trügt der nächste Nachbar?\nUm die Vorhersage zu verbessern,\nkönnen wir nicht nur den nächst gelegenen Nachbarn betrachten,\nsondern die \\(k\\) nächst gelegenen, z.B. \\(k=3\\), s. Abb 6.6.\nFigure 6.6: kNN mit k=3\nDie Entscheidungsregel ist dann einfach eine Mehrheitsentscheidung:\nWir klassifizieren den neuen Fall entsprechend der Mehrheit den \\(k\\) nächst gelegenen Nachbarn.","code":""},{"path":"knn.html","id":"berechnung-der-nähe","chapter":"Kapitel 6 kNN","heading":"6.5 Berechnung der Nähe","text":"Es gibt verschiedenen Algorithmen,\num die Nähe bzw. Distanz der Nachbarn zum zu klassifizieren Fall zu berechnen.Eine gebräuchliche Methode ist der euklidische Abstand,\nder mit Pythagoras berechnet werden kann, s. Abb. 6.7 aus Sauer (2019).\nFigure 6.7: Euklidischer Abstand wird mit der Regel von Pythagoras berechnet\nWie war das noch mal?\\[c^2 = ^2 + b^2\\]Im Beispiel oben also:\\(c^2 = 3^2 + 4^2 = 5^2\\)Damit gilt: \\(c = \\sqrt{c^2} = \\sqrt{5^2}=5\\).Im 2D-Raum ist das einfach, dass man das (fat) mit bloßem Augenschein entscheiden kann.\nmehr als 2 Dimensionen wird es aber schwierig für das Auge, wie ein Beispiel aus Timbers, Campbell, Lee (2022) zeigt.Allerdings kann man den guten alten Pythagoras auch auf Dreiecke mit mehr als zwei Dimensionen anwenden, s. Abb. 6.8 aus Sauer (2019), Kap. 21.1.2.\nFigure 6.8: Pythagoras der Ebene (links) und 3D (rechts)\nBleiben wir beim Beispiel von Anna und Berta und nehmen wir eine dritte Variable\nhinzu (Statistikliebe).\nSagen wir, der Unterschied dieser dritten Variable zwischen Anna und Berta betrage 2.Es gilt:\\[\n\\begin{aligned}\ne^2 &= c^2 + d^2 \\\\\ne^2 &= 5^2 + 2^2 \\\\\ne^2 &= 25 + 4\\\\\ne &= \\sqrt{29} \\approx 5.4\n\\end{aligned}\n\\]","code":""},{"path":"knn.html","id":"knn-mit-tidymodels","chapter":"Kapitel 6 kNN","heading":"6.6 kNN mit Tidymodels","text":"","code":""},{"path":"knn.html","id":"analog-zu-timbers-et-al.","chapter":"Kapitel 6 kNN","heading":"6.6.1 Analog zu Timbers et al.","text":"Eine Anwendung von kNN mit Tidymodels ist Timbers, Campbell, Lee (2022), Kap. 5.6, hier beschrieben.Die Daten aus Timbers, Campbell, Lee (2022) finden sich diesem Github-Repo-Die (z-transformierten) Daten zur Tumorklassifikation können hier bezogen werden.Timbers, Campbell, Lee (2022) verwenden Kap. 5 auch noch nicht standardisierte Daten, unscales_wdbc.csv, die hier als CSV-Datei heruntergeladen werden können.","code":"## # A tibble: 569 × 3\n##    Class  Area Smoothness\n##    <fct> <dbl>      <dbl>\n##  1 M     1001      0.118 \n##  2 M     1326      0.0847\n##  3 M     1203      0.110 \n##  4 M      386.     0.142 \n##  5 M     1297      0.100 \n##  6 M      477.     0.128 \n##  7 M     1040      0.0946\n##  8 M      578.     0.119 \n##  9 M      520.     0.127 \n## 10 M      476.     0.119 \n## # … with 559 more rows"},{"path":"knn.html","id":"rezept-definieren","chapter":"Kapitel 6 kNN","heading":"6.6.2 Rezept definieren","text":"Und jetzt die z-Transformation:Die Schritte prep() und bake() sparen wir uns, da fit() und predict()\ndas für uns besorgen.","code":"## Recipe\n## \n## Inputs:\n## \n##       role #variables\n##    outcome          1\n##  predictor          2"},{"path":"knn.html","id":"modell-definieren","chapter":"Kapitel 6 kNN","heading":"6.6.3 Modell definieren","text":"","code":"## K-Nearest Neighbor Model Specification (classification)\n## \n## Main Arguments:\n##   neighbors = 5\n##   weight_func = rectangular\n## \n## Computational engine: kknn"},{"path":"knn.html","id":"workflow-definieren","chapter":"Kapitel 6 kNN","heading":"6.6.4 Workflow definieren","text":"","code":"## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: nearest_neighbor()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 2 Recipe Steps\n## \n## • step_scale()\n## • step_center()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## kknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(5,     data, 5), kernel = ~\"rectangular\")\n## \n## Type of response variable: nominal\n## Minimal misclassification: 0.1107206\n## Best kernel: rectangular\n## Best k: 5"},{"path":"knn.html","id":"vorhersagen-2","chapter":"Kapitel 6 kNN","heading":"6.6.5 Vorhersagen","text":"","code":"## # A tibble: 2 × 1\n##   .pred_class\n##   <fct>      \n## 1 B          \n## 2 M"},{"path":"knn.html","id":"mit-train-test-aufteilung","chapter":"Kapitel 6 kNN","heading":"6.7 Mit Train-Test-Aufteilung","text":"Im Kapitel 5 greifen Timbers, Campbell, Lee (2022) die Aufteilung Train- vs. Test-Sample noch nicht auf (aber Kapitel 6).Da diesem Kurs diese Aufteilung aber schon besprochen wurde,\nsoll dies hier auch dargestellt werden.","code":""},{"path":"knn.html","id":"rezept-definieren-1","chapter":"Kapitel 6 kNN","heading":"6.7.1 Rezept definieren","text":"","code":""},{"path":"knn.html","id":"modell-definieren-1","chapter":"Kapitel 6 kNN","heading":"6.7.2 Modell definieren","text":"","code":""},{"path":"knn.html","id":"workflow-definieren-1","chapter":"Kapitel 6 kNN","heading":"6.7.3 Workflow definieren","text":"","code":"## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: nearest_neighbor()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 2 Recipe Steps\n## \n## • step_scale()\n## • step_center()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## \n## Call:\n## kknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(3,     data, 5), kernel = ~\"rectangular\")\n## \n## Type of response variable: nominal\n## Minimal misclassification: 0.129108\n## Best kernel: rectangular\n## Best k: 3"},{"path":"knn.html","id":"vorhersagen-3","chapter":"Kapitel 6 kNN","heading":"6.7.4 Vorhersagen","text":"Im Gegensatz zu Timbers, Campbell, Lee (2022) verwenden wir hier last_fit() und collect_metrics(),\nda wir dies bereits eingeführt haben und künftig darauf aufbauen werden.","code":"## # Resampling results\n## # Manual resampling \n## # A tibble: 1 × 6\n##   splits            id               .metrics .notes   .predictions .workflow \n##   <list>            <chr>            <list>   <list>   <list>       <list>    \n## 1 <split [426/143]> train/test split <tibble> <tibble> <tibble>     <workflow>"},{"path":"knn.html","id":"modellgüte-1","chapter":"Kapitel 6 kNN","heading":"6.7.5 Modellgüte","text":"Die eigentlichen Predictions stecken der Listenspalte .predictions im Fit-Objekt:Genau genommen ist .predictions eine Spalte, der jeder Zeile (und damit Zelle) eine Tabelle (Tibble) steht.\nWir haben nur eine Zeile und wollen das erste Element dieser Spalte herausziehen.\nDa hilft pluck():","code":"## # A tibble: 2 × 4\n##   .metric  .estimator .estimate .config             \n##   <chr>    <chr>          <dbl> <chr>               \n## 1 accuracy binary         0.825 Preprocessor1_Model1\n## 2 roc_auc  binary         0.886 Preprocessor1_Model1## [1] \"splits\"       \"id\"           \".metrics\"     \".notes\"       \".predictions\"\n## [6] \".workflow\"##           Truth\n## Prediction  B  M\n##          B 79 14\n##          M 11 39"},{"path":"knn.html","id":"visualisierung","chapter":"Kapitel 6 kNN","heading":"6.7.6 Visualisierung","text":"","code":""},{"path":"knn.html","id":"kennzahlen-der-klassifikation","chapter":"Kapitel 6 kNN","heading":"6.8 Kennzahlen der Klassifikation","text":"Sauer (2019), Kap. 19.6, findet sich einige Erklärung zu Kennzahlen der Klassifikationsgüte.Ein Test kann vier verschiedenen Ergebnisse haben:Table 6.1: Vier Arten von Ergebnissen von KlassifikationenEs gibt eine verwirrende Vielfalt von Kennzahlen,\num die Güte einer Klassifikation einzuschätzen.\nHier sind einige davon:","code":""},{"path":"knn.html","id":"krebstest-beispiel","chapter":"Kapitel 6 kNN","heading":"6.9 Krebstest-Beispiel","text":"Betrachten wir Daten eines fiktiven Krebstest, aber realistischen\nDaten.Wie gut ist dieser Test?\nBerechnen wir einige Kennzahlen.Da die Funktionen zur Klassifikation stets einen Faktor wollen,\nwandeln wir die relevanten Spalten zuerst einen Faktor um (aktuell sind es numerische Spalten).Gesamtgenauigkeit:Sensitivität:Spezifität:Kappa:Positiver Vorhersagewert:Negativer Vorhersagewert:Während Sensitivität und Spezitivität sehr hoch sind,\nist die der negative Vorhersagewert sehr gering:Wenn man einen positiven Test erhält, ist die\nWahrscheinlichkeit, Wahrheit krank zu sein gering, zum Glück!","code":"## # A tibble: 1 × 7\n##   format width height colorspace matte filesize density\n##   <chr>  <int>  <int> <chr>      <lgl>    <int> <chr>  \n## 1 PNG      500    429 sRGB       TRUE     40643 72x72## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary          0.87## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 sens    binary         0.884## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 spec    binary           0.6## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 kap     binary         0.261## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 ppv     binary         0.977## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 npv     binary         0.214"},{"path":"knn.html","id":"aufgaben-3","chapter":"Kapitel 6 kNN","heading":"6.10 Aufgaben","text":"Arbeiten Sie sich gut als möglich durch diese Analyse zum Verlauf von Covid-FällenFallstudie zur Modellierung einer logististischen Regression mit tidymodelsFallstudie zu VulkanausbrüchenFallstudie Himalaya","code":""},{"path":"resampling-und-tuning.html","id":"resampling-und-tuning","chapter":"Kapitel 7 Resampling und Tuning","heading":"Kapitel 7 Resampling und Tuning","text":"Benötigte R-Pakete für dieses Kapitel:","code":""},{"path":"resampling-und-tuning.html","id":"lernsteuerung-4","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.1 Lernsteuerung","text":"","code":""},{"path":"resampling-und-tuning.html","id":"vorbereitung-4","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.1.1 Vorbereitung","text":"Lesen Sie die Literatur.","code":""},{"path":"resampling-und-tuning.html","id":"lernziele-5","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.1.2 Lernziele","text":"Sie verstehen den Nutzen von Resampling und Tuning im maschinellen Nutzen.Sie können Methoden des Resampling und Tunings mit Hilfe von Tidymodels anwenden.","code":""},{"path":"resampling-und-tuning.html","id":"literatur-5","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.1.3 Literatur","text":"Rhys, Kap. 3TMWR, Kap. 10, 12","code":""},{"path":"resampling-und-tuning.html","id":"überblick-2","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.2 Überblick","text":"Der Standardablauf des maschinellen Lernens ist Abb. 7.1 dargestellt.\nEine alternative, hilfreich Abbildung findet sich hier Kap. 10.2 Silge Kuhn (2022).\nFigure 7.1: Standardablauf des maschinellen Lernens mit Tuning und Resampling\n","code":""},{"path":"resampling-und-tuning.html","id":"tidymodels-1","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.3 tidymodels","text":"","code":""},{"path":"resampling-und-tuning.html","id":"datensatz-aufteilen","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.3.1 Datensatz aufteilen","text":"","code":""},{"path":"resampling-und-tuning.html","id":"rezept-modell-und-workflow-definieren","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.3.2 Rezept, Modell und Workflow definieren","text":"gewohnter Weise definieren wir den Workflow\nmit einem kNN-Modell.Das kNN-Modell ist noch nicht berechnet,\nes ist nur ein “Rezept” erstellt:","code":"## K-Nearest Neighbor Model Specification (regression)\n## \n## Computational engine: kknn## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: nearest_neighbor()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 4 Recipe Steps\n## \n## • step_log()\n## • step_other()\n## • step_dummy()\n## • step_zv()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## K-Nearest Neighbor Model Specification (regression)\n## \n## Computational engine: kknn"},{"path":"resampling-und-tuning.html","id":"resampling","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.4 Resampling","text":"Vergleichen Sie die drei Fälle, die sich der Nutzung von Train- und Test-Sample unterscheiden:Wir fitten ein Klassifikationsmodell einer Stichprobe, sagen die Y-Werte dieser Stichprobe “vorher”. Wir finden eine Gesamtgenauigkeit von 80%.Wir fitten ein Klassifikationsmodell einem Teil der ursprünglichen Stichprobe (Train-Sample) und sagen Y-die Werte im verbleibenden Teil der ursprünglichen Stichprobe vorher (Test-Sample). Wir finden eine Gesamtgenauigkeit von 70%.Wir wiederholen Fall 2 noch drei Mal mit jeweils anderer Zuweisung der Fälle zum Train- bzw. zum Test-Sample. Wir finden insgesamt folgende Werte Gesamtgenauigkeit: 70%, 70%, 65%, 75%.Welchen der drei Fälle finden Sie sinnvollsten? Warum?","code":""},{"path":"resampling-und-tuning.html","id":"illustration-des-resampling","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.5 Illustration des Resampling","text":"Resampling stellt einen Oberbegriff dar; Kreuzvalidierung ist ein Unterbegriff dazu.\nEs gibt noch andere Arten des Resampling, etwa Bootstrapping oder Leave-One--Cross-Validation (LOOCV).Im Folgenden ist nur die Kreuzvalidierung dargestellt,\nda es eines der wichtigsten und vielleicht das Wichtigste ist.\nvielen Quellen finden sich Erläuterungen anderer Verfahren dargestellt,\netwa Silge Kuhn (2022), James et al. (2021) oder Rhys (2020).","code":""},{"path":"resampling-und-tuning.html","id":"einfache-v-fache-kreuzvalidierung","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.5.1 Einfache v-fache Kreuzvalidierung","text":"Abb. 7.2 illustriert die zufällige Aufteilung von \\(n=10\\) Fällen der Originalstrichprobe auf eine Train- bzw. Test-Stichpobe.\nMan spricht von Kreuzvalidierung (cross validation, CV).diesem Fall wurden 70% der (\\(n=10\\)) Fälle der Train-Stichprobe zugewiesen (der Rest der Test-Stichprobe);\nein willkürlicher, aber nicht unüblicher Anteil.\nDiese Aufteilung wurde \\(v=3\\) Mal vorgenommen,\nes resultieren drei “Resampling-Stichproben”, die\nmanchmal auch als “Faltungen” bezeichnet werden.\nFigure 7.2: Resampling: Eine Stichprobe wird mehrfach (hier 3 Mal) zu 70% ein Train- und zu 30% die Test-Stichprobe aufgeteilt\nSauer (2019) stellt das Resampling dar (S. 259), s. Abb. 7.3.\nFigure 7.3: Kreuzvalidierung, Aufteilung Train- vs. Testsample\nDer Gesamtfehler der Vorhersage wird als Mittelwerte der Vorhersagefehler den einzelnen Faltungen berechnet.Warum ist die Vorhersage besser,\nwenn man mehrere Faltungen, mehrere Schätzungen für \\(y\\) also, vornimmt?Der Grund ist das Gesetz der großen Zahl,\nnachdem sich eine Schätzung Mittelwert und Variabilität stabilisiert mit steigendem\nStichprobenumfang,\ndem wahren Mittelwert also präziser schätzt.\nBei Normalverteilungen klappt das gut,\nbei randlastigen Verteilungen leider nicht mehr (Taleb 2019).Häufig werden \\(v=10\\) Faltungen verwendet,\nsich empirisch als guter Kompromiss von Rechenaufwand und Fehlerreduktion herausgestellt hat.","code":""},{"path":"resampling-und-tuning.html","id":"wiederholte-kreuzvalidierung","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.5.2 Wiederholte Kreuzvalidierung","text":"Die \\(r\\)-fach wiederholte Kreuzvalidierung wiederholte die einfache Kreuzvalidierung mehrfach (nämlich \\(r=4\\) mal),\nSauer (2019) stellt das Resampling dar (S. 259), s. Abb. 7.4.\nFigure 7.4: Wiederholte Kreuzvalidierung\nDie wiederholte Kreuzvalidierung reduziert den Standardfehler der Vorhersagen.Silge Kuhn (2022) zeigen die Verringerung des Schätzfehlers als Funktion der \\(r\\) Wiederholungen dar,\ns. Abb. 7.5.\nFigure 7.5: Reduktion des Schätzfehlers als Funktion der r Wiederhoulugen der Kreuzvalidierung\nWarum ist die Wiederholung der Kreuzvalidierung nützlich?Die Kreuvalidierung liefert einen Schätzwert der Modellparameter,\ndie wahren Modellparameter werden also anhand einer Stichprobe von \\(n=1\\) geschätzt.\nMit höherem Stichprobenumfang kann diese Schätzung natürlich präzisiert werden.Da jede Stichprobenverteilung bei \\(n \\rightarrow \\infty\\) normalverteilt ist -\nein zentrales Theorem der Statistik, der Zentrale Grenzwertsatz (Central Limit Theorem) -\nkann man hoffen, dass sich eine bestimmte Stichprobenverteilung bei kleinerem \\(n\\) ebenfalls annähernd\nnormalverteilt3.\nDann sind die Quantile bekannt und man kann die Streuung der Schätzers,\n\\({\\sigma }_{\\bar {x}}\\), z.B. für den Mittelwert,\neinfach schätzen:\\[{\\displaystyle {\\sigma }_{\\bar {x}}\\ ={\\frac {\\sigma }{\\sqrt {n}}}}\\]","code":""},{"path":"resampling-und-tuning.html","id":"resampling-passiert-im-train-sample","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.5.3 Resampling passiert im Train-Sample","text":"Wichtig zu beachten ist, dass\ndie Resampling nur im Train-Sample stattfindet.\nDas Test-Sample bleibt unangerührt.\nDieser Sachverhalt ist Abb. 7.6, aus Silge Kuhn (2022), illustriert.\nFigure 7.6: Resampling im Train-, nicht im Test-Sample\nWie Abb. 7.6 dargestellt,\nwird das Modell im Analyse-Sample berechnet (gefittet),\nund im Assessment-Sample auf Modellgüte hin überprüft.Die letztliche Modellgüte ist dann die Zusammenfassung (Mittelwert) der einzelnen Resamples.","code":""},{"path":"resampling-und-tuning.html","id":"andere-illustrationen","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.5.4 Andere Illustrationen","text":"Es gibt eine Reihe vergleichbarer Illustrationen anderen Büchern:Timbers, Campbell & Lee, 2022, Kap. 6Silge & Kuhn, 2022, Abb. 10.1Silge & Kuhn, 2022, Abb. 10.2Silge & Kuhn, 2022, Abb. 10.3James, Witten, hastie & Tishirani, 2021, Abb. 5.3","code":""},{"path":"resampling-und-tuning.html","id":"gesetz-der-großen-zahl","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.6 Gesetz der großen Zahl","text":"Nach dem Gesetz der großen Zahl (Law Large Numbers) sollte sich der Mittelwert einer großen Stichprobe\ndem theoretischen Mittelwert der zugrundeliegenden Verteilung (Population, datengeneriender Prozess)\nsehr nahe kommen.\\[\\displaystyle \\lim _{n\\\\infty }\\sum _{=1}^{n}{\\frac {X_{}}{n}}={\\overline {X}}\\]David Salazar visualisiert das folgendermaßen diesem Post seines lesenswerten Blogs, s. Abb. 7.7.\nFigure 7.7: Gesetz der großen Zahl\nWie man sieht, nähert sich der empirische Mittelwert (also der Stichprobe)\nimmer mehr dem theoretischen Mittelwert, 0, .Achtung: Bei randlastigen Verteilungen darf man dieses schöne, wohlerzogene Verhalten nicht erwarten (Taleb 2019).","code":"\n# source: https://david-salazar.github.io/2020/04/17/fat-vs-thin-does-lln-work/\nsamples <- 1000\n\nthin <- rnorm(samples, sd = 20)\n\ncumulative_mean <- function(numbers) {\n    x <- seq(1, length(numbers))\n    cum_mean <- cumsum(numbers)/x \n    cum_mean\n}\n\nthin_cum_mean <- cumulative_mean(thin)\n\nthin_cum_mean %>%\n  tibble(running_mean = .) %>% \n  add_rownames(var = 'number_samples') %>% \n  mutate(number_samples = as.double(number_samples)) %>% \n  arrange(number_samples) %>% \n  ggplot(aes(x = number_samples, y = running_mean)) +\n    geom_line(color = 'dodgerblue4') +\n    geom_hline(yintercept = 0, linetype = 2, color = 'red') +\n  hrbrthemes::theme_ipsum_rc(grid = 'Y') +\n  scale_x_continuous(labels = scales::comma) +\n  labs(x = \"Stichprobengröße\",\n       title = \"Gesetz der großen Zahl\", \n       subtitle = \"Kumulierter Mittelwert aus einer Normalverteilung mit sd=20\")"},{"path":"resampling-und-tuning.html","id":"über--und-unteranpassung-an-einem-beispiel","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.7 Über- und Unteranpassung an einem Beispiel","text":"\nFigure 7.8: Welches Modell (Teile C-E) passt besten zu den Daten (Teil B)? Die ‘wahre Funktion’, der datengenerierende Prozess ist im Teil dargestellt\nAbb. 7.8 zeigt:Teil : Die ‘wahre Funktion’, \\(f\\), die die Daten erzeugt. Man spricht auch von der “datengenerierenden Funktion”. Wir gehen gemeinhin davon aus, dass es eine wahre Funktion gibt. Das heißt nicht, dass die wahre Funktion die Daten perfekt erklärt, schließlich kann die Funktion zwar wahr, aber unvollständig sein oder unsere Messinstrumente sind nicht perfekt präzise.Teil B: Die Daten, erzeugt aus plus etwas zufälliges Fehler (Rauschen).Teil C: Ein zu einfaches Modell: Unteranpassung. Vorhersagen einer neuen Stichprobe (basierend auf dem datengenerierenden Prozess aus ) werden nicht gut sein.Teil D: Ein zu komplexes Modell: Überanpassung. Vorhersagen einer neuen Stichprobe (basierend auf dem datengenerierenden Prozess aus ) werden nicht gut sein.Teil E: Ein Modell mittlerer Komplexität. Keine Überanpassung, keine Unteranpassung. Vorhersagen einer neuen Stichprobe (basierend auf dem datengenerierenden Prozess aus ) werden gut sein.","code":""},{"path":"resampling-und-tuning.html","id":"cv-in-tidymodels","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.8 CV in tidymodels","text":"","code":""},{"path":"resampling-und-tuning.html","id":"cv-definieren","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.8.1 CV definieren","text":"kann man eine einfache v-fache Kreuzvalidierung Tidymodels auszeichnen:Werfen wir einen Blick die Spalte splits, erste Zeile:Möchte man die Defaults vpn vfold_cv wissen, schaut man der Hilfe nach: ?vfold_cv:vfold_cv(data, v = 10, repeats = 1, strata = NULL, breaks = 4, pool = 0.1, ...)Probieren wir \\(v=5\\) und \\(r=2\\):","code":"## #  10-fold cross-validation using stratification \n## # A tibble: 10 × 2\n##    splits             id    \n##    <list>             <chr> \n##  1 <split [1976/221]> Fold01\n##  2 <split [1976/221]> Fold02\n##  3 <split [1976/221]> Fold03\n##  4 <split [1976/221]> Fold04\n##  5 <split [1977/220]> Fold05\n##  6 <split [1977/220]> Fold06\n##  7 <split [1978/219]> Fold07\n##  8 <split [1978/219]> Fold08\n##  9 <split [1979/218]> Fold09\n## 10 <split [1980/217]> Fold10## <Analysis/Assess/Total>\n## <1976/221/2197>## #  5-fold cross-validation repeated 2 times using stratification \n## # A tibble: 10 × 3\n##    splits             id      id2  \n##    <list>             <chr>   <chr>\n##  1 <split [1756/441]> Repeat1 Fold1\n##  2 <split [1757/440]> Repeat1 Fold2\n##  3 <split [1757/440]> Repeat1 Fold3\n##  4 <split [1758/439]> Repeat1 Fold4\n##  5 <split [1760/437]> Repeat1 Fold5\n##  6 <split [1756/441]> Repeat2 Fold1\n##  7 <split [1757/440]> Repeat2 Fold2\n##  8 <split [1757/440]> Repeat2 Fold3\n##  9 <split [1758/439]> Repeat2 Fold4\n## 10 <split [1760/437]> Repeat2 Fold5"},{"path":"resampling-und-tuning.html","id":"resamples-fitten","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.8.2 Resamples fitten","text":"Hat unser Computer mehrere Rechenkerne, dann können wir diese nutzen und die Berechnungen beschleunigen.\nIm Standard wird sonst nur ein Kern verwendet.Auf Unix/MacOC-Systemen kann man dann die Anzahl der parallen Kerne einstellen:, und jetzt fitten wir die Resamples und trachten die Modellgüte den Resamples:Natürlich interessiert uns primär die Modellgüte im Test-Sample:","code":"## [1] 4## # A tibble: 2 × 6\n##   .metric .estimator   mean     n std_err .config             \n##   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n## 1 rmse    standard   0.0928    10 0.00187 Preprocessor1_Model1\n## 2 rsq     standard   0.722     10 0.00864 Preprocessor1_Model1## # A tibble: 2 × 4\n##   .metric .estimator .estimate .config             \n##   <chr>   <chr>          <dbl> <chr>               \n## 1 rmse    standard       0.103 Preprocessor1_Model1\n## 2 rsq     standard       0.678 Preprocessor1_Model1"},{"path":"resampling-und-tuning.html","id":"tuning","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.9 Tuning","text":"","code":""},{"path":"resampling-und-tuning.html","id":"tuning-auszeichnen","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.9.1 Tuning auszeichnen","text":"der Modellspezifikation des Modells können wir mit tune() auszeichnen,\nwelche Parameter wir tunen möchten.\nWir könennWir können dem Tuningparameter auch einen Namen (ID/Laben) geben, z.B. “K”:","code":""},{"path":"resampling-und-tuning.html","id":"grid-search-vs.-iterative-search","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.9.2 Grid Search vs. Iterative Search","text":"Im K-Nächste-Nachbarn-Modell ist der vorhergesagt Wert, \\(\\hat{y}\\) für eine neue Beobachtung \\(x_0\\) wie folgt definiert:\\[\n\\hat y = \\frac{1}{K}\\sum_{\\ell = 1}^K x_\\ell^*,\n\\]wobei \\(K\\) die Anzahl der zu berücksichtigen nächsten Nachbarn darstellt und \\(x_\\ell^*\\) die Werte dieser berücksichtiggten Nachbarn.Die Wahl von \\(K\\) hat einen gewaltigen Einfluss auf die Vorhersagen und damit auf die Vorhersagegüte.\nAllerdings wird \\(K\\) nicht vom Modell geschätzt.\nEs liegt den Nutzi,\ndiesen Wert zu wählen.Parameter dieser Art (die von den Nutzi zu bestimmen sind, nicht vom Algorithmus),\nnennt man Tuningparameter.Abbildung 7.9 aus Silge Kuhn (2022) stellt exemplarisch dar,\nwelchen großen Einfluss die Wahl des Werts eines Tuningparameters auf die\nVorhersagen eines Modells haben.\nFigure 7.9: Overfitting als Funktion der Modellparameter und insofern als Problem de Wahl der Tuningparameter\nAber wie wählt man “gute” Werte der Tuningparater?\nZwei Ansätze, grob gesprochen, bieten sich .Grid Search: Probiere viele Werte aus und schaue, welcher der beste ist. Dabei musst du hoffen, dass du die Werte erwischt, die nicht nur im Train-, sondern auch im Test-Sample gut funktionieren werden.Grid Search: Probiere viele Werte aus und schaue, welcher der beste ist. Dabei musst du hoffen, dass du die Werte erwischt, die nicht nur im Train-, sondern auch im Test-Sample gut funktionieren werden.Iterative Search: Wenn du einen Wert eines Tuningparameters hast, nutze diesen, um intelligenter einen neuen Wert eines Tuningparameters zu finden.Iterative Search: Wenn du einen Wert eines Tuningparameters hast, nutze diesen, um intelligenter einen neuen Wert eines Tuningparameters zu finden.Der Unterschied beider Ansätze ist Silge Kuhn (2022) wie Abb. 7.10 dargestellt.\nFigure 7.10: Links: Grid Search. Rechts: Iterative Search2\ntidymodels kann man mit tune() angeben, dass man einen bestimmten Parameter tunen möchte.\ntidymodels führt das dann ohne weiteres Federlesens für uns durch.","code":""},{"path":"resampling-und-tuning.html","id":"tuning-mit-tidymodels","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.10 Tuning mit Tidymodels","text":"","code":""},{"path":"resampling-und-tuning.html","id":"tuningparameter-betrachten","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.10.0.1 Tuningparameter betrachten","text":"Möchte man wissen,\nwelche und wie viele Tuningparameter tidymodels einem Modell berücksichtigt,\nkann man extract_parameter_set_dials() aufrufen:Die Ausgabe informiert uns,\ndass es nur einen Tuningparameter gibt diesem Modell und\ndass der Name (Label, ID) des Tuningparameters “K” ist.\nAußerdem sollen die Anzahl der Nachbarn getunt werden.\nDer Tuningparameter ist numerisch; das sieht man nparam[+].Schauen wir uns mal ,\nauf welchen Wertebereich tidymodels den Parameter \\(K\\) begrenzt hat:Aktualisieren wir mal unseren Workflow entsprechend:Wir können auch Einfluss nehmen und angeben,\ndass die Grenzen des Wertebereichs zwischen 1 und 50 liegen soll\n(für den Tuningparameter neighbors):","code":"## Collection of 1 parameters for tuning\n## \n##  identifier      type    object\n##           K neighbors nparam[+]## # Nearest Neighbors (quantitative)\n## Range: [1, 15]## Collection of 1 parameters for tuning\n## \n##  identifier      type    object\n##           K neighbors nparam[+]"},{"path":"resampling-und-tuning.html","id":"datenabhängige-tuningparameter","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.10.1 Datenabhängige Tuningparameter","text":"Manche Tuningparameter kann man nur bestimmen,\nwenn man den Datensatz kennt.\nist die Anzahl der Prädiktoren, mtry einem Random-Forest-Modell\nsinnvollerweise als Funktion der Prädiktorenzahl zu wählen.\nDer Workflow kennt aber den Datensatz nicht.\nDaher muss der Workflow noch “finalisiert” oder “aktualisiert” werden,\num den Wertebereich (Unter- und Obergrenze) eines Tuningparameters zu bestimmen.Wenn wir im Rezept aber z.B. die Anzahl der Prädiktoren verändert haben,\nmöchten wir die Grenzen des Wertebereichs für mtry (oder andere Tuningparameter) vielleicht nicht händisch, “hartverdrahtet” selber bestimmen,\nsondern lieber den Computer anweisen, und sinngemäß sagen:\n“Warte mal mit der Bestimmung der Werte der Tuningparameter,\nbis du den Datensatz bzw. dessen Dimensionen kennst. Merk dir,\ndass du, wenn du den Datensatz kennst, die Werte des Tuningparameter noch ändern musst. Und tu das dann auch.” Dazu später mehr.","code":""},{"path":"resampling-und-tuning.html","id":"modelle-mit-tuning-berechnen","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.10.2 Modelle mit Tuning berechnen","text":"Nachdem wir die Tuningwerte bestimmt haben,\nkönnen wir jetzt das Modell berechnen:\nFür jeden Wert des Tuningparameters wird ein Modell berechnet:Im Default berechnet tiymodels 10 Kandidatenmodelle.Die Spalte .metrics beinhaltet die Modellgüte für jedes Kandidatenmodell.Das können wir uns einfach visualisieren lassen:Auf Basis dieser Ergebnisse könnte es Sinn machen,\nnoch größere Werte für \\(K\\) zu überprüfen.","code":"## # Tuning results\n## # 10-fold cross-validation using stratification \n## # A tibble: 10 × 4\n##    splits             id     .metrics          .notes          \n##    <list>             <chr>  <list>            <list>          \n##  1 <split [1976/221]> Fold01 <tibble [16 × 5]> <tibble [0 × 3]>\n##  2 <split [1976/221]> Fold02 <tibble [16 × 5]> <tibble [0 × 3]>\n##  3 <split [1976/221]> Fold03 <tibble [16 × 5]> <tibble [0 × 3]>\n##  4 <split [1976/221]> Fold04 <tibble [16 × 5]> <tibble [0 × 3]>\n##  5 <split [1977/220]> Fold05 <tibble [16 × 5]> <tibble [0 × 3]>\n##  6 <split [1977/220]> Fold06 <tibble [16 × 5]> <tibble [0 × 3]>\n##  7 <split [1978/219]> Fold07 <tibble [16 × 5]> <tibble [0 × 3]>\n##  8 <split [1978/219]> Fold08 <tibble [16 × 5]> <tibble [0 × 3]>\n##  9 <split [1979/218]> Fold09 <tibble [16 × 5]> <tibble [0 × 3]>\n## 10 <split [1980/217]> Fold10 <tibble [16 × 5]> <tibble [0 × 3]>## # A tibble: 16 × 7\n##        K .metric .estimator   mean     n std_err .config             \n##    <int> <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n##  1     2 rmse    standard   0.103     10 0.00213 Preprocessor1_Model1\n##  2     2 rsq     standard   0.662     10 0.0112  Preprocessor1_Model1\n##  3     4 rmse    standard   0.0950    10 0.00188 Preprocessor1_Model2\n##  4     4 rsq     standard   0.708     10 0.00916 Preprocessor1_Model2\n##  5     6 rmse    standard   0.0912    10 0.00189 Preprocessor1_Model3\n##  6     6 rsq     standard   0.732     10 0.00842 Preprocessor1_Model3\n##  7     7 rmse    standard   0.0900    10 0.00192 Preprocessor1_Model4\n##  8     7 rsq     standard   0.740     10 0.00829 Preprocessor1_Model4\n##  9     9 rmse    standard   0.0883    10 0.00201 Preprocessor1_Model5\n## 10     9 rsq     standard   0.752     10 0.00827 Preprocessor1_Model5\n## 11    11 rmse    standard   0.0872    10 0.00211 Preprocessor1_Model6\n## 12    11 rsq     standard   0.761     10 0.00845 Preprocessor1_Model6\n## 13    13 rmse    standard   0.0865    10 0.00217 Preprocessor1_Model7\n## 14    13 rsq     standard   0.767     10 0.00848 Preprocessor1_Model7\n## 15    15 rmse    standard   0.0861    10 0.00221 Preprocessor1_Model8\n## 16    15 rsq     standard   0.772     10 0.00850 Preprocessor1_Model8"},{"path":"resampling-und-tuning.html","id":"vorhersage-im-test-sample","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.10.3 Vorhersage im Test-Sample","text":"Welches Modellkandidat war jetzt besten?Wählen wir jetzt mal das beste Modell aus (im Sinne des Optimierungskriteriusms):Ok,\nnotieren wir uns die Kombination der Tuningparameterwerte\nim besten Kandiatenmodell.\ndiesem Fall hat das Modull nur einen Tuningparameter:Unser Workflow weiß noch nicht,\nwelche Tuningparameterwerte besten sind:neighbors = tune(\"K\") sagt uns,\ndass er diesen Parameter tunen .\nDas haben wir jetzt ja erledigt.\nWir wollen für das Test-Sample nur noch einen Wert,\neben aus dem besten Kandidatenmodell,\nverwenden:Wie man sieht,\nsteht im Workflow nichts mehr von Tuningparameter.Wir können jetzt das ganze Train-Sample fitten,\nalso das Modell auf das ganze Train-Sample anwenden -\nnicht nur auf ein Analysis-Sample.\nUnd mit den dann resultierenden Modellkoeffizienten sagen\nwir das TestSample vorher:Holen wir uns die Modellgüte:","code":"## # A tibble: 5 × 7\n##       K .metric .estimator   mean     n std_err .config             \n##   <int> <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n## 1    15 rmse    standard   0.0861    10 0.00221 Preprocessor1_Model8\n## 2    13 rmse    standard   0.0865    10 0.00217 Preprocessor1_Model7\n## 3    11 rmse    standard   0.0872    10 0.00211 Preprocessor1_Model6\n## 4     9 rmse    standard   0.0883    10 0.00201 Preprocessor1_Model5\n## 5     7 rmse    standard   0.0900    10 0.00192 Preprocessor1_Model4## # A tibble: 1 × 2\n##       K .config             \n##   <int> <chr>               \n## 1    15 Preprocessor1_Model8## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: nearest_neighbor()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 4 Recipe Steps\n## \n## • step_log()\n## • step_other()\n## • step_dummy()\n## • step_zv()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## K-Nearest Neighbor Model Specification (regression)\n## \n## Main Arguments:\n##   neighbors = tune(\"K\")\n## \n## Computational engine: kknn## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: nearest_neighbor()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 4 Recipe Steps\n## \n## • step_log()\n## • step_other()\n## • step_dummy()\n## • step_zv()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## K-Nearest Neighbor Model Specification (regression)\n## \n## Main Arguments:\n##   neighbors = 15\n## \n## Computational engine: kknn## # Resampling results\n## # Manual resampling \n## # A tibble: 1 × 6\n##   splits             id               .metrics .notes   .predictions .workflow \n##   <list>             <chr>            <list>   <list>   <list>       <list>    \n## 1 <split [2197/733]> train/test split <tibble> <tibble> <tibble>     <workflow>## # A tibble: 2 × 4\n##   .metric .estimator .estimate .config             \n##   <chr>   <chr>          <dbl> <chr>               \n## 1 rmse    standard      0.0951 Preprocessor1_Model1\n## 2 rsq     standard      0.736  Preprocessor1_Model1"},{"path":"resampling-und-tuning.html","id":"aufgaben-4","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.11 Aufgaben","text":"Arbeiten Sie sich gut als möglich durch diese Analyse zum Verlauf von Covid-FällenFallstudie zur Modellierung einer logististischen Regression mit tidymodelsFallstudie zu VulkanausbrüchenFallstudie Himalaya","code":""},{"path":"resampling-und-tuning.html","id":"vertiefung-2","chapter":"Kapitel 7 Resampling und Tuning","heading":"7.12 Vertiefung","text":"Fields arranged purity, xkcd 435","code":""},{"path":"logistische-regression.html","id":"logistische-regression","chapter":"Kapitel 8 Logistische Regression","heading":"Kapitel 8 Logistische Regression","text":"Benötigte R-Pakete für dieses Kapitel:","code":""},{"path":"logistische-regression.html","id":"lernsteuerung-5","chapter":"Kapitel 8 Logistische Regression","heading":"8.1 Lernsteuerung","text":"","code":""},{"path":"logistische-regression.html","id":"vorbereitung-5","chapter":"Kapitel 8 Logistische Regression","heading":"8.1.1 Vorbereitung","text":"Frischen Sie Ihr Wissen zur logistischen Regression auf bzw. machen Sie sich mit den Grundlagen des Verfahrens vertraut.","code":""},{"path":"logistische-regression.html","id":"lernziele-6","chapter":"Kapitel 8 Logistische Regression","heading":"8.1.2 Lernziele","text":"Sie verstehen den Zusammenhang von linearen und logistischen ModellenSie können die logistische Regression mit Methoden von tidymodels anwenden","code":""},{"path":"logistische-regression.html","id":"literatur-6","chapter":"Kapitel 8 Logistische Regression","heading":"8.1.3 Literatur","text":"Rhys, Kap. 4","code":""},{"path":"logistische-regression.html","id":"intuitive-erklärung-1","chapter":"Kapitel 8 Logistische Regression","heading":"8.2 Intuitive Erklärung","text":"Die logistische Reression ist ein Spezialfall des linearen Modells (lineare Regression),\nder für binäre (dichotom) AV eingesetzt wird (es gibt auch eine Variante für multinominale AV).\nEs können eine oder mehrere UV eine logistische Regression einfließen,\nmit beliebigem Skalenniveau.Beispiele für Forschungsfragen, die mit der logistischen Regression modelliert werden sind:Welche Faktoren sind prädiktiv, um vorherzusagen, ob jemand einen Kredit zurückzahlen kann oder nicht?Haben weibliche Passagiere aus der 1. Klasse eine höhere Überlebenschance als andere Personen auf der Titanic?Welche Faktoren hängen damit zusammen, ob ein Kunde eine Webseite verlässt, bevor er einen Kauf abschließt?Der Name stammt von der logistischen Funktion,\ndie man der einfachsten Form darstellen kann:\\[f(x) = \\frac{x}{1+e^{-x}}\\]Da die AV als dichotom modelliert wird, spricht man von einer Klassifikation.Allerdings ist das Modell reichhaltiger als eine bloße Klassifikation,\ndie (im binären Fall) nur 1 Bit Information liefert: “ja” vs. “nein” bzw. 0 vs. 1.Das Modell liefert nämlich nicht nur eine Klassifikation zurück,\nsondern auch eine Indikation der Stärke (epistemologisch) der Klassenzugehörigkeit.Einfach gesagt heißt das,\ndass die logistische Regression eine Wahrscheinlichkeit der Klassenzugehörigkeit zurückliefert.\nFigure 8.1: Definition eines Models tidymodels\n","code":""},{"path":"logistische-regression.html","id":"profil","chapter":"Kapitel 8 Logistische Regression","heading":"8.3 Profil","text":"Das Profil des Modells kann man wie folgt charakterisieren, vgl. Tab. 8.1.Table 8.1: Profil der logistischen Regression","code":""},{"path":"logistische-regression.html","id":"warum-nicht-die-lineare-regression-verwenden","chapter":"Kapitel 8 Logistische Regression","heading":"8.4 Warum nicht die lineare Regression verwenden?","text":"Forschungsfrage: Kann man anhand des Spritverbrauchs vorhersagen, ob ein Auto eine Automatik- bzw. ein manuelle Schaltung hat? Anders gesagt: Hängen Spritverbrauch und Getriebeart? (Datensatz mtcars)\\(Pr(\\text{}=1|m91,\\text{mpg_z}=0) = 0.46\\):\nDie Wahrscheinlichkeit einer manuelle Schaltung,\ngegeben einem durchschnittlichen Verbrauch (und dem Modell m81) liegt bei knapp 50%.","code":"## (Intercept)          iv \n##   0.4062500   0.2993109"},{"path":"logistische-regression.html","id":"lineare-modelle-running-wild","chapter":"Kapitel 8 Logistische Regression","heading":"8.4.1 Lineare Modelle running wild","text":"Wie groß ist die Wahrscheinlichkeit für eine manuelle Schaltung …… bei mpg_z = -2?\\(Pr(\\hat{y})<0\\) macht keinen Sinn. ⚡… bei mpg_z = +2?\\(Pr(\\hat{y})>1\\) macht keinen Sinn. ⚡Schauen Sie sich mal die Vorhersage für mpg_z=5 🤯","code":"\npredict(m81, newdata = data.frame(iv = -2))##          1 \n## -0.1923719\npredict(m81, newdata = data.frame(iv = +2))##        1 \n## 1.004872"},{"path":"logistische-regression.html","id":"wir-müssen-die-regressionsgerade-umbiegen","chapter":"Kapitel 8 Logistische Regression","heading":"8.4.2 Wir müssen die Regressionsgerade umbiegen","text":"… wenn der vorhergesagte Wert eine Wahrscheinlichkeit, \\(p_i\\), ist.Die schwarze Gerade verlässt den Wertebereich der Wahrscheinlichkeit.\nDie blaue Kurve, \\(\\mathcal{f}\\), bleibt im erlaubten Bereich, \\(Pr(y) \\[0,1]\\).\nWir müssen also die linke oder die rechte Seite des linearen Modells transformieren:\n\\(p_i = f(\\alpha + \\beta \\cdot x)\\) bzw.:\\(f(p) = \\alpha + \\beta \\cdot x\\)\\(\\mathcal{f}\\) nennt man eine Link-Funktion.","code":""},{"path":"logistische-regression.html","id":"verallgemeinerte-lineare-modelle-zur-rettung","chapter":"Kapitel 8 Logistische Regression","heading":"8.4.3 Verallgemeinerte lineare Modelle zur Rettung","text":"Für metrische AV mit theoretisch unendlichen Grenzen des Wertebereichs haben wir bisher eine Normalverteilung verwendet:\\[y_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\\]Dann ist die Normalverteilung eine voraussetzungsarme Wahl (maximiert die Entropie).Aber wenn die AV binär ist bzw. Häufigkeiten modelliert,\nbraucht man eine Variable die nur positive Werte zulässt.Diese Verallgemeinerung des linearen Modells bezeichnet man als verallgemeinertes lineares Modell (generalized linear model, GLM).Im Falle einer binären (bzw. dichotomen) AV liegt eine bestimmte Form des GLM vor,\ndie man als logistische Regression bezeichnet.","code":""},{"path":"logistische-regression.html","id":"der-logit-link","chapter":"Kapitel 8 Logistische Regression","heading":"8.5 Der Logit-Link","text":"Der Logit-Link wird auch \\(\\mathcal{L}\\), logit, Log-Odds oder Logit-Funktion genannt.Er “biegt” die lineare Funktion die richtige Form.Der Logit-Link ordnet einen Parameter, der als Wahrscheinlichkeitsmasse definiert ist (und daher im Bereich von 0 bis 1 liegt), einem linearen Modell zu (das jeden beliebigen reellen Wert annehmen kann):\\[\n\\begin{align}\n    \\text{logit}(p_i) &= \\alpha + \\beta x_i\n\\end{align}\n\\]Die Logit-Funktion \\(\\mathcal{L}\\) ist definiert als der (natürliche) Logarithmus des Verhältnisses der Wahrscheinlichkeit zu Gegenwahrscheinlichkeit:\\[\\mathcal{L} = \\text{log} \\frac{p_i}{1-p_i}\\]Das Verhältnis der Wahrscheinlichkeit zu Gegenwahrscheinlichkeit nennt man auch Odds.Das Verhältnis der Wahrscheinlichkeit zu Gegenwahrscheinlichkeit nennt man auch Odds.Also:Also:\\[\\mathcal{L} = \\text{log} \\frac{p_i}{1-p_i} = \\alpha + \\beta x_i\\]","code":""},{"path":"logistische-regression.html","id":"aber-warum","chapter":"Kapitel 8 Logistische Regression","heading":"8.6 Aber warum?","text":"Forschungsfrage: Hängt das Überleben (statistisch) auf der Titanic vom Geschlecht ab?Wie war eigentlich insgesamt, also ohne auf einen (oder mehrere) Prädiktoren zu bedingen, die Überlebenswahrscheinlichkeit?Die Wahrscheinlichkeit zu Überleben \\(Pr(y=1)\\) lag bei einem guten Drittel (0.38).Das hätte man auch ausrechnen:Anders gesagt: \\(p(y=1) = \\frac{549}{549+342} \\approx 0.38\\)","code":"## (Intercept) \n##   0.3838384##   Survived   n      prop\n## 1        0 549 0.6161616\n## 2        1 342 0.3838384"},{"path":"logistische-regression.html","id":"tidymodels-m83","chapter":"Kapitel 8 Logistische Regression","heading":"8.6.1 tidymodels, m83","text":"Berechnen wir jetzt ein lineares Modell für die AV Survived mit dem Geschlecht als Pädiktor:Die Faktorstufen, genannt levels von Survived sind:Und zwar genau dieser Reihenfolge.","code":"## [1] \"0\" \"1\""},{"path":"logistische-regression.html","id":"lm83-glm","chapter":"Kapitel 8 Logistische Regression","heading":"8.7 lm83, glm","text":"Die klassische Methoden R, ein logistisches Modell zu berechnen, ist\nmit der Funktion glm().\nTidymodels greift intern auf diese Funktion zurück.\nDaher sind die Ergebnisse numerisch identisch.AV: Überleben (binär/Faktor)UV: TicketpreisMit easystats kann man sich model_parameter() einfach ausgeben lassen:Und auch visualisieren lassen:","code":"## (Intercept)          iv \n##  -2.6827432   0.7479317## Parameter   | Log-Odds |   SE |         95% CI |      z |      p\n## ----------------------------------------------------------------\n## (Intercept) |    -2.68 | 0.26 | [-3.19, -2.19] | -10.46 | < .001\n## iv          |     0.75 | 0.08 | [ 0.59,  0.91] |   9.13 | < .001"},{"path":"logistische-regression.html","id":"m83-tidymodels","chapter":"Kapitel 8 Logistische Regression","heading":"8.8 m83, tidymodels","text":"Achtung! Bei tidymodels muss bei einer Klassifikation die AV vom Type factor sein.\nAußerdem wird bei tidymodels, im Gegensatz zu (g)lm nicht die zweite,\nsondern die erste als Ereignis modelliert wird.`Daher wechseln wir die referenzkategorie, wir “re-leveln”, mit relevel():Check:Passt.Die erste Stufe ist jetzt 1, also Überleben.Jetzt berechnen wir das Modell gewohnter Weise mit tidymodels.Hier sind die Koeffizienten, die kann man sich aus m83_fit herausziehen:Die Koeffizienten werden Logits angegeben.Abb. 8.2 ist das Modell und die Daten visualisiert.\nFigure 8.2: m83 und die Titanic-Daten\nDefinieren wir als \\(y=1\\) das zu modellierende Ereignis, hier “Überleben auf der Titanic” (hat also überlebt).Wie wir oben schon gesehen haben, funktioniert die lineare Regression nicht einwandfrei bei binären (oder dichotomen) AV.","code":"## [1] \"1\" \"0\"## [1]  2.6827432 -0.7479317## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    2.68     0.256      10.5  1.26e-25\n## 2 iv            -0.748    0.0819     -9.13 6.87e-20"},{"path":"logistische-regression.html","id":"wahrscheinlichkeit-in-odds","chapter":"Kapitel 8 Logistische Regression","heading":"8.8.1 Wahrscheinlichkeit in Odds","text":"Probieren wir Folgendes: Rechnen wir die Wahrscheinlichkeit zu Überlegen für \\(y\\), kurz \\(p\\), Odds (Chancen) um.\\(odds = \\frac{p}{1-p}\\)R:Bildlich gesprochen sagen die Odds: für 38 Menschen, die überlebt haben, kommen (ca.) 62 Menschen, die nicht überlebt haben, s. Abb. 8.3.\nFigure 8.3: Odds: 38 zu 62\nPlotten wir die Odds als Funktion der UV, s. Abb. 8.4.\nFigure 8.4: Odds als Funktion der UV\nWir sind noch nicht Ziel;\ndie Variable ist noch nicht “richtig gebogen”.","code":"## [1] 0.6129032"},{"path":"logistische-regression.html","id":"von-odds-zu-log-odds","chapter":"Kapitel 8 Logistische Regression","heading":"8.8.2 Von Odds zu Log-Odds","text":"Wenn wir jetzt den Logarithmus berechnen der Log-Odds bekommen wir eine “brav gebogenen” Funktion, die Log-Odds, \\(\\mathcal{L}\\), als Funktion der UV, s. Abb. 8.5.\\[\\mathcal{L} = log (odds) = log \\left(\\frac{p}{1-p}\\right)\\]\nFigure 8.5: Logit als Funktion der UV\nLinear!Es gilt also:\\[\\text {log-odds} = b_0 + b_1x\\]Log-Odds (Log-Odds) bezeichnet man auch als Logits.","code":""},{"path":"logistische-regression.html","id":"inverser-logit","chapter":"Kapitel 8 Logistische Regression","heading":"8.9 Inverser Logit","text":"Um nach \\(p\\) aufzulösen, müssen wir einige Algebra bemühen:\\[\n\\begin{align}\n\\text{log} \\frac{p}{1-p} &= \\alpha + \\beta x & & \\text{Exponentieren}\\\\\n\\frac{p}{1-p} &= e^{\\alpha + \\beta x} \\\\\np_i &= e^{\\alpha + \\beta x_i} (1-p) & & \\text{Zur Vereinfachung: } x := e^{\\alpha + \\beta x_i} \\\\\np_i &= x (1-p) \\\\\n&= x - xp \\\\\np + px &= x \\\\\np(1+x) &= x \\\\\np &= \\frac{x} {1+x} & & \\text{Lösen wir x wieder auf.} \\\\\np &= \\frac{e^{\\alpha + \\beta x_i}}{1 + e^{\\alpha + \\beta x_i}} = \\mathcal{L}^{-1}\n\\end{align}\n\\]Diese Funktion nennt man auch inverser Logit, \\(\\text{logit}^{-1}, \\mathcal{L}^{-1}\\).Zum Glück macht das alles die Rechenmaschine für uns 😄.","code":""},{"path":"logistische-regression.html","id":"vom-logit-zur-klasse","chapter":"Kapitel 8 Logistische Regression","heading":"8.10 Vom Logit zur Klasse","text":"Praktisch können wir uns die Logits und ihre zugehörige Wahrscheinlichkeit einfach ausgeben lassen mit R. Und die vorhergesagte Klasse (.pred_class) auch:","code":"##                                                  Name     logits\n## 1                             Braund, Mr. Owen Harris  1.2010894\n## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) -0.5084287\n## 3                              Heikkinen, Miss. Laina  1.1345079"},{"path":"logistische-regression.html","id":"grenzwert-wechseln","chapter":"Kapitel 8 Logistische Regression","heading":"8.10.1 Grenzwert wechseln","text":"Im Standard wird 50% als Grenzwert für die vorhergesagte Klasse \\(c\\) genommen:wenn \\(p <= .5 \\rightarrow c = 0\\)wenn \\(p > .5 \\rightarrow c = 1\\)Man kann aber den Grenzwert beliebig wählen, um Kosten-Nutzen-Abwägungen zu optimieren;\nmehr dazu findet sich z.B. hier.","code":""},{"path":"logistische-regression.html","id":"logit-und-inverser-logit","chapter":"Kapitel 8 Logistische Regression","heading":"8.11 Logit und Inverser Logit","text":"","code":""},{"path":"logistische-regression.html","id":"logit","chapter":"Kapitel 8 Logistische Regression","heading":"8.11.1 Logit","text":"\\((0,1) \\rightarrow (-\\infty, +\\infty)\\)Praktisch, um Wahrscheinlichkeit zu modellieren.\\[p \\rightarrow \\fbox{logit} \\rightarrow \\alpha + \\beta x\\]","code":""},{"path":"logistische-regression.html","id":"inv-logit","chapter":"Kapitel 8 Logistische Regression","heading":"8.11.2 Inv-Logit","text":"\\((-\\infty, +\\infty) \\rightarrow (0,1)\\)Praktisch, um Wahrscheinlichkeiten umzurechnen.\\[p \\leftarrow \\fbox{inv-logit} \\leftarrow \\alpha + \\beta x\\]","code":""},{"path":"logistische-regression.html","id":"logistische-regression-im-überblick","chapter":"Kapitel 8 Logistische Regression","heading":"8.12 Logistische Regression im Überblick","text":"Eine Regression mit binomial verteilter AV und Logit-Link nennt man logistische Regression.Eine Regression mit binomial verteilter AV und Logit-Link nennt man logistische Regression.Man verwendet die logistische Regression um binomial verteilte AV zu modellieren, z.B.\nWie hoch ist die Wahrscheinlichkeit, dass ein Kunde das Produkt kauft?\nWie hoch ist die Wahrscheinlichkeit, dass ein Mitarbeiter kündigt?\nWie hoch ist die Wahrscheinlichkeit, die Klausur zu bestehen?\nMan verwendet die logistische Regression um binomial verteilte AV zu modellieren, z.B.Wie hoch ist die Wahrscheinlichkeit, dass ein Kunde das Produkt kauft?Wie hoch ist die Wahrscheinlichkeit, dass ein Mitarbeiter kündigt?Wie hoch ist die Wahrscheinlichkeit, die Klausur zu bestehen?Die logistische Regression ist eine normale, lineare Regression für den Logit von \\(Pr(y=1)\\), wobei \\(y\\) (AV) binomialvereteilt mit \\(n=1\\) angenommen wird:Die logistische Regression ist eine normale, lineare Regression für den Logit von \\(Pr(y=1)\\), wobei \\(y\\) (AV) binomialvereteilt mit \\(n=1\\) angenommen wird:\\[\n\\begin{align}\ny_i &\\sim \\mathcal{B}(1, p_i) \\\\\n\\text{logit}(p_i) &= \\alpha + \\beta x_i\n\\end{align}\n\\]Da es sich um eine normale, lineare Regression handelt, sind alle bekannten Methoden und Techniken der linearen Regression zulässig.Da es sich um eine normale, lineare Regression handelt, sind alle bekannten Methoden und Techniken der linearen Regression zulässig.Da Logits nicht einfach zu interpretieren sind, rechnet man nach der Berechnung des Modells den Logit häufig Wahrscheinlichkeiten um.Da Logits nicht einfach zu interpretieren sind, rechnet man nach der Berechnung des Modells den Logit häufig Wahrscheinlichkeiten um.","code":""},{"path":"logistische-regression.html","id":"die-koeffizienten-sind-schwer-zu-interpretieren","chapter":"Kapitel 8 Logistische Regression","heading":"8.12.1 Die Koeffizienten sind schwer zu interpretieren","text":"der logistischen Regression gilt nicht mehr, dass eine konstante Veränderung der UV mit einer konstanten Veränderung der AV einhergeht.Stattdessen geht eine konstante Veränderung der UV mit einer konstanten Veränderung im Logit der AV einher.Beim logistischen Modell hier gilt, dass der Nähe von \\(x=0\\) die größte Veränderung \\(p\\) von statten geht; je weiter weg von \\(x=0\\), desto geringer ist die Veränderung \\(p\\).","code":""},{"path":"logistische-regression.html","id":"logits-vs.-wahrscheinlichkeiten","chapter":"Kapitel 8 Logistische Regression","heading":"8.12.2 Logits vs. Wahrscheinlichkeiten","text":"","code":"\nkonvert_logits <-\n  tibble(\n    logit = c( -10, -3, \n              -2, -1, -0.5, -.25, \n              0, \n              .25, .5, 1, 2, \n              3, 10),\n    p = rstanarm::invlogit(logit)\n  )  %>% \n  gt() %>% \n  fmt_number(everything(), decimals = 2)"},{"path":"logistische-regression.html","id":"aufgaben-5","chapter":"Kapitel 8 Logistische Regression","heading":"8.13 Aufgaben","text":"Fallstudien zu Studiengebühren1. Modell der Fallstudie Hotel BookingsAufgaben zur logistischen Regression, PDF","code":""},{"path":"entscheidungsbäume.html","id":"entscheidungsbäume","chapter":"Kapitel 9 Entscheidungsbäume","heading":"Kapitel 9 Entscheidungsbäume","text":"","code":""},{"path":"entscheidungsbäume.html","id":"lernsteuerung-6","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.1 Lernsteuerung","text":"","code":""},{"path":"entscheidungsbäume.html","id":"lernziele-7","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.1.1 Lernziele","text":"Sie können den rpart-Algorithmus erklärenSie wissen, wie man Overfitting bei Entscheidungsbäume begrenzen kannSie können Entscheidungsbäume R berechnen","code":""},{"path":"entscheidungsbäume.html","id":"literatur-7","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.1.2 Literatur","text":"Rhys, Kap. 7","code":""},{"path":"entscheidungsbäume.html","id":"vorbereitung-6","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.2 Vorbereitung","text":"diesem Kapitel werden folgende R-Pakete benötigt:","code":"\nlibrary(titanic)\n#library(rpart)  # Berechnung von Entscheidungsbäumen\nlibrary(tidymodels)\nlibrary(tictoc)  # Zeitmessung"},{"path":"entscheidungsbäume.html","id":"anatomie-eines-baumes","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.3 Anatomie eines Baumes","text":"Ein Baum 🌳 hat (u..):WurzelBlätterÄsteIn einem Entscheidungsbaum ist die Terminologie ähnlich, s. Abb. 9.1.\nAllgemein gesagt, kann ein Entscheidungsbaum einem baumähnlichen Graphen visualisiert werden.\nDort gibt es Knoten, die durch Kanten verbunden sind,\nwobei zu einem Knoten genau ein Kanten führt.Ein Beispiel für einen einfachen Baum sowie die zugehörige rekursive Partionierung ist Abb. 9.1 dargestellt;\nman erkennt \\(R=3\\) Regionen bzw. Blätter (James et al. 2021).\nFigure 9.1: Einfaches Beispiel für einen Baum sowie der zugehörigen rekursiven Partionierung\nAbb. ?? wird der Knoten der Spitze auch als Wurzel(knoten) bezeichnet.\nVon diesem Knoten entspringen alle Pfade.\nEin Pfad ist die geordnete Menge der Pfade mit ihren Knoten ausgehend von der Wurzel bis zu einem Blatt.\nKnoten, aus denen kein Kanten mehr wegführt (“Endknoten”) werden als Blätter bezeichnet.\nVon einem Knoten gehen zwei Kanten aus (oder gar keine).\nKnoten, von denen zwei Kanten ausgehen, spiegeln eine Bedingung (Prüfung) wider, im Sinne einer Aussage,\ndie mit ja oder nein beantwortet werden kann.\nDie Anzahl der Knoten eines Pfads entsprechen den Ebenen bzw. der Tiefe des Baumes.\nVon der obersten Ebene (Wurzelknoten) kann man die \\(e\\) Ebenen aufsteigend durchnummerieren,\nbeginnend bei 1: \\(1,2,\\ldots,e\\).","code":""},{"path":"entscheidungsbäume.html","id":"bäume-als-regelmaschinen-rekursiver-partionierung","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.4 Bäume als Regelmaschinen rekursiver Partionierung","text":"Ein Baum kann man als eine Menge von Regeln, im Sinne von Wenn-dann-sonst-Aussagen, sehen:diesem Fall, zwei Prädiktoren, ist der Prädiktorenraum drei Regionen unterteilt:\nDer Baum hat drei Blätter.Für Abb. 9.2 ergibt sich eine komplexere Aufteilung, s. auch Abb. 9.3.\nFigure 9.2: Beispiel für einen Entscheidungsbaum\nKleine Lesehilft für Abb. 9.2:Für jeden Knoten steht der ersten Zeile der vorhergesagte Wert, z.B. 0 im Wurzelknotendarunter steht der Anteil (die Wahrscheinlichkeit) für die diesem Knoten vorhergesagte Kategorie (0 oder 1)darunter (3. Zeile) steht der Anteil der Fälle (Gesamt-Datensatz) diesem Knoten, z.B. 100%\nFigure 9.3: Partionierung Rechtecke durch Entscheidungsbäume\nWie der Algorithmus oben zeigt,\nwird der Prädiktorraum wiederholt (rekursiv) aufgeteilt,\nund zwar Rechtecke,s. Abb. 9.3.\nMan nennt (eine Implementierung) dieses Algorithmus auch rpart.Das Regelwerk zum Baum aus Abb. 9.2 sieht aus:Kleine Lesehilfe:\nAnder Wurzel root des Baumes, Knoten 1)haben wir 891 Fälle,\nvon denen 342 nicht unserer Vorhersage yval entsprechen, also loss sind,\ndas ist ein Anteil, (yprob) von 0.38.\nUnsere Vorhersage ist 0, da das die Mehrheit diesem Knoten ist,\ndieser Anteil beträgt ca. 61%.\nder Klammer stehen also die Wahrscheinlichkeiten für alle Ausprägungen von Y:, 0 und 1,\ndiesem Fall.\nEntsprechendes gilt für jeden weiteren Knoten.Ein kurzer Check der Häufigkeit Wurzelknoten:Solche Entscheidungsbäume zu erstellen, ist nichts neues.\nMan kann sie mit einer einfachen Checkliste oder Entscheidungssystem vergleichen.\nDer Unterschied zu Entscheidungsbäumen im maschinellen Lernen ist nur,\ndass die Regeln aus den Daten gelernt werden, man muss sie nicht vorab kennen.Noch ein Beispiel ist Abb. 9.4 gezeigt (James et al. 2021):\nOben links zeigt eine unmögliche Partionierung (für einen Entscheidungsbaum).\nOben rechts zeigt die Regionen,\ndie sich durch den Entscheidungsbaum unten links ergeben.\nUntenrechts ist der Baum 3D dargestellt.\nFigure 9.4: Ein weiteres Beispiel zur Darstellung von Entscheidungsbäumen\n","code":"Wenn Prädiktor A = 1 ist dann\n|  Wenn Prädiktor B = 0 ist dann p = 10%\n|  sonst p = 30%\nsonst p = 50%\ntitanic_train$Survived = as.factor(titanic_train$Survived)\n\nti_tree <-\n  decision_tree() %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"classification\") %>%\n  fit(Survived ~ Pclass + Age, data = titanic_train)\n\nti_tree## parsnip model object\n## \n## n= 891 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##   1) root 891 342 0 (0.61616162 0.38383838)  \n##     2) Pclass>=2.5 491 119 0 (0.75763747 0.24236253)  \n##       4) Age>=6.5 461 102 0 (0.77874187 0.22125813) *\n##       5) Age< 6.5 30  13 1 (0.43333333 0.56666667) *\n##     3) Pclass< 2.5 400 177 1 (0.44250000 0.55750000)  \n##       6) Age>=17.5 365 174 1 (0.47671233 0.52328767)  \n##        12) Pclass>=1.5 161  66 0 (0.59006211 0.40993789) *\n##        13) Pclass< 1.5 204  79 1 (0.38725490 0.61274510)  \n##          26) Age>=44.5 67  32 0 (0.52238806 0.47761194)  \n##            52) Age>=60.5 14   3 0 (0.78571429 0.21428571) *\n##            53) Age< 60.5 53  24 1 (0.45283019 0.54716981)  \n##             106) Age< 47.5 13   3 0 (0.76923077 0.23076923) *\n##             107) Age>=47.5 40  14 1 (0.35000000 0.65000000) *\n##          27) Age< 44.5 137  44 1 (0.32116788 0.67883212) *\n##       7) Age< 17.5 35   3 1 (0.08571429 0.91428571) *##   Survived   n\n## 1        0 549\n## 2        1 342"},{"path":"entscheidungsbäume.html","id":"klassifikation","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.5 Klassifikation","text":"Bäume können für Zwecke der Klassifikation (nominal skalierte AV) oder Regression (numerische AV) verwendet werden.\nBetrachten wir zunächst die binäre Klassifikation, also für eine zweistufige (nominalskalierte) AV.\nDas Ziel des Entscheidungsmodel-Algorithmus ist es,\nzu Blättern zu kommen, die möglichst “sortenrein” sind,\nsich also möglichst klar für eine (der beiden) Klassen \\(\\) oder \\(B\\) aussprechen.\nNach dem Motto: “Wenn Prädiktor 1 kleiner \\(x\\) und wenn Prädiktor 2 gleich \\(y\\),\ndann handelt es sich beim vorliegenden Fall ziemlich sicher um Klasse \\(\\).”Je homogener die Verteilung der AV pro Blatt, desto genauer die Vorhersagen.Unsere Vorhersage einem Blatt entspricht der Merheit bzw. der häufigsten Kategorie diesem Blatt.","code":""},{"path":"entscheidungsbäume.html","id":"gini-als-optimierungskriterium","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.6 Gini als Optimierungskriterium","text":"Es gibt mehrere Kennzahlen, die zur Optimierung bzw. zur Entscheidung zum Aufbau des Entscheidungsbaum herangezogen werden.\nZwei übliche sind der Gini-Koeffizient und die Entropie.\nBei Kennzahlen sind Maß für die Homogenität oder “Sortenreinheit” (vs. Heterogenität, engl. auch impurity).Den Algorithmus zur Erzeugung des Baumes kann man darstellen:Ein Bedingung könnte sein Age >= 18 oder Years < 4.5.Es kommen mehrere Abbruchkriterium Frage:Eine Mindestanzahl von Beobachtungen pro Knoten wird unterschritten (minsplit)Die maximale Anzahl Ebenen ist erreicht (maxdepth)Die minimale Zahl Beobachtungen eines Blatts wird unterschritten (minbucket)Der Gini-Koeffizient ist im Fall einer UV mit zwei Stufen, \\(c_A\\) und \\(c_B\\), definiert:\\[G = 1 - \\left(p(c_A)^2 + (1-p(c_A))^2\\right)\\]Der Algorithmus ist “gierig” (greedy): Optimiert werden lokal optimale Aufteilungen,\nauch wenn das bei späteren Aufteilungen im Baum dann insgesamt zu geringerer Homogenität führt.Die Entropie ist definiert als\\[D = - \\sum_{k=1}^K p_k \\cdot log(p_k),\\]wobei \\(K\\) die Anzahl der Kategorien indiziert.Gini-Koeffizient und Entropie kommen oft zu ähnlichen numerischen Ergebnissen,\ndass wir uns im Folgenden auf den Gini-Koeffizienten konzentieren werden.BeispielVergleichen wir drei Bedingungen mit jeweils \\(n=20\\) Fällen, die zu unterschiedlich homogenen Knoten führen:10/1015/519/1Was ist jeweils der Wert des Gini-Koeffizienten?Wie man sieht, sinkt der Wert des Gini-Koeffizienten (“G-Wert”), je homogener die Verteilung ist.\nMaximal heterogen (“gemischt”) ist die Verteilung, wenn alle Werte gleich oft vorkommen,\ndiesem Fall also 50%/50%.Neben dem G-Wert für einzelne Knoten kann man den G-Wert für eine Aufteilung (“Split”) berechnen,\nalso die Fraeg beantworten, ob die Aufteilung eines Knoten zwei zu mehr Homogenität führt.\nDer G-Wert einer Aufteilung ist die gewichtete Summe der G-Werte der beiden Knoten (links, \\(l\\) und rechts, \\(r\\)):\\[G_{split} = p(l) G_{l} + p(r) G_r\\]Der Gewinn (gain) Homogenität ist dann die Differenz des G-Werts der kleineren Ebene und der Aufteilung:\\[G_{gain} = G - G_{split}\\]Der Algorithmus kann auch bei UV mit mehr als zwei, also \\(K\\) Stufen, \\(c_1, c_2, \\ldots, c_K\\) verwendet werden:\\[G= 1- \\sum_{k=1}^K p(c_k)^2\\]","code":"Wiederhole für jede Ebenes\n|  prüfe für alle Prädiktoren alle möglichen Bedingungen\n|  wähle denjenigen Prädiktor mit derjenigen Bedingung, der die Homogenität maximiert\nsolange bis Abbruchkriterium erreicht ist.\nG1 <- 1 - ((10/20)^2 + (10/20)^2)\nG1## [1] 0.5\nG2 <- 1 - ((15/20)^2 + (5/20)^2)\nG2## [1] 0.375\nG3 <- 1 - ((19/20)^2 + (1/20)^2)\nG3## [1] 0.095"},{"path":"entscheidungsbäume.html","id":"metrische-prädiktoren","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.7 Metrische Prädiktoren","text":"Außerdem ist es möglich, Bedingung bei metrischen UV auf ihre Homogenität hin zu bewerten,\nalso Aufteilungen der Art Years < 4.5 zu tätigen.\nDazu muss man einen Wert identifieren, bei dem man auftrennt.Das geht etwa :Abbildung 9.5 stellt dieses Vorgehen schematisch dar (Rhys 2020).\nFigure 9.5: Aufteilungswert bei metrischen Prädiktoren\n","code":"Sortiere die Werte eines Prädiktors (aufsteigend)\nFür jedes Paar an aufeinanderfolgenden Werten berechne den G-Wert\nFinde das Paar mit dem höchsten G-Wert aus allen Paaren\nNimm den Mittelwert der beiden Werte dieses Paares: Das ist der Aufteilungswert"},{"path":"entscheidungsbäume.html","id":"regressionbäume","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.8 Regressionbäume","text":"Bei Regressionsbäumen wird nicht ein Homogenitätsmaß wie der Gini-Koeffizient als Optimierungskriterium\nherangezogen, sondern die RSS (Residual Sum Squares) bietet sich .Die \\(J\\) Regionen (Partionierungen) des Prädiktorraums \\(R_1, R_2, \\ldots, R_J\\) müssen gewählt werden,\ndass RSS minimal ist:\\[RSS = \\sum^J_{j=1}\\sum_{\\R_j}(u_i - \\hat{y}_{R_j})^2,\\]wobei \\(\\hat{y}\\) der (vom Baum) vorhergesagte Wert ist für die \\(j\\)-te Region.","code":""},{"path":"entscheidungsbäume.html","id":"baum-beschneiden","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.9 Baum beschneiden","text":"Ein Problem mit Entscheidungsbäumen ist,\ndass ein zu komplexer Baum, “zu verästelt” sozusagen,\nhohem Maße Overfitting ausgesetzt ist:\nBei höheren Ebenen im Baum ist die Anzahl der Beobachtungen zwangsläufig klein,\nbedeutet, dass viel Rauschen gefittet wird.Um das Overfitting zu vermeiden, gibt es zwei auf der Hand liegende Maßnahmen:Den Baum nicht groß werden lassenDen Baum “zurückschneiden”Die 1. Maßnahme beruht auf dem Festlegen einer maximalen Zahl Ebenen (maxdepth) oder einer minimalen Zahl Fällen pro Knoten (minsplit) oder im Blatt (minbucket).Die 2. Maßnahme, das Zurückschneiden (pruning) des Baumes hat als Idee, einen “Teilbaum” \\(T\\) zu finden,\nder klein wie möglich ist, aber gut wie möglich präzise Vorhersagen erlaubt.\nDazu belegen wir die RSS eines Teilbaums (subtree) mit einem Strafterm \\(s = \\alpha |T|\\),\nwobei \\(|T|\\) die Anzahl der Blätter des Baums entspricht. \\(\\alpha\\) ist ein Tuningparameter,\nalso ein Wert, der nicht vom Modell berechnet wird, sondern von uns gesetzt werden muss -\nzumeist durch schlichtes Ausprobieren.\n\\(\\alpha\\) wägt ab zwischen Komplexität und Fit (geringe RSS).\nWenn \\(\\alpha=0\\) haben wir eine normalen, unbeschnittenen Baum \\(T_0\\).\nJe größer \\(\\alpha\\) wird, desto höher wird der “Preis” für viele Blätter, also für Komplexität\nund der Baum wird kleiner.\nDieses Vorgehen nennt man auch cost complexity pruning.\nDaher nennt man den zugehörigen Tuningparameter auch Cost Complexity \\(C_p\\).","code":""},{"path":"entscheidungsbäume.html","id":"das-rechteck-schlägt-zurück","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.10 Das Rechteck schlägt zurück","text":"Entscheidungsbäume zeichnen sich durch rechtecke (rekursive) Partionierungen des Prädiktorenraums aus.\nLineare Modelle durch eine einfache lineare Partionierung (wenn man Klassifizieren möchte),\nAbb. 9.6 verdeutlicht diesen Unterschied (James et al. 2021).\nFigure 9.6: Rechteckige vs. lineare Partionierung\nJetzt kann sich fragen: Welches Vorgehen ist besser - das rechteckige oder das lineare Partionierungen.\nDa gibt es eine klare Antwort: Es kommt drauf .\nWie Abb. 9.6 gibt es Datenlagen, denen das eine Vorgehen zu homogenerer Klassifikation führt\nund Situationen, denen das andere Vorgehen besser ist, vgl. Abb. 9.7.\nFigure 9.7: Free Lunch?\n","code":""},{"path":"entscheidungsbäume.html","id":"tidymodels-2","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.11 Tidymodels","text":"Probieren wir den Algorithmus Entscheidungsbäume einem einfachen Beispiel R mit Tidymodels aus.Die Aufgabe sei, Spritverbrauch (möglichst exakt) vorherzusagen.Ein ähnliches Beispiel, mit analogem Vorgehen, findet sich dieser Fallstude.","code":""},{"path":"entscheidungsbäume.html","id":"initiale-datenaufteilung","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.11.1 Initiale Datenaufteilung","text":"Die Warnung zeigt uns, dass der Datensatz sehr klein ist; stimmt. Ignorieren wir hier einfach.Wie man auf der Hilfeseite der Funktion sieht,\nwird per Voreinstellung 3/1 aufgeteilt, also 75% das Train-Sample, 25% der Daten ins Test-Sample.Bei \\(n=32\\) finden also 8 Autos ihren Weg ins Test-Sample und die übrigen 24 ins Train-Sample.\nBei der kleinen Zahl könnte man sich (berechtigterweise) fragen,\nob es Sinn macht, die spärlichen Daten noch mit einem Test-Sample weiter zu dezimieren.\nDer Einwand ist nicht unberechtigt,\nallerdings zieht der Verzicht auf ein Test-Sample andere Probleme, Overfitting namentlich, nach sich.","code":"\nlibrary(tidymodels)\ndata(\"mtcars\")\n\nset.seed(42)  # Reproduzierbarkeit\nd_split <- initial_split(mtcars, strata = mpg)## Warning: The number of observations in each quantile is below the recommended threshold of 20.\n## • Stratification will use 1 breaks instead.## Warning: Too little data to stratify.\n## • Resampling will be unstratified.\nd_train <- training(d_split)\nd_test <- testing(d_split)"},{"path":"entscheidungsbäume.html","id":"kreuzvalidierung-definieren","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.11.2 Kreuzvalidierung definieren","text":"Die Defaults (Voreinstellungen) der Funktion vfold_cv() können, wie immer, auf der Hilfeseite der Funktion nachgelesen werden.Da die Stichprobe sehr klein ist,\nbietet es sich , eine kleine Zahl Faltungen (folds) zu wählen.\nBei 10 Faltungen beinhaltete eine Stichprobe gerade 10% der Fälle Train-Sample,\nalso etwa … 2!Zur Erinnerung:\nJe größer die Anzahl der Repeats,\ndesto genauer schätzen wir die Modellgüte.","code":"\nd_cv <- vfold_cv(d_train, strata = mpg, repeats = 5, v = 5) \nd_cv## #  5-fold cross-validation repeated 5 times using stratification \n## # A tibble: 25 × 3\n##    splits         id      id2  \n##    <list>         <chr>   <chr>\n##  1 <split [19/5]> Repeat1 Fold1\n##  2 <split [19/5]> Repeat1 Fold2\n##  3 <split [19/5]> Repeat1 Fold3\n##  4 <split [19/5]> Repeat1 Fold4\n##  5 <split [20/4]> Repeat1 Fold5\n##  6 <split [19/5]> Repeat2 Fold1\n##  7 <split [19/5]> Repeat2 Fold2\n##  8 <split [19/5]> Repeat2 Fold3\n##  9 <split [19/5]> Repeat2 Fold4\n## 10 <split [20/4]> Repeat2 Fold5\n## # … with 15 more rows"},{"path":"entscheidungsbäume.html","id":"rezept-definieren-2","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.11.3 Rezept definieren","text":"Hier ein einfaches Rezept:","code":"\nrecipe1 <-\n  recipe(mpg ~ ., data = d_train) %>% \n  step_impute_knn() %>% \n  step_normalize() %>% \n  step_dummy() %>% \n  step_other(threshold = .1)"},{"path":"entscheidungsbäume.html","id":"modell-definieren-2","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.11.4 Modell definieren","text":"Wenn Sie sich fragen, woher Sie die Optionen für die Tuningparameter wissen sollen: Schauen Sie mal die Hilfeseite des Pakets {{dials}}; das Paket ist Teil von Tidymodels.Die Berechnung des Modells läuft über das Paket {{rpart}},\nwir durch set_engine() festgelegt haben.Der Parameter Cost Complexity, \\(C_p\\) oder manchmal auch mit \\(\\alpha\\) bezeichnet,\nhat einen typischen Wertebereich von \\(10^{-10}\\) bis \\(10^{-1}\\):Hier ist der Wert Log-Einheiten angegeben. Wenn Sie sich fragen, woher Sie das bitteschön wissen sollen:\nNaja, es steht auf der Hilfeseite 😄.Unser Modell ist also definiert:Mit tune() weist man den betreffenden Parameter als “zu tunen” aus -\ngute Werte sollen durch Ausprobieren während des Berechnens bestimmt werden.\nGenauer gesagt soll das Modell für jeden Wert (oder jede Kombination Werten von Tuningparametern)\nberechnet werden.Eine Kombination Tuningparameter-Werten, die ein Modell spezifizieren,\nsozusagen erst “fertig definieren”, nennen wir einen Modellkandidaten.Definieren wir also eine Tabelle (grid) mit Werten, die ausprobiert, “getuned” werden sollen.\nWir haben oben dre Tuningparameter bestimmt. Sagen wir,\nwir hätten gerne jeweils 5 Werte pro Parameter.Für jeden Parameter sind Wertebereiche definiert;\ndieser Wertebereich wird gleichmäßig (daher grid regular) aufgeteilt;\ndie Anzahl der verschiedenen Werte pro Parameter wird druch levels gegeben.Mehr dazu findet sich auf der Hilfeseite zu grid_regular().Wenn man die alle miteinander durchprobiert, entstehen \\(4^3\\) Kombinationen,\nalso Modellkandidaten.Allgemeiner gesagt sind das bei \\(n\\) Tuningparametern mit jeweils \\(m\\) verschiedenen Werten \\(m^n\\) Möglichkeiten,\nspricht Modellkandidaten. Um diesen Faktor erhöht sich die Rechenzeit im Vergleich zu einem Modell ohne Tuning.\nMan sieht gleich, dass die Rechenzeit schnell unangenehm lang werden kann.Entsprechend hat unsere Tabelle diese Zahl Zeilen.\nJede Zeile definiert einen Modellkandidaten,\nalso eine Berechnung des Modells.Man beachte, dass außer Definitionen bisher nichts passiert ist – vor allem haben wir noch\nnichts berechnet.\nSie scharren mit den Hufen? Wollen endlich loslegen?\nAlso gut.","code":"\ntree_model <-\n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune(),\n    min_n = tune()\n  ) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"regression\")\ncost_complexity()## Cost-Complexity Parameter (quantitative)\n## Transformer: log-10 [1e-100, Inf]\n## Range (transformed scale): [-10, -1]\ntree_model## Decision Tree Model Specification (regression)\n## \n## Main Arguments:\n##   cost_complexity = tune()\n##   tree_depth = tune()\n##   min_n = tune()\n## \n## Computational engine: rpart\ntree_grid <-\n  grid_regular(\n    cost_complexity(),\n    tree_depth(),\n    min_n(),\n    levels = 4\n  )\ndim(tree_grid)## [1] 64  3\nhead(tree_grid)## # A tibble: 6 × 3\n##   cost_complexity tree_depth min_n\n##             <dbl>      <int> <int>\n## 1    0.0000000001          1     2\n## 2    0.0000001             1     2\n## 3    0.0001                1     2\n## 4    0.1                   1     2\n## 5    0.0000000001          5     2\n## 6    0.0000001             5     2"},{"path":"entscheidungsbäume.html","id":"workflow-definieren-2","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.11.5 Workflow definieren","text":"Fast vergessen: Wir brauchen noch einen Workflow.","code":"\ntree_wf <-\n  workflow() %>% \n  add_model(tree_model) %>% \n  add_recipe(recipe1)"},{"path":"entscheidungsbäume.html","id":"modell-tunen-und-berechnen","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.11.6 Modell tunen und berechnen","text":"Achtung: Das Modell zu berechnen kann etwas dauern.\nEs kann daher Sinn machen,\ndas Modell abzuspeichern,\ndass Sie beim erneuten Durchlaufen nicht nochmal berechnen müssen,\nsondern einfach von der Festplatte laden können;\ndas setzt natürlich voraus,\ndass sich Modell nichts geändert hat.Es bietet sich dem Fall , das Ergebnis-Objekt als R Data serialized (rds) abzuspeichern:Bzw. wieder aus der RDS-Datei zu importieren:Hier oder hier kann man einiges zum Unterschied einer RDS-Datei vs. einer “normalen” R-Data-Datei nachlesen.\nWenn man möchte 😉.Die Warnhinweise kann man sich ausgeben lassen:Wie gesagt,\ndiesem Fall war die Stichprobengröße sehr klein.","code":"\ndoParallel::registerDoParallel()  # mehrere Kerne parallel nutzen\n\nset.seed(42)\ntic()  # Stoppuhr an\ntrees_tuned <-\n  tune_grid(\n    object = tree_wf,\n    grid = tree_grid,\n    resamples = d_cv\n  )\ntoc()  # Stoppuhr aus\nwrite_rds(trees_tuned, \"objects/trees1.rds\")\ntrees_tuned <- read_rds(\"objects/trees1.rds\")\ntrees_tuned## # Tuning results\n## # 5-fold cross-validation repeated 5 times using stratification \n## # A tibble: 25 × 5\n##    splits         id      id2   .metrics           .notes           \n##    <list>         <chr>   <chr> <list>             <list>           \n##  1 <split [19/5]> Repeat1 Fold1 <tibble [128 × 7]> <tibble [32 × 3]>\n##  2 <split [19/5]> Repeat1 Fold2 <tibble [128 × 7]> <tibble [32 × 3]>\n##  3 <split [19/5]> Repeat1 Fold3 <tibble [128 × 7]> <tibble [32 × 3]>\n##  4 <split [19/5]> Repeat1 Fold4 <tibble [128 × 7]> <tibble [32 × 3]>\n##  5 <split [20/4]> Repeat1 Fold5 <tibble [128 × 7]> <tibble [32 × 3]>\n##  6 <split [19/5]> Repeat2 Fold1 <tibble [128 × 7]> <tibble [32 × 3]>\n##  7 <split [19/5]> Repeat2 Fold2 <tibble [128 × 7]> <tibble [32 × 3]>\n##  8 <split [19/5]> Repeat2 Fold3 <tibble [128 × 7]> <tibble [32 × 3]>\n##  9 <split [19/5]> Repeat2 Fold4 <tibble [128 × 7]> <tibble [32 × 3]>\n## 10 <split [20/4]> Repeat2 Fold5 <tibble [128 × 7]> <tibble [32 × 3]>\n## # … with 15 more rows\n## \n## There were issues with some computations:\n## \n##   - Warning(s) x320: 27 samples were requested but there were 19 rows in the data. 19 ...   - Warning(s) x80: 27 samples were requested but there were 19 rows in the data. 19 ...   - Warning(s) x320: 27 samples were requested but there were 19 rows in the data. 19 ...   - Warning(s) x80: 27 samples were requested but there were 19 rows in the data. 19 ...   - Warning(s) x4: 27 samples were requested but there were 19 rows in the data. 19 ...\n## \n## Use `collect_notes(object)` for more information.\ncollect_notes(trees_tuned)## # A tibble: 804 × 5\n##    id      id2   location                      type    note                     \n##    <chr>   <chr> <chr>                         <chr>   <chr>                    \n##  1 Repeat1 Fold1 preprocessor 1/1, model 33/64 warning 27 samples were requeste…\n##  2 Repeat1 Fold1 preprocessor 1/1, model 34/64 warning 27 samples were requeste…\n##  3 Repeat1 Fold1 preprocessor 1/1, model 35/64 warning 27 samples were requeste…\n##  4 Repeat1 Fold1 preprocessor 1/1, model 36/64 warning 27 samples were requeste…\n##  5 Repeat1 Fold1 preprocessor 1/1, model 37/64 warning 27 samples were requeste…\n##  6 Repeat1 Fold1 preprocessor 1/1, model 38/64 warning 27 samples were requeste…\n##  7 Repeat1 Fold1 preprocessor 1/1, model 39/64 warning 27 samples were requeste…\n##  8 Repeat1 Fold1 preprocessor 1/1, model 40/64 warning 27 samples were requeste…\n##  9 Repeat1 Fold1 preprocessor 1/1, model 41/64 warning 27 samples were requeste…\n## 10 Repeat1 Fold1 preprocessor 1/1, model 42/64 warning 27 samples were requeste…\n## # … with 794 more rows"},{"path":"entscheidungsbäume.html","id":"modellgüte-evaluieren","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.11.7 Modellgüte evaluieren","text":"Praktischerweise gibt es eine Autoplot-Funktion, um die besten Modellparameter auszulesen:","code":"\ncollect_metrics(trees_tuned)## # A tibble: 128 × 9\n##    cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n##              <dbl>      <int> <int> <chr>   <chr>      <dbl> <int>   <dbl>\n##  1    0.0000000001          1     2 rmse    standard   3.46     25  0.223 \n##  2    0.0000000001          1     2 rsq     standard   0.666    21  0.0385\n##  3    0.0000001             1     2 rmse    standard   3.46     25  0.223 \n##  4    0.0000001             1     2 rsq     standard   0.666    21  0.0385\n##  5    0.0001                1     2 rmse    standard   3.46     25  0.223 \n##  6    0.0001                1     2 rsq     standard   0.666    21  0.0385\n##  7    0.1                   1     2 rmse    standard   3.46     25  0.223 \n##  8    0.1                   1     2 rsq     standard   0.666    21  0.0385\n##  9    0.0000000001          5     2 rmse    standard   2.62     25  0.265 \n## 10    0.0000000001          5     2 rsq     standard   0.863    25  0.0279\n## # … with 118 more rows, and 1 more variable: .config <chr>\nautoplot(trees_tuned)"},{"path":"entscheidungsbäume.html","id":"bestes-modell-auswählen","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.11.8 Bestes Modell auswählen","text":"Aus allen Modellkandidaten wählen wir jetzt das beste Modell aus:Mit diesem besten Kandidaten definieren wir jetzt das “finale” Modell,\nwir “finalisieren” das Modell mit den besten Modellparametern:Hier ist, unser finaler Baum 🌳.Schließlich updaten wir mit dem finalen Baum noch den Workflow:","code":"\nselect_best(trees_tuned)## # A tibble: 1 × 4\n##   cost_complexity tree_depth min_n .config              \n##             <dbl>      <int> <int> <chr>                \n## 1          0.0001          5     2 Preprocessor1_Model07\ntree_final <-\n  finalize_model(tree_model, parameters = select_best(trees_tuned))\n\ntree_final## Decision Tree Model Specification (regression)\n## \n## Main Arguments:\n##   cost_complexity = 1e-04\n##   tree_depth = 5\n##   min_n = 2\n## \n## Computational engine: rpart\nfinal_wf <-\n  tree_wf %>% \n  update_model(tree_final)"},{"path":"entscheidungsbäume.html","id":"final-fit","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.11.9 Final Fit","text":"Jetzt fitten wir dieses Modell auf das ganze Train-Sample und predicten auf das Test-Sample:Voilà: Die Modellgüte für das Test-Sample:\nIm Schnitt liegen wir ca. 4 Meilen daneben mit unseren Vorhersagen,\nwenn wir RMSE mal locker interpretieren wollen.der Regel ist übrigens RMSE interessanter als R-Quadrat,\nda R-Quadrat die Güte eines Korrelationsmusters vorhersagt,\naber RMSE die Präzision der Vorhersage,\nalso sozusagen die Kürze der Fehlerbalken.","code":"\ntree_fit_final <-\n  final_wf %>% \n  last_fit(d_split)\n\ntree_fit_final## # Resampling results\n## # Manual resampling \n## # A tibble: 1 × 6\n##   splits         id               .metrics .notes   .predictions     .workflow \n##   <list>         <chr>            <list>   <list>   <list>           <list>    \n## 1 <split [24/8]> train/test split <tibble> <tibble> <tibble [8 × 4]> <workflow>\ncollect_metrics(tree_fit_final)## # A tibble: 2 × 4\n##   .metric .estimator .estimate .config             \n##   <chr>   <chr>          <dbl> <chr>               \n## 1 rmse    standard       3.93  Preprocessor1_Model1\n## 2 rsq     standard       0.683 Preprocessor1_Model1"},{"path":"entscheidungsbäume.html","id":"nur-zum-spaß-vergleich-mit-linearem-modell","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.11.10 Nur zum Spaß: Vergleich mit linearem Modell","text":"Ein einfaches lineares Modell,\nhätte das jetzt wohl für eine Modellgüte?Wie präzise ist die Vorhersage im Test-Sample?Das lineare Modell schneidet etwas (deutlich?) schlechter ab als das einfache Baummodell.Man beachte, dass die Modellgüte im Train-Samle höher ist als im Test-Sample (Overfitting).","code":"## 7.754 sec elapsed## # A tibble: 2 × 6\n##   .metric .estimator  mean     n std_err .config             \n##   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n## 1 rmse    standard   4.16     25  0.362  Preprocessor1_Model1\n## 2 rsq     standard   0.624    25  0.0587 Preprocessor1_Model1## # A tibble: 2 × 4\n##   .metric .estimator .estimate .config             \n##   <chr>   <chr>          <dbl> <chr>               \n## 1 rmse    standard       5.24  Preprocessor1_Model1\n## 2 rsq     standard       0.434 Preprocessor1_Model1"},{"path":"entscheidungsbäume.html","id":"aufgaben-6","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.12 Aufgaben","text":"Fallstudie Oregon SchoolsFallstudie WindturbinenFallstudie Churn","code":""},{"path":"entscheidungsbäume.html","id":"vertiefung-3","chapter":"Kapitel 9 Entscheidungsbäume","heading":"9.13 Vertiefung","text":"Visualisierung des ML-Ablaufs Beispiel des Entscheidungsbaums, Teil 1Visualisierung des ML-Ablaufs Beispiel des Entscheidungsbaums, Teil 2","code":""},{"path":"ensemble-lerner.html","id":"ensemble-lerner","chapter":"Kapitel 10 Ensemble Lerner","heading":"Kapitel 10 Ensemble Lerner","text":"","code":""},{"path":"ensemble-lerner.html","id":"lernsteuerung-7","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.1 Lernsteuerung","text":"","code":""},{"path":"ensemble-lerner.html","id":"lernziele-8","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.1.1 Lernziele","text":"Sie können Algorithmen für Ensemble-Lernen erklären, d.. Bagging, AdaBoost, XGBoost, Random ForestSie wissen, anhand welche Tuningparamter man Overfitting bei diesen Algorithmen begrenzen kannSie können diese Verfahren R berechnen","code":""},{"path":"ensemble-lerner.html","id":"literatur-8","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.1.2 Literatur","text":"Rhys, Kap. 8","code":""},{"path":"ensemble-lerner.html","id":"hinweise-3","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.1.3 Hinweise","text":"Nutzen Sie StackOverflow als Forum für Ihre Fragen - Hier ein Beispiel zu einer Fehlermeldung, die mir Kopfzerbrechen bereitete","code":""},{"path":"ensemble-lerner.html","id":"vorbereitung-7","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.2 Vorbereitung","text":"diesem Kapitel werden folgende R-Pakete benötigt:","code":"\nlibrary(tidymodels)\nlibrary(tictoc)  # Zeitmessung\nlibrary(vip)  # Variable importance plot"},{"path":"ensemble-lerner.html","id":"hinweise-zur-literatur","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.3 Hinweise zur Literatur","text":"Die folgenden Ausführungen basieren primär auf Rhys (2020), aber auch auf James et al. (2021) und (weniger) Kuhn Johnson (2013).","code":""},{"path":"ensemble-lerner.html","id":"wir-brauchen-einen-wald","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.4 Wir brauchen einen Wald","text":"Ein Pluspunkt von Entscheidungsbäumen ist ihre gute Interpretierbarkeit.\nMan könnte behaupten, dass Bäume eine typische Art des menschlichen Entscheidungsverhalten\nnachahmen: “Wenn , dann tue B, ansonsten tue C” (etc.).\nAllerdings: Einzelne Entscheidungsbäume haben oft keine gute Prognosegenauigkeit.\nDer oder zumindest ein Grund ist, dass sie (zwar wenig Bias aber) viel Varianz aufweisen.\nDas sieht man z.B. daran, dass die Vorhersagegenauigkeit stark schwankt,\nwählt man eine andere Aufteilung von Train- vs. Test-Sample.\nAnders gesagt: Bäume overfitten ziemlich schnell.\nUnd obwohl das -Free-Lunch-Theorem zu den Grundfesten des maschinellen Lernens\n(oder zu allem wissenschaftlichen Wissen) gehört,\nkann man festhalten, dass sog. Ensemble-Lernen fast immer besser sind\nals einzelne Baummodelle.\nKurz gesagt: Wir brauchen einen Wald: 🌳🌳🌳4","code":""},{"path":"ensemble-lerner.html","id":"was-ist-ein-ensemble-lerner","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.5 Was ist ein Ensemble-Lerner?","text":"Ensemble-Lerner kombinieren mehrere schwache Lerner zu einem starken Lerner.\nDas Paradebeispiel sind baumbasierte Modelle;\ndarauf wird sich die folgende Ausführung auch begrenzen.\nAber theoretisch kann man jede Art von Lerner kombinieren.\nBei numerischer Prädiktion wird bei Ensemble-Lerner zumeist der Mittelwert als Optmierungskriterium\nherangezogen; bei Klassifikation (nominaler Prädiktion) hingegen die modale Klasse (also die häufigste).\nWarum hilft es, mehrere Modelle (Lerner) zu einem zu aggregieren?\nDie Antwort lautet, dass die Streuung der Mittelwerte sinkt,\nwenn die Stichprobengröße steigt.\nZieht man Stichproben der Größe 1, werden die Mittelwerte stark variieren,\naber bei größeren Stichproben (z.B. Größe 100) deutlich weniger5.\nDie Streuung der Mittelwerte den Stichproben nennt man bekanntlich Standardefehler (se).\nDen se des Mittelwerts (\\(se_M\\)) für eine normalverteilte Variable \\(X \\sim \\mathcal{N}(\\mu, \\sigma)\\) gilt:\n\\(se_{M} = \\sigma / \\sqrt(n)\\), wobei \\(\\sigma\\) die SD der Verteilung und \\(\\mu\\) den Erwartungswert (“Mittelwert”) meint,\nund \\(n\\) ist die Stichprobengröße.Je größer die Stichprobe, desto kleiner die Varianz des Schätzers (ceteris paribus).\nAnders gesagt: Größere Stichproben schätzen genauer als kleine Stichproben.Aus diesem Grund bietet es sich ,\nschwache Lerner mit viel Varianz zu kombinieren,\nda die Varianz verringert wird.","code":""},{"path":"ensemble-lerner.html","id":"bagging","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.6 Bagging","text":"","code":""},{"path":"ensemble-lerner.html","id":"bootstrapping","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.6.1 Bootstrapping","text":"Das erste baumbasierte Modell, vorgestellt werden soll,\nbasiert auf sog. Bootstrapping, ein Standardverfahren der Statistik (James et al. 2021).Bootstrapping ist eine Nachahmung für folgende Idee:\nHätte man viele Stichproben aus der relevanten Verteilung,\nkönnte man z.B. die Genauigkeit eines Modells \\(\\hat{f}_{\\bar{X}}\\) zur Schätzung des Erwartungswertes \\(\\mu\\) einfach dadurch bestimmen,\nindem man se berechnet, also die Streuung der Mitterwerte \\(\\bar{X}\\) berechnet.\nAußerdem gilt, dass die Präzision der Schätzung des Erwartungswerts steigt mit steigendem Stichprobenumfang \\(n\\).\nWir könnten also für jede der \\(B\\) Stichproben, \\(b=1,\\ldots, B\\), ein (Baum-)Modell berechnen, \\(\\hat{f}^b\\),\nund dann deren Vorhersagen aggregieren (zum Mittelwert oder Modalwert).\nDas kann man formal darstellen (James et al. 2021):\\[\\hat{f}_{\\bar{X}} = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{f}^b\\]Mit diesem Vorgehen kann die Varianz des Modells \\(\\hat{f}_{\\bar{X}}\\) verringert werden;\ndie Vorhersagegenauigkeit steigt.Leider haben wir der Regel nicht viele (\\(B\\)) Datensätze.Daher “bauen” wir uns aus dem einzelnen Datensatz, der uns zur Verfügung steht,\nviele Datensätze.\nDas hört sich nach “good true” an6\nWeil es sich unglaubwürdig anhört, nennt man das entsprechende Verfahren (gleich kommt es!) auch “Münchhausen-Methode”,\nnach dem berühmten Lübgenbaron.\nDie Amerikaner ziehen sich übrigens nicht Schopf aus dem Sumpf, sondern\nmit den Stiefelschlaufen (die Cowboys wieder),\ndaher spricht man im Amerikanischen auch von der “Boostrapping-Methode”.Diese “Pseudo-Stichproben” oder “Bootstrapping-Stichproben” sind aber recht einfach zu gewinnen..\nGegeben sei Stichprobe der Größe \\(n\\):Ziehe mit Zurücklegen (ZmZ) aus der Stichprobe \\(n\\) BeobachtungenFertig ist die Bootstrapping-Stichprobe.Abb. 10.1 verdeutlicht das Prinzip des ZMZ, d.h. des Bootstrappings.\nWie man sieht, sind die Bootstrap-Stichproben (rechts) vom gleichen Umfang \\(n\\)\nwie die Originalstichprobe (links).\nAllerdins kommen nicht alle Fälle (der Regel) den “Boostrap-Beutel” (bag),\nsondern einige Fälle werden oft mehrfach gezogen, dass\neinige Fälle nicht gezogen werden (bag).\nFigure 10.1: Bootstrapping: Der Topf links symbolisiert die Original-Stichprobe, aus der wir hier mehrere ZMZ-Stichproben ziehen (Rechts), dargestellt mit ‘bag’\nMan kann zeigen, dass ca. 2/3 der Fälle gezogen werden,\nbzw. ca. 1/3 nicht gezogen werden. Die nicht gezogenen Fälle nennt man auch bag (OOB).Für die Entwicklung des Bootstrapping wurde der Autor, Bradley Efron, im Jahr 2018\nmit dem internationalen Preis für Statistik ausgezeichnet;“statistics offers magic pill quantitative scientific investigations, bootstrap best statistical pain reliever ever produced,” says Xiao-Li Meng, Whipple V. N. Jones Professor Statistics Harvard University.“","code":""},{"path":"ensemble-lerner.html","id":"bagging-algorithmus","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.7 Bagging-Algorithmus","text":"Bagging, die Kurzform für Bootstrap-Aggregation ist wenig mehr als die Umsetzung des Boostrappings.Der Algorithmus von Bagging kann beschrieben werden:Wähle \\(B\\), die Anzahl der Boostrap-Stichproben und damit auch Anzahl der Submodelle (Lerner)Ziehe \\(B\\) Boostrap-StichprobenBerechne das Modell \\(\\hat{f}^{*b}\\) für jede der \\(B\\) Stichproben (typischerweise ein einfacher Baum)Schicke die Test-Daten durch jedes Sub-ModellAggregiere ihre Vorhersage zu einem Wert (Modus bzw. Mittelwert) pro Fall aus dem Test-Sample, zu \\(\\hat{f}_{\\text{bag}}\\)Anders gesagt:\\[\\hat{f}_{\\text{bag}} = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{f}^{*b}\\]Der Bagging-Algorithmus ist Abbildung 10.2 dargestellt.\nFigure 10.2: Bagging schematisch illustriert\nDie Anzahl der Bäume (allgemeiner: Submodelle) \\(B\\) ist häufig im oberen drei- oder niedrigem vierstelligen\nBereich, z.B. \\(B=1000\\).\nEine gute Nachricht ist, dass Bagging nicht überanpasst, wenn \\(B\\) groß wird.","code":""},{"path":"ensemble-lerner.html","id":"variablenrelevanz","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.7.1 Variablenrelevanz","text":"Man kann die Relevanz der Prädiktoren einem Bagging-Modell auf mehrere Arten schätzen.\nEin Weg (bei numerischer Prädiktion) ist, dass man die RSS-Verringerung, die durch Aufteilung anhand eines Prädiktors\nerzeugt wird, mittelt über alle beteiligten Bäume (Modelle).\nBei Klassifikation kann man die analog die Reduktion des Gini-Wertes über alle Bäume mitteln\nund als Schätzwert für die Relevanz des Prädiktors heranziehen.","code":""},{"path":"ensemble-lerner.html","id":"out-of-bag-vorhersagen","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.7.2 Out of Bag Vorhersagen","text":"Da nicht alle Fälle der Stichprobe das Modell einfließen (sondern nur ca. 2/3),\nkann der Rest der Fälle zur Vorhersage genutzt werden.\nBagging erzeugt sozusagen innerhalb der Stichprobe selbständig ein Train- und ein Test-Sample.\nMan spricht von --Bag-Schätzung (OOB-Schätzung).\nDer OOB-Fehler (z.B. MSE bei numerischen Modellen und Genauigkeit bei nominalen)\nist eine valide Schätzung des typischen Test-Sample-Fehlers.Hat man aber Tuningparameter, wird man dennoch auf die typische Train-Test-Aufteilung\nzurückgreifen, um Overfitting durch das Ausprobieren der Tuning-Kandidaten zu vermeiden\n(sonst zu Zufallstreffern führen würde bei genügend vielen Modellkandidaten).","code":""},{"path":"ensemble-lerner.html","id":"random-forests","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.8 Random Forests","text":"Random Forests (“Zufallswälder”) sind eine Weiterentwicklung von Bagging-Modellen.\nSie sind Bagging-Modelle, aber haben noch ein Ass im Ärmel:\nUnd zwar wird jedem Slit (Astgabel, Aufteilung) nur eine Zufallsauswahl \\(m\\) Prädiktoren berücksichtigt.\nDas hört sich verrückt : “Wie, mit weniger Prädiktoren soll eine bessere Vorhersage erreicht werden?!”\nJa, genau ist es!\nNehmen Sie , es gibt im Datensatz einen sehr starken und ein paar mittelstarke Prädiktoren;\nder Rest der Prädiktoren ist wenig relevant.\nWenn Sie jetzt viele “gebootstrapte”7 ziehen,\nwerden diese Bäume sehr ähnlich sein: Der stärkste Prädiktor steht vermutlich immer ob der Wurzel,\ndann kommen die mittelstarken Prädiktoren.\nJeder zusätzliche Baum trägt dann wenig neue Information bei.\nAnders gesagt: Die Vorhersagen der Bäume sind dann sehr ähnlich bzw. hoch korreliert.\nBildet man den Mittelwert von hoch korrelierten Variablen,\nverringert sich leider die Varianzu nur wenig im Vergleich zu nicht oder gering korrelierten Variablen (James et al. 2021).\nDadurch dass Random Forests nur \\(m\\) der \\(p\\) Prädiktoren pro Split zulassen,\nwerden die Bäume unterschiedlicher. Wir “dekorrelieren” die Bäume.\nBildet man den Mittelwert von gering(er) korrelierten Variablen,\nist die Varianzreduktion höher - und die Vohersage genauer.\nLässt man pro Split \\(m=p\\) Prädiktoren zu,\ngleicht Bagging dem Random Forest.\nDie Anzahl \\(m\\) der erlaubten Prädiktoren werden als Zufallstichprobe aus den \\(p\\)\nPrädiktoren des Datensatzes gezogen (ohne Zurücklegen).\n\\(m\\) ist ein Tuningparameter; \\(m=\\sqrt(p)\\) ist ein beliebter Startwert.\nden meisten Implementationen wird \\(m\\) mit mtry bezeichnet (auch Tidymodels).Der Random-Forest-Algorithmus ist Abb. 10.3 illustriert.\nFigure 10.3: Zufallswälder durch Ziehen mit Zurücklegen (zmz) und Ziehen ohne Zurücklegen (ZoZ)\nAbb. 10.4 vergleicht die Test-Sample-Vorhersagegüte von Bagging- und Random-Forest-Algorithmen aus James et al. (2021).\ndiesem Fall ist die Vorhersagegüte deutlich unter der OOB-Güte; laut James et al. (2021) ist dies hier “Zufall”.\nFigure 10.4: Test-Sample-Vorhersagegüte von Bagging- und Random-Forest-Algorithmen\nDen Effekt von \\(m\\) (Anzahl der Prädiktoren pro Split) ist Abb. 10.5 dargestellt (James et al. 2021).\nMan erkennt, dass der Zusatznutzen zusätzlichen Bäumen, \\(B\\), sich abschwächt.\n\\(m=\\sqrt{p}\\) schneidet wie erwartet besten ab.\nFigure 10.5: Test-Sample-Vorhersagegüte von Bagging- und Random-Forest-Algorithmen\n","code":""},{"path":"ensemble-lerner.html","id":"boosting","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.9 Boosting","text":"Im Unterschied zu Bagging und Random-Forest-Modellen wird beim Boosting der “Wald”\nsequenziell entwickelt, nicht gleichzeitig wie bei den anderen vorgestellten “Wald-Modellen”.\nDie zwei bekanntesten Implementierungen bzw. Algorithmus-Varianten sind AdaBoost und XGBoost.\nGerade XGBoost hat den Ruf, hervorragende Vorhersagen zu leisten.\nAuf Kaggle gewinnt nach einigen Berichten oft XGBoost.\nNur neuronale Netze schneiden besser ab.\nRandom-Forest-Modelle kommen nach diesem Bereich auf Platz 3.\nAllerdings benötigen neuronale Netzen oft riesige Stichprobengrößen\nund bei spielen ihre Nuanciertheit vor allem bei komplexen Daten wie Bildern oder Sprache aus.\nFür “rechteckige” Daten (also aus einfachen, normalen Tabellen) wird ein baumbasiertes Modell oft besser abschneiden.Die Idee des Boosting ist es, anschaulich gesprochen,\naus Fehlern zu lernen: Fitte einen Baum, schau welche Fälle er schlecht vorhergesagt hat,\nkonzentriere dich beim nächsten Baum auf diese Fälle und weiter.Wie andere Ensemble-Methoden auch kann Boosting theoretisch für beliebige Algorithmen eingesetzt werden.\nEs macht aber Sinn, Boosting bei “schwachen Lernern” einzusetzen.\nTypisches Beispiel ist ein einfacher Baum; “einfach” soll heißen, der Baum hat nur wenig Gabeln oder vielleicht sogar nur eine einzige.\nDann spricht man von einem Stumpf, intuitiv gut passt.","code":""},{"path":"ensemble-lerner.html","id":"adaboost","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.9.1 AdaBoost","text":"Der AdaBoost-Algorithmus funktioniert, einfach dargestellt, wie folgt.\nZuerst hat jeder Fall \\(\\) im Datensatz des gleiche Gewicht.\nDie erste (und alle weiteren) Stichprobe werden per Bootstrapping aus dem\nDatensatz gezogen. Dabei ist die Wahrscheinlichkeit, gezogen zu werden,\nproportional zum Gewicht des Falles, \\(w_i\\). Da im ersten Durchgang die Gewichte identisch sind,\nhaben zunächst alle Fälle die gleiche Wahrscheinlichkeit, das Bootstrap-Sample gezogen zu werden.\nDie Bäume bei AdaBoost sind eigentlich nur “Stümpfe”: Sie bestehen aus einem einzelnen Split, s. Abb. 10.6.\nFigure 10.6: Ein Baumstumpf bei AdaBoost\nNach Berechnung des Baumes und der Vorhersagen werden die richtig klassifizierten Fälle heruntergewichtet\nund die falsch klassifizierten Fälle hoch gewichtet, also stärker gewichtet (bleiben wir aus Gründen der Einfachheit zunächst bei der Klassifikation).\nDieses Vorgehen folgt dem Gedanken, dass man sich seine Fehler genauer anschauen muss,\ndie falsch klassifizierten Fälle sozusagen mehr Aufmerksamkeit bedürfen.\nDas nächste (zweite) Modell zieht ein weiteres Bootstrap-Sample.\nJetzt sind allerdings die Gewichte schon angepasst,\ndass mehr Fälle, die im vorherigen Modell falsch klassifiziert wurden, den neuen (zweiten)\nBaum gezogen werden.\nDas neue Modell hat also bessere Chancen,\ndie Aspekte, die das Vorgänger-Modell übersah zu korrigieren bzw. zu lernen.\nJetzt haben wir zwei Modelle. Die können wir aggregieren, genau\nwie beim Bagging: Der Modus der Vorhersage über alle (beide) Bäume hinwig ist\ndann die Vorhersage für einen bestimmten Fall (“Fall” und “Beobachtung” sind stets synonym für \\(y_i\\) zu verstehen).\nwiederholt sich das Vorgehen für \\(B\\) Bäume:\nDie Gewichte werden angepasst, das neue Modell wird berechnet,\nalle Modelle machen ihre Vorhersagen, per Mehrheitsbeschluss - mit gewichteten Modellen - wird die Vorhersage bestimmt pro Fall.\nIrgendwann erreichen wir die vorab definierte Maximalzahl Bäumen, \\(B\\), und das Modell kommt zu einem Ende.Da das Modell die Fehler seiner Vorgänger reduziert,\nwird der Bias im Gesamtmodell verringert.\nDa wir gleichzeitig auch Bagging vornehmen,\nwird aber die Varianz auch verringert.\nKlingt schon wieder (fast) nach -Good---True!Das Gewicht \\(w_i^b\\) des \\(\\)ten Falls im \\(b\\)ten Modell von \\(B\\) berechnet sich wie folgt (Rhys 2020):\\[ w_i^b = \\begin{cases}\nw_i^{b-1} \\cdot e^{-\\text{model weight}} \\qquad \\text{wenn korrekt klassifiziert} \\\\\nw_i^{b-1} \\cdot e^{\\text{model weight}} \\qquad \\text{wenn inkorrekt klassifiziert} \\\\\n\\end{cases}\\]Das Modellgewicht \\(mw\\) berechnet sich dabei (Rhys 2020):\\[mw_b = 0.5 \\cdot log\\left( \\frac{1-p(\\text{inkorrect})}{p(\\text{korrekt})} \\right) \\propto \\mathcal{L(p)} \\]\\(p(\\cdot)\\) ist der Anteil (Wahrscheinlichkeit) einer Vorhersage.Das Modellgewicht ist ein Faktor, der schlechtere Modelle bestraft.\nDas folgt dem Gedanken,\ndass schlechteren Modellen weniger Gehört geschenkt werden soll,\naber schlecht klassifizierten Fällen mehr Gehör.Das Vorgehen von AdaBoost ist Abb. 10.7 illustriert.\nFigure 10.7: AdaBoost illustriert\n","code":""},{"path":"ensemble-lerner.html","id":"xgboost","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.9.2 XGBoost","text":"XGBoost ist ein Gradientenverfahren,\neine Methode also, die die Richtung des parziellen Ableitungskoeffizienten als Optimierungskriterium heranzieht.\nXGBoost ist ähnlich zu AdaBoost,\nnur dass Residuen modelliert werden, nicht \\(y\\).\nDie Vorhersagefehler von \\(\\hat{f}^b\\) werden die Zielvariable von \\(\\hat{f}^{b+1}\\).\nEin Residuum ist der Vorhersagefehler, bei metrischen Modellen etwa RMSE,\noder schlicht \\(r_i = y_i - \\hat{y}_i\\).\nDetails finden sich z.B. hier, dem Original XGBoost-Paper (Chen Guestrin 2016).Die hohe Vorhersagegüte von Boosting-Modellen ist exemplarisch Abb. 10.8 dargestellt (James et al. 2021, S. 358ff).\nAllerdings verwenden die Autoren Friedmans (2001) Gradient Boosting Machine, eine weitere Variante des Boosting .\nFigure 10.8: Vorhersagegüte von Boosting und Random Forest\n","code":""},{"path":"ensemble-lerner.html","id":"tidymodels-3","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10 Tidymodels","text":"","code":""},{"path":"ensemble-lerner.html","id":"datensatz-churn","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.1 Datensatz Churn","text":"Wir betrachten einen Datensatz zur Kundenabwanderung (Churn) aus dieser Quelle.Ein Blick die Daten:","code":""},{"path":"ensemble-lerner.html","id":"data-splitting-und-cv","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.2 Data Splitting und CV","text":"Das Kreuzvalidieren (CV) fassen wir auch unter diesen Punkt.","code":""},{"path":"ensemble-lerner.html","id":"feature-engineering","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.3 Feature Engineering","text":"Hier definieren wir zwei Rezepte.\nGleichzeitig verändern wir die Prädiktoren (normalisieren, dummysieren, …).\nDas nennt man auch Feature Engineering.step_YeoJohnson() reduziert Schiefe der Verteilung.","code":""},{"path":"ensemble-lerner.html","id":"modelle","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.4 Modelle","text":"","code":""},{"path":"ensemble-lerner.html","id":"workflows-1","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.5 Workflows","text":"Wir definieren ein Workflow-Set:Infos zu workflow_set bekommt man wie gewohnt mit ?workflow_set.Im Standard werden alle Rezepte und Modelle miteinander kombiniert (cross = TRUE),\nalso preproc * models Modelle gefittet.","code":""},{"path":"ensemble-lerner.html","id":"modelle-berechnen-mit-tuning-einzeln","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.6 Modelle berechnen mit Tuning, einzeln","text":"Wir könnten jetzt jedes Modell einzeln tunen, wenn wir wollen.","code":""},{"path":"ensemble-lerner.html","id":"baum","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.6.1 Baum","text":"Im Standard werden 10 Modellkandidaten getuned.Schauen wir uns das Objekt etwas näher :30 Zeilen: 3 Gütemetriken (Sens, Spec, ROC AUC) mit je 10 Werten (Submodellen),\ngibt 30 Koeffizienten.Für jeden der 5 Faltungen haben wir also 10 Submodelle.Welches Modell ist das beste?Aha, das sind die fünf besten Modelle, bzw. ihre Tuningparameter,\nihre mittlere Güte zusammen mit dem Standardfehler.","code":"## 16.001 sec elapsed## # Tuning results\n## # 5-fold cross-validation \n## # A tibble: 5 × 4\n##   splits             id    .metrics          .notes          \n##   <list>             <chr> <list>            <list>          \n## 1 <split [2393/599]> Fold1 <tibble [30 × 7]> <tibble [0 × 3]>\n## 2 <split [2393/599]> Fold2 <tibble [30 × 7]> <tibble [0 × 3]>\n## 3 <split [2394/598]> Fold3 <tibble [30 × 7]> <tibble [0 × 3]>\n## 4 <split [2394/598]> Fold4 <tibble [30 × 7]> <tibble [0 × 3]>\n## 5 <split [2394/598]> Fold5 <tibble [30 × 7]> <tibble [0 × 3]>## # A tibble: 30 × 7\n##    cost_complexity tree_depth min_n .metric .estimator .estimate .config        \n##              <dbl>      <int> <int> <chr>   <chr>          <dbl> <chr>          \n##  1        2.96e- 2          6     6 sens    binary         0.659 Preprocessor1_…\n##  2        2.96e- 2          6     6 spec    binary         0.955 Preprocessor1_…\n##  3        2.96e- 2          6     6 roc_auc binary         0.857 Preprocessor1_…\n##  4        1.63e- 3         14    39 sens    binary         0.799 Preprocessor1_…\n##  5        1.63e- 3         14    39 spec    binary         0.913 Preprocessor1_…\n##  6        1.63e- 3         14    39 roc_auc binary         0.910 Preprocessor1_…\n##  7        6.08e- 6          1    20 sens    binary         0.860 Preprocessor1_…\n##  8        6.08e- 6          1    20 spec    binary         0.642 Preprocessor1_…\n##  9        6.08e- 6          1    20 roc_auc binary         0.751 Preprocessor1_…\n## 10        1.34e-10          3    26 sens    binary         0.780 Preprocessor1_…\n## # … with 20 more rows## # A tibble: 5 × 9\n##   cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n##             <dbl>      <int> <int> <chr>   <chr>      <dbl> <int>   <dbl>\n## 1        2.15e- 7          9    22 roc_auc binary     0.923     5 0.00444\n## 2        8.84e-10          7    33 roc_auc binary     0.923     5 0.00198\n## 3        1.53e- 3          9    31 roc_auc binary     0.921     5 0.00266\n## 4        1.12e- 6         12    14 roc_auc binary     0.916     5 0.00458\n## 5        1.63e- 3         14    39 roc_auc binary     0.908     5 0.00454\n## # … with 1 more variable: .config <chr>"},{"path":"ensemble-lerner.html","id":"rf","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.6.2 RF","text":"für Tuningparameter hat den der Algorithmus bzw. seine Implementierung?Da die Berechnung einiges Zeit braucht,\nkann man das (schon früher einmal berechnete) Ergebnisobjekt\nvon der Festplatte lesen (sofern es existiert).\nAnsonsten berechnet man neu:\nkann man das berechnete Objekt abspeichern auf Festplatte,\num künftig Zeit zu sparen:","code":"## Information for `rand_forest`\n##  modes: unknown, classification, regression, censored regression \n## \n##  engines: \n##    classification: randomForest, ranger, spark\n##    regression:     randomForest, ranger, spark\n## \n##  arguments: \n##    ranger:       \n##       mtry  --> mtry\n##       trees --> num.trees\n##       min_n --> min.node.size\n##    randomForest: \n##       mtry  --> mtry\n##       trees --> ntree\n##       min_n --> nodesize\n##    spark:        \n##       mtry  --> feature_subset_strategy\n##       trees --> num_trees\n##       min_n --> min_instances_per_node\n## \n##  fit modules:\n##          engine           mode\n##          ranger classification\n##          ranger     regression\n##    randomForest classification\n##    randomForest     regression\n##           spark classification\n##           spark     regression\n## \n##  prediction modules:\n##              mode       engine                    methods\n##    classification randomForest           class, prob, raw\n##    classification       ranger class, conf_int, prob, raw\n##    classification        spark                class, prob\n##        regression randomForest               numeric, raw\n##        regression       ranger     conf_int, numeric, raw\n##        regression        spark                    numeric## # Tuning results\n## # 5-fold cross-validation \n## # A tibble: 5 × 4\n##   splits             id    .metrics          .notes          \n##   <list>             <chr> <list>            <list>          \n## 1 <split [2393/599]> Fold1 <tibble [30 × 7]> <tibble [0 × 3]>\n## 2 <split [2393/599]> Fold2 <tibble [30 × 7]> <tibble [0 × 3]>\n## 3 <split [2394/598]> Fold3 <tibble [30 × 7]> <tibble [0 × 3]>\n## 4 <split [2394/598]> Fold4 <tibble [30 × 7]> <tibble [0 × 3]>\n## 5 <split [2394/598]> Fold5 <tibble [30 × 7]> <tibble [0 × 3]>## # A tibble: 5 × 9\n##    mtry trees min_n .metric .estimator  mean     n std_err .config              \n##   <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n## 1     6  1686    18 roc_auc binary     0.958     5 0.00330 Preprocessor1_Model03\n## 2     5   747    34 roc_auc binary     0.958     5 0.00324 Preprocessor1_Model10\n## 3    10   818    22 roc_auc binary     0.956     5 0.00378 Preprocessor1_Model01\n## 4     8   342     2 roc_auc binary     0.955     5 0.00361 Preprocessor1_Model09\n## 5    13  1184    25 roc_auc binary     0.954     5 0.00423 Preprocessor1_Model08"},{"path":"ensemble-lerner.html","id":"xgboost-1","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.6.3 XGBoost","text":"Wieder auf Festplatte speichern:Und weiter.","code":""},{"path":"ensemble-lerner.html","id":"workflow-set-tunen","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.7 Workflow-Set tunen","text":"Da die Berechnung schon etwas Zeit braucht,\nmacht es Sinn, das Modell (bzw. das Ergebnisobjekt) auf Festplatte zu speichern:Achtung Dieser Schritt ist gefährlich:\nWenn Sie Ihr Rezept und Fit-Objekt ändenr, kriegt das Ihre Festplatte nicht unbedingt\nmit. Sie könnten also unbemerkt mit dem alten Objekt von Ihrer Festplatte weiterarbeiten,\nohne durch eine Fehlermeldung gewarnt zu werden.Entsprechend kann man das Modellobjekt wieder importieren, wenn einmal abgespeichert:","code":""},{"path":"ensemble-lerner.html","id":"ergebnisse-im-train-sest","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.8 Ergebnisse im Train-Sest","text":"Hier ist die Rangfolge der Modelle, geordnet nach mittlerem ROC AUC:","code":"## # A tibble: 122 × 9\n##    wflow_id    .config      .metric  mean std_err     n preprocessor model  rank\n##    <chr>       <chr>        <chr>   <dbl>   <dbl> <int> <chr>        <chr> <int>\n##  1 rec2_boost1 Preprocesso… roc_auc 0.963 0.00104     5 recipe       boos…     1\n##  2 rec1_boost1 Preprocesso… roc_auc 0.963 0.00104     5 recipe       boos…     2\n##  3 rec2_boost1 Preprocesso… roc_auc 0.961 0.00106     5 recipe       boos…     3\n##  4 rec1_boost1 Preprocesso… roc_auc 0.961 0.00106     5 recipe       boos…     4\n##  5 rec2_glm1   Preprocesso… roc_auc 0.961 0.00272     5 recipe       logi…     5\n##  6 rec1_boost1 Preprocesso… roc_auc 0.961 0.00102     5 recipe       boos…     6\n##  7 rec2_boost1 Preprocesso… roc_auc 0.961 0.00102     5 recipe       boos…     7\n##  8 rec2_boost1 Preprocesso… roc_auc 0.960 0.00120     5 recipe       boos…     8\n##  9 rec1_boost1 Preprocesso… roc_auc 0.960 0.00120     5 recipe       boos…     9\n## 10 rec1_rf1    Preprocesso… roc_auc 0.960 0.00278     5 recipe       rand…    10\n## # … with 112 more rows"},{"path":"ensemble-lerner.html","id":"bestes-modell","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.9 Bestes Modell","text":"Und hier nur der beste Kandidat pro Algorithmus:Boosting hat - knapp - besten abgeschnitten.\nAllerdings sind Random Forest und die schlichte, einfache logistische Regression auch fast genau gut.\nDas wäre ein Grund für das einfachste Modell, das GLM, zu votieren.\nZumal die Interpretierbarkeit besten ist.\nAlternativ könnte man sich für das Boosting-Modell aussprechen.Man kann sich das beste Submodell auch von Tidymodels bestimmen lassen.\nDas scheint aber (noch) nicht für ein Workflow-Set zu funktionieren,\nsondern nur für das Ergebnisobjekt von tune_grid.rf_fit1 haben wir mit tune_grid() berechnet;\nmit diesem Modell kann select_best() arbeiten:Aber wir können uns händisch behelfen.Schauen wir uns mal die Metriken (Vorhersagegüte) :rec1_boost1 scheint das beste Modell zu sein.","code":"## Error in `is_metric_maximize()`:\n## ! Please check the value of `metric`.## # A tibble: 1 × 4\n##    mtry trees min_n .config              \n##   <int> <int> <int> <chr>                \n## 1     6  1686    18 Preprocessor1_Model03## # A tibble: 122 × 9\n##    wflow_id    .config      preproc model .metric .estimator  mean     n std_err\n##    <chr>       <chr>        <chr>   <chr> <chr>   <chr>      <dbl> <int>   <dbl>\n##  1 rec1_boost1 Preprocesso… recipe  boos… roc_auc binary     0.963     5 0.00104\n##  2 rec2_boost1 Preprocesso… recipe  boos… roc_auc binary     0.963     5 0.00104\n##  3 rec1_boost1 Preprocesso… recipe  boos… roc_auc binary     0.961     5 0.00106\n##  4 rec2_boost1 Preprocesso… recipe  boos… roc_auc binary     0.961     5 0.00106\n##  5 rec2_glm1   Preprocesso… recipe  logi… roc_auc binary     0.961     5 0.00272\n##  6 rec1_boost1 Preprocesso… recipe  boos… roc_auc binary     0.961     5 0.00102\n##  7 rec2_boost1 Preprocesso… recipe  boos… roc_auc binary     0.961     5 0.00102\n##  8 rec1_boost1 Preprocesso… recipe  boos… roc_auc binary     0.960     5 0.00120\n##  9 rec2_boost1 Preprocesso… recipe  boos… roc_auc binary     0.960     5 0.00120\n## 10 rec1_rf1    Preprocesso… recipe  rand… roc_auc binary     0.960     5 0.00278\n## # … with 112 more rows## # A tibble: 1 × 4\n##    mtry trees min_n .config              \n##   <int> <int> <int> <chr>                \n## 1     6    80    21 Preprocessor1_Model05"},{"path":"ensemble-lerner.html","id":"finalisisieren","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.10 Finalisisieren","text":"Wir entscheiden uns mal für das Boosting-Modell, rec1_boost1.\nDiesen Workflow, finalisierter Form,\nbrauchen wir für den “final Fit”.\nFinalisierte Form heißt:Schritt 1: Nimm den passenden Workflow, hier rec1 und boost1; das hatte uns oben rank_results() verraten.Schritt 2: Update (Finalisiere) ihn mit den besten Tuningparameter-WertenJetzt finalisieren wir den Workflow,\nd.h. wir setzen die Parameterwerte des besten Submodells ein:","code":"## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: boost_tree()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 2 Recipe Steps\n## \n## • step_normalize()\n## • step_dummy()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Boosted Tree Model Specification (classification)\n## \n## Main Arguments:\n##   mtry = tune()\n##   trees = tune()\n##   min_n = tune()\n## \n## Engine-Specific Arguments:\n##   nthreads = parallel::detectCores()\n## \n## Computational engine: xgboost## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: boost_tree()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 2 Recipe Steps\n## \n## • step_normalize()\n## • step_dummy()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Boosted Tree Model Specification (classification)\n## \n## Main Arguments:\n##   mtry = 6\n##   trees = 80\n##   min_n = 21\n## \n## Engine-Specific Arguments:\n##   nthreads = parallel::detectCores()\n## \n## Computational engine: xgboost"},{"path":"ensemble-lerner.html","id":"last-fit","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.11 Last Fit","text":"","code":"## # Resampling results\n## # Manual resampling \n## # A tibble: 1 × 6\n##   splits             id               .metrics .notes   .predictions .workflow \n##   <list>             <chr>            <list>   <list>   <list>       <list>    \n## 1 <split [2992/998]> train/test split <tibble> <tibble> <tibble>     <workflow>## # A tibble: 2 × 4\n##   .metric  .estimator .estimate .config             \n##   <chr>    <chr>          <dbl> <chr>               \n## 1 accuracy binary         0.895 Preprocessor1_Model1\n## 2 roc_auc  binary         0.959 Preprocessor1_Model1"},{"path":"ensemble-lerner.html","id":"variablenrelevanz-1","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.12 Variablenrelevanz","text":"Um die Variablenrelevanz zu plotten,\nmüssen wir aus dem Tidymodels-Ergebnisobjekt\ndas eigentliche Ergebnisobjekt herausziehen, von der R-Funktion, die die eigentliche\nBerechnung durchführt,\ndas wäre glm() bei einer logistischen Regression oder xgboost::xgb.train() bei\nXGBoost:Dieses Objekt übergeben wir dann vip:","code":"## parsnip model object\n## \n## ##### xgb.Booster\n## raw: 112.2 Kb \n## call:\n##   xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n##     colsample_bytree = 1, colsample_bynode = 0.285714285714286, \n##     min_child_weight = 21L, subsample = 1, objective = \"binary:logistic\"), \n##     data = x$data, nrounds = 80L, watchlist = x$watchlist, verbose = 0, \n##     nthreads = 8L, nthread = 1)\n## params (as set within xgb.train):\n##   eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"0.285714285714286\", min_child_weight = \"21\", subsample = \"1\", objective = \"binary:logistic\", nthreads = \"8\", nthread = \"1\", validate_parameters = \"TRUE\"\n## xgb.attributes:\n##   niter\n## callbacks:\n##   cb.evaluation.log()\n## # of features: 21 \n## niter: 80\n## nfeatures : 21 \n## evaluation_log:\n##     iter training_logloss\n##        1         0.561902\n##        2         0.486567\n## ---                      \n##       79         0.187956\n##       80         0.187517"},{"path":"ensemble-lerner.html","id":"roc-curve","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.10.13 ROC-Curve","text":"Eine ROC-Kurve berechnet Sensitivität und Spezifität aus den Vorhersagen,\nbzw. aus dem Vergleich von Vorhersagen und wahrem Wert (d.h. der beobachtete Wert).Ziehen wir also zuerst die Vorhersagen heraus:Praktischerweise werden die “wahren Werte” (also die beobachtaten Werte), canceled_service,\nausch angegeben.Dann berechnen wir die roc_curve und autoplotten sie.","code":"## # A tibble: 998 × 7\n##    id              .pred_yes .pred_no  .row .pred_class canceled_service .config\n##    <chr>               <dbl>    <dbl> <int> <fct>       <fct>            <chr>  \n##  1 train/test spl…     0.546 0.454        2 yes         yes              Prepro…\n##  2 train/test spl…     0.925 0.0749       9 yes         yes              Prepro…\n##  3 train/test spl…     0.656 0.344       13 yes         yes              Prepro…\n##  4 train/test spl…     0.918 0.0823      15 yes         yes              Prepro…\n##  5 train/test spl…     0.866 0.134       20 yes         yes              Prepro…\n##  6 train/test spl…     0.903 0.0974      21 yes         yes              Prepro…\n##  7 train/test spl…     0.997 0.00348     22 yes         yes              Prepro…\n##  8 train/test spl…     0.994 0.00595     23 yes         yes              Prepro…\n##  9 train/test spl…     0.979 0.0207      24 yes         yes              Prepro…\n## 10 train/test spl…     0.999 0.000919    26 yes         yes              Prepro…\n## # … with 988 more rows"},{"path":"ensemble-lerner.html","id":"aufgaben-7","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.11 Aufgaben","text":"","code":""},{"path":"ensemble-lerner.html","id":"aufgaben-8","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.12 Aufgaben","text":"Aufgaben zu Tidymodels, PDFAufgaben zu Tidymodels, HTML","code":""},{"path":"ensemble-lerner.html","id":"vertiefung-4","chapter":"Kapitel 10 Ensemble Lerner","heading":"10.13 Vertiefung","text":"Einfache Durchführung eines Modellierung mit XGBoostFallstudie Oregon SchoolsFallstudie ChurnFallstudie IkeaFallstudie Wasserquellen Sierra LeoneFallstudie Bäume San FranciscoFallstudie VulkanausbrücheFallstudie Brettspiele mit XGBoost","code":""},{"path":"regularisierte-modelle.html","id":"regularisierte-modelle","chapter":"Kapitel 11 Regularisierte Modelle","heading":"Kapitel 11 Regularisierte Modelle","text":"","code":""},{"path":"regularisierte-modelle.html","id":"lernsteuerung-8","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.1 Lernsteuerung","text":"","code":""},{"path":"regularisierte-modelle.html","id":"lernziele-9","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.1.1 Lernziele","text":"Sie können Algorithmen für regularisierte lineare Modell erklären, d.h. Lasso- und Ridge-RegressionSie wissen, anhand welche Tuningparamter man Overfitting bei diesen Algorithmen begrenzen kannSie können diese Verfahren R berechnen","code":""},{"path":"regularisierte-modelle.html","id":"literatur-9","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.1.2 Literatur","text":"Rhys, Kap. 11","code":""},{"path":"regularisierte-modelle.html","id":"hinweise-4","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.1.3 Hinweise","text":"Rhys und ISLR sind eine gute Quelle zum Einstieg das Thema","code":""},{"path":"regularisierte-modelle.html","id":"vorbereitung-8","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.2 Vorbereitung","text":"diesem Kapitel werden folgende R-Pakete benötigt:","code":"\nlibrary(tidymodels)\nlibrary(tictoc)  # Zeitmessung"},{"path":"regularisierte-modelle.html","id":"regularisierung","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.3 Regularisierung","text":"","code":""},{"path":"regularisierte-modelle.html","id":"was-ist-regularisierung","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.3.1 Was ist Regularisierung?","text":"Regularisieren verweist auf “regulär”;\nlaut Duden bedeutet das Wort viel wie “den Regeln, Bestimmungen,\nVorschriften entsprechend; vorschriftsmäßig, ordnungsgemäß, richtig” oder “üblich”.Im Englischen spricht man auch von “penalized models”, “bestrafte Modell” und von “shrinkage”,\nvon “Schrumpfung” im Zusammenhang mit dieer Art von Modellen.Regularisierung ist ein Metalalgorithmus, also ein Verfahren, als zweiter Schritt “auf” verschiedene\nModelle angewendet werden kann - zumeist aber auf lineare Modelle, worauf\nwir uns im Folgenden konzentrieren.Das Ziel von Regularisierung ist es, Overfitting zu vermeiden,\ndem die Komplexität eines Modells reduziert wird.\nDer Effekt von Regularisierung ist,\ndass die Varianz der Modelle verringert wird und damit das Overfitting.\nDer Preis ist, dass der Bias erhöht wird,\naber oft geht die Rechnung auf, dass der Gewinn größer ist als der Verlust.Im Kontext von linearen Modellen bedeutet das,\ndass die Koeffizienten (\\(\\beta\\)s) im Betrag verringert werden durch Regularisierung,\nalso Richtung Null “geschrumpft” werden.Dem liegt die Idee zugrunde,\ndass extreme Werte den Koeffizienten vermutlich nicht “echt”, sondern durch Rauschen\nfälschlich vorgegaukelt werden.Die bekanntesten Vertreter dieser Modellart sind Ridge Regression, \\(L2\\), das Lasso, \\(L1\\), sowie Elastic Net.","code":""},{"path":"regularisierte-modelle.html","id":"ähnliche-verfahren","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.3.2 Ähnliche Verfahren","text":"Ein ähnliches\nZiel wie der Regulaisierung liegt dem Pruning zugrunde,\ndem nachträglichen Beschneiden von Entscheidungsbäumen.\nbeiden Fällen wird die Komplexität des Modells verringert,\nund damit die Varianz auf Kosten eines möglichen Anstiegs der Verzerrung (Bias)\ndes Modells. Unterm Strich hofft man, dass der Gewinn die Kosten übersteigt\nund somit der Fit im Test-Sample besser wird.Eine Andere Art der Regularisierung wird durch die Verwendung von Bayes-Modellen erreicht:\nSetzt man einen konservativen Prior, etwa mit Mittelwert Null und kleiner Streuung,\nwerden die Posteriori-Koeffizienten gegen Null hin geschrumpft werden.Mit Mehrebenen-Modellen (Multi Level Models) lässt sich ein ähnlicher Effekt erreichen.","code":""},{"path":"regularisierte-modelle.html","id":"normale-regression-ols","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.3.3 Normale Regression (OLS)","text":"Man kann sich fragen, warum sollte man der normalen Least-Square-Regression\n(OLS: Ordinary Least Square) weiter herumbasteln wollen,\nschließlich garantiert das Gauss-Markov-Theorem, dass eine lineare Regression\nden besten linearen unverzerrten Schätzwert (BLUE, best linear unbiased estimator) stellt,\nvorausgesetzt die Voraussetzungen der Regression sind erfüllt.Ja, die Schätzwerte (Vorhersagen) der Regression sind BLUE, schätzen also den wahren\nWert korrekt und maximal präzise. Das gilt (natürlich) nur, wenn die Voraussetzungen der Regression erfüllt\nsind, also vor allem, dass die Beziehung auch linear-additiv ist.Zur Erinnerung, mit OLS minimiert man man den quadrierten Fehler, \\(RSS\\), Residual Sum Square:\\[RSS = \\sum_{=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)\\]Man sucht also diejenigen Koeffizientenwerte \\(\\beta\\) (Argumente der Loss-Funktion RSS),\ndie RSS minimieren:\\[\\beta = \\underset {\\beta}{\\operatorname {arg\\,min(RSS)}}\\]Es handelt sich hier um Schätzwerte, die meist mit dem Hütchen \\(\\hat{\\beta}\\) ausgedrückt werden,\nhier aber zur einfacheren Notation weggelassen sind.Abb. 11.1 visualisiert die Optimierung mit OLS Quelle.\ngleicher Stelle findet sich\neine gute Darstellung zu den (mathematischen) Grundlagen der OLS-Regression.\nFigure 11.1: Visualisierung der Minimierung der RSS durch OLS\nÜbrigens nennt man Funktionen, die man minimiert mit Hilfe von Methoden des maschinellen Lernens\nmit dem Ziel die optimalen Koeffizienten (wie \\(\\beta\\)s) zu finden, auch Loss Functions (Kostenfunktion).Das Problem der Regression ist, dass die schöne Eigenschaft BLUE nur im Train-Sample, nicht (notwendig)\nim Test-Sample gilt.","code":""},{"path":"regularisierte-modelle.html","id":"ridge-regression-l2","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.4 Ridge Regression, L2","text":"","code":""},{"path":"regularisierte-modelle.html","id":"strafterm","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.4.1 Strafterm","text":"Ridge Regression ist sehr ähnlich zum OLS-Algorithmus,\nnur das ein “Strafterm aufgebrummt” wird, der \\(RSS\\) erhöht.Der Gesamtterm, der optimiert wird, \\(L_{L2}\\) (Loss Level 2) ist also\ndie Summe aus RSS und dem Strafterm:\\[L_{L2} = RSS + \\text{Strafterm}\\]Der Strafterm ist aufgebaut,\ndass (im Absolutbetrag) größere Koeffizienten mehr zum Fehler beitragen,\nalso eine Funktion der (quadrierten) Summe der Absolutwerte der Koeffizienten:\\[\\text{Strafterm} = \\lambda \\sum_{j=1}^p \\beta_j^2\\]Man nennt den L2-Strafterm auch L2-Norm8.Dabei ist \\(\\lambda\\) (lambda) ein Tuningparameter,\nder bestimmt, wie stark die Bestrafung ausfällt. Den Wert von \\(\\lambda\\) lassen wir durch\nTuning bestimmen, wobei \\(\\lambda \\\\mathbb{R}^+\\setminus\\{0\\}\\).\nEs gilt: Je größer lambda, desto stärker die Schrumpfung der Koeffizienten gegen Null,\nda der gesamte zu minimierende Term, \\(L_{L2}\\) entsprechend durch lambda vergrößert wird.Der Begriff “L2” beschreibt dass es sich um eine quadrierte Normierung handelt.Der Begriff “Norm” stammt aus der Vektoralgebra. Die L2-Norm eines Vektors \\(||v||\\) mit \\(k\\) Elementen ist definiert Quelle:\\[||v|| = \\left(|{v_1}|^2+ |{v_2}|^2+ |{v_i}|^2+ \\ldots + |{v_k}|^2 \\right)^{1/2} \\]\nwobei \\(|{v_i}|\\) den Absolutwert (Betrag) meint de Elements \\(v_i\\) meint.\nIm Falle von reellen Zahlen und Quadrierung braucht es hier die Absolutfunktion nicht.Im Falle von zwei Elementen vereinfacht sich obiger Ausdruck zu:\\[||v|| = \\sqrt{\\left({v_1}^2+ {v_2}^2\\right)} \\]Das ist nichts anderes als Pythagoras’ Gesetz im euklidischen Raum.Der Effekt von \\(\\lambda \\sum_{j=1}^p \\beta_j^2\\) ist wie gesagt, dass\ndie Koeffizienten Richtung Null geschrumpft werden. Wenn \\(\\lambda = 0\\),\nresultiert OLS.\nWenn \\(\\lambda \\rightarrow \\infty\\), werden alle Koeffizienten auf Null geschätzt werden,\nAbb. 11.2 verdeutlicht dies (James et al. 2021).\nFigure 11.2: Links: Regressionskoeffizienten als Funktion von lambda. Rechts: L2-Norm der Ridge-Regression im Verhältnis zur OLS-Regression\n","code":""},{"path":"regularisierte-modelle.html","id":"standardisierung","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.4.2 Standardisierung","text":"Die Straftermformel sagt uns, dass die Ridge-Regression abhängig von der Skalierung\nder Prädiktoren ist.\nDaher sollten die Prädiktoren vor der Ridge-Regression zunächst auf \\(sd=1\\) standardisiert werden.\nDa wir \\(\\beta_0\\) nicht schrumpfen wollen, sondern nur die Koeffizienten der Prädiktoren\nbietet es sich , die Prädiktoren dazu noch zu zentieren.\nKurz: Die z-Transformation bietet sich als Vorverarbeitung zur Ridge-Regression .","code":""},{"path":"regularisierte-modelle.html","id":"lasso-l1","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.5 Lasso, L1","text":"","code":""},{"path":"regularisierte-modelle.html","id":"strafterm-1","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.5.1 Strafterm","text":"Der Strafterm der “Lasso-Variante” der regularisierten Regression lautet :\\[\\text{Strafterm} = \\lambda \\sum_{j=1}^p |\\beta_j|,\\]ist also analog zur Ridge-Regression konzipiert.Genau wie bei der L2-Norm-Regularisierung ist ein “guter” Wert von lambda entscheidend.\nDieser Wert wird, wie bei der Ridge-Regression, durch Tuning bestimmt.Der Unterschied ist, dass die L1-Norm (Absolutwerte) und nicht die L2-Norm (Quadratwerte)\nverwendet werden.Die L1-Norm eines Vektors ist definiert durch \\(||\\beta||_1 = \\sum|\\beta_j|\\).","code":""},{"path":"regularisierte-modelle.html","id":"variablenselektion","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.5.2 Variablenselektion","text":"Genau wie die Ridge-Regression führt ein höhere lambda-Wert zu einer Regularisierung (Schrumpfung)\nder Koeffizienten.\nIm Unterschied zur Ridge-Regression hat das Lasso die Eigenschaft,\neinzelne Parameter auf exakt Null zu schrumpfen und damit faktisch als Prädiktor auszuschließen.\nAnders gesagt hat das Lasso die praktische Eigenschaft,\nVariablenselektion zu ermöglichen.Abb. 11.3 verdeutlicht den Effekt der Variablenselektion, vgl. James et al. (2021), Kap. 6.2.\nDie Ellipsen um \\(\\hat{beta}\\) herum nent man Kontourlinien. Alle Punkte einer Kontourlinie\nhaben den gleiche RSS-Wert,\nstehen also für eine gleichwertige OLS-Lösung.\nFigure 11.3: lambda der Lasso-Regression\nWarum erlaubt die L1-Norm Variablenselektion,\ndie L2-Norm aber nicht?\nAbb. 11.4 verdeutlicht den Unterschied zwischen L1- und L2-Norm.\nEs ist eine Regression mit zwei Prädiktoren, also den zwei Koeffizienten \\(\\beta1, \\beta_2\\) dargestellt.\nFigure 11.4: Verlauf des Strafterms bei der L1-Norm (links) und der L2-Norm (rechts)\nBetrachten wir zunächst das rechte Teilbild für die L2-Norm aus Abb. 11.4,\ndas Abb. 11.5 den Fokus gerückt wird (Rhys 2020).\nFigure 11.5: Verlauf des Strafterms bei der L2-Norm\nWenn lambda gleich Null ist, entspricht \\(L_{L2}\\) genau der OLS-Lösung.\nVergrößert man lambda,\nliegt \\(L_{L2}\\) dem Schnittpunkt des OLS-Kreises mit dem zugehörigen lambda-Kreis.\nWie man sieht, führt eine Erhöhung von lambda zu einer Reduktion der Absolutwerte von \\(\\beta_1\\) und \\(\\beta_2\\).\nAllerdings werden, wie man im Diagramm sieht, auch bei hohen lambda-Werten die\nRegressionskoeffizienten nicht exakt Null sein.Warum lässt die L2-Norm für bestimmte lambda-Werte den charakteristischen Kreis entstehen?\nDie Antwort ist, dass die Lösungen für \\(\\beta_1^2 + \\beta_2^2=1\\) (mit \\(\\lambda=1\\)) graphisch als Kreis dargestellt werden können.Anders ist die Situation bei der L1-Norm, dem Lasso, vgl. Abb. 11.6 (Rhys 2020).\nFigure 11.6: Verlauf des Strafterms bei der L1-Norm\nEine Erhöhung von $ führt aufgrund der charakteristischen Kontourlinie zu einem Schnittpunkt (von OLS-Lösung und lambda-Wert),\nder - wenn lambda groß genug ist, stets auf einer der beiden Achsen liegt,\nalso zu einer Nullsetzung des Parameters führt.Damit kann man argumentieren,\ndass das Lasso implizit davon ausgeht,\ndass einige Koeffizienten Wirklichkeit exakt Null sind,\ndie L2-Norm aber nicht.","code":""},{"path":"regularisierte-modelle.html","id":"l1-vs.-l2","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.6 L1 vs. L2","text":"","code":""},{"path":"regularisierte-modelle.html","id":"wer-ist-stärker","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.6.1 Wer ist stärker?","text":"Man kann nicht sagen, dass die L1- oder die L2-Norm strikt besser sei.\nEs kommt auf den Datensatz .\nWenn man einen Datensatz hat, dem es eingie wenige starke Prädiktoren gibt\nund viele sehr schwache (oder exakt irrelevante) Prädiktoren gibt,\ndann wird L1 tendenziell zu besseren Ergebnissen führen(James et al. 2021, S. 246).\nDas Lasso hat noch den Vorteil der Einfachheit, da\nweniger Prädiktoren im Modell verbleiben.Ridge-Regression wird dann besser abschneiden (tendenziell),\nwenn die Prädiktoren etwa alle gleich stark sind.","code":""},{"path":"regularisierte-modelle.html","id":"elastic-net-als-kompromiss","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.6.2 Elastic Net als Kompromiss","text":"Das Elastic Net (EN) ist ein Kompromiss zwischen L1- und L2-Norm.\n\\(\\lambda\\) wird auf einen Wert zwischen 1 und 2 eingestellt;\nauch hier wird der Wert für \\(\\lambda\\) wieder per Tuning gefunden.\\[L_{EN} = RSS + \\lambda\\left((1-\\alpha))\\cdot \\text{L2-Strafterm} + \\alpha \\cdot  \\text{L1-Strafterm}\\right)\\]\\(\\alpha\\) ist ein Tuningparameter, der einstellt, wie sehr wir uns Richtung L1- vs. L2-Norm bewegen.\nDamit wird sozusagen die “Mischung” eingestellt (von L1- vs. L2).Spezialfälle:Wenn \\(\\alpha=0\\) resultiert die Ridge-Regression (L1-Strafterm wird Null)Wenn \\(\\alpha=1\\) resultiert die Lasso-Regression (L2-Strafterm wird Null)","code":""},{"path":"regularisierte-modelle.html","id":"aufgaben-9","chapter":"Kapitel 11 Regularisierte Modelle","heading":"11.7 Aufgaben","text":"Fallstudie Serie OfficeFallstudie NBER Papers","code":""},{"path":"kaggle.html","id":"kaggle","chapter":"Kapitel 12 Kaggle","heading":"Kapitel 12 Kaggle","text":"","code":""},{"path":"kaggle.html","id":"vorbereitung-9","chapter":"Kapitel 12 Kaggle","heading":"12.1 Vorbereitung","text":"","code":""},{"path":"kaggle.html","id":"lernsteuerung-9","chapter":"Kapitel 12 Kaggle","heading":"12.1.1 Lernsteuerung","text":"","code":""},{"path":"kaggle.html","id":"lernziele-10","chapter":"Kapitel 12 Kaggle","heading":"12.1.2 Lernziele","text":"Sie wissen, wie man einen Datensatz für einen Prognosewettbwerb bei Kaggle einreichtSie kennen einige Beispiele von Notebooks auf Kaggle (für die Sprache R)Sie wissen, wie man ein Workflow-Set Tidymodels berechnetSie wissen, dass Tidymodels im Rezept keine Transformationen im Test-Sample berücksichtigt und wie man damit umgeht","code":""},{"path":"kaggle.html","id":"hinweise-5","chapter":"Kapitel 12 Kaggle","heading":"12.1.3 Hinweise","text":"Machen Sie sich mit Kaggle vertraut. Als Übungs-Wettbewerb dient uns TMDB Box-office Revenue","code":""},{"path":"kaggle.html","id":"r-pakete","chapter":"Kapitel 12 Kaggle","heading":"12.1.4 R-Pakete","text":"diesem Kapitel werden folgende R-Pakete benötigt:","code":"\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tictoc)  # Rechenzeit messen\nlibrary(lubridate)  # Datumsangaben\nlibrary(VIM)  # fehlende Werte\nlibrary(visdat)  # Datensatz visualisieren"},{"path":"kaggle.html","id":"was-ist-kaggle","chapter":"Kapitel 12 Kaggle","heading":"12.2 Was ist Kaggle?","text":"Kaggle, subsidiary Google LLC, online community data scientists machine learning practitioners. Kaggle allows users find publish data sets, explore build models web-based data-science environment, work data scientists machine learning engineers, enter competitions solve data science challenges.QuelleKaggle AirBnB Data Scientists?!","code":""},{"path":"kaggle.html","id":"fallstudie-tmdb","chapter":"Kapitel 12 Kaggle","heading":"12.3 Fallstudie TMDB","text":"Wir bearbeiten hier die Fallstudie TMDB Box Office Prediction -\nCan predict movie’s worldwide box office revenue?,\nein Kaggle-Prognosewettbewerb.Ziel ist es, genaue Vorhersagen zu machen,\ndiesem Fall für Filme.","code":""},{"path":"kaggle.html","id":"aufgabe","chapter":"Kapitel 12 Kaggle","heading":"12.3.1 Aufgabe","text":"Reichen Sie bei Kaggle eine Submission für die Fallstudie ein! Berichten Sie den Score!","code":""},{"path":"kaggle.html","id":"hinweise-6","chapter":"Kapitel 12 Kaggle","heading":"12.3.2 Hinweise","text":"Sie müssen sich bei Kaggle ein Konto anlegen (kostenlos und anonym möglich); alternativ können Sie sich mit einem Google-Konto anmelden.Halten Sie das Modell einfach wie möglich. Verwenden Sie als Algorithmus die lineare Regression ohne weitere Schnörkel.Logarithmieren Sie budget und revenue.Minimieren Sie die Vorverarbeitung (steps) weit als möglich.Verwenden Sie tidymodels.Die Zielgröße ist revenue Dollars; nicht “Log-Dollars”. Sie müssen also rücktransformieren,\nwenn Sie revenue logarithmiert haben.","code":""},{"path":"kaggle.html","id":"daten-1","chapter":"Kapitel 12 Kaggle","heading":"12.3.3 Daten","text":"Die Daten können Sie von der Kaggle-Projektseite beziehen oder :Mal einen Blick werfen:","code":"## Rows: 3,000\n## Columns: 23\n## $ id                    <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n## $ belongs_to_collection <chr> \"[{'id': 313576, 'name': 'Hot Tub Time Machine C…\n## $ budget                <dbl> 1.40e+07, 4.00e+07, 3.30e+06, 1.20e+06, 0.00e+00…\n## $ genres                <chr> \"[{'id': 35, 'name': 'Comedy'}]\", \"[{'id': 35, '…\n## $ homepage              <chr> NA, NA, \"http://sonyclassics.com/whiplash/\", \"ht…\n## $ imdb_id               <chr> \"tt2637294\", \"tt0368933\", \"tt2582802\", \"tt182148…\n## $ original_language     <chr> \"en\", \"en\", \"en\", \"hi\", \"ko\", \"en\", \"en\", \"en\", …\n## $ original_title        <chr> \"Hot Tub Time Machine 2\", \"The Princess Diaries …\n## $ overview              <chr> \"When Lou, who has become the \\\"father of the In…\n## $ popularity            <dbl> 6.575393, 8.248895, 64.299990, 3.174936, 1.14807…\n## $ poster_path           <chr> \"/tQtWuwvMf0hCc2QR2tkolwl7c3c.jpg\", \"/w9Z7A0GHEh…\n## $ production_companies  <chr> \"[{'name': 'Paramount Pictures', 'id': 4}, {'nam…\n## $ production_countries  <chr> \"[{'iso_3166_1': 'US', 'name': 'United States of…\n## $ release_date          <chr> \"2/20/15\", \"8/6/04\", \"10/10/14\", \"3/9/12\", \"2/5/…\n## $ runtime               <dbl> 93, 113, 105, 122, 118, 83, 92, 84, 100, 91, 119…\n## $ spoken_languages      <chr> \"[{'iso_639_1': 'en', 'name': 'English'}]\", \"[{'…\n## $ status                <chr> \"Released\", \"Released\", \"Released\", \"Released\", …\n## $ tagline               <chr> \"The Laws of Space and Time are About to be Viol…\n## $ title                 <chr> \"Hot Tub Time Machine 2\", \"The Princess Diaries …\n## $ Keywords              <chr> \"[{'id': 4379, 'name': 'time travel'}, {'id': 96…\n## $ cast                  <chr> \"[{'cast_id': 4, 'character': 'Lou', 'credit_id'…\n## $ crew                  <chr> \"[{'credit_id': '59ac067c92514107af02c8c8', 'dep…\n## $ revenue               <dbl> 12314651, 95149435, 13092000, 16000000, 3923970,…## Rows: 4,398\n## Columns: 22\n## $ id                    <dbl> 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, …\n## $ belongs_to_collection <chr> \"[{'id': 34055, 'name': 'Pokémon Collection', 'p…\n## $ budget                <dbl> 0.00e+00, 8.80e+04, 0.00e+00, 6.80e+06, 2.00e+06…\n## $ genres                <chr> \"[{'id': 12, 'name': 'Adventure'}, {'id': 16, 'n…\n## $ homepage              <chr> \"http://www.pokemon.com/us/movies/movie-pokemon-…\n## $ imdb_id               <chr> \"tt1226251\", \"tt0051380\", \"tt0118556\", \"tt125595…\n## $ original_language     <chr> \"ja\", \"en\", \"en\", \"fr\", \"en\", \"en\", \"de\", \"en\", …\n## $ original_title        <chr> \"ディアルガVSパルキアVSダークライ\", \"Attack of t…\n## $ overview              <chr> \"Ash and friends (this time accompanied by newco…\n## $ popularity            <dbl> 3.851534, 3.559789, 8.085194, 8.596012, 3.217680…\n## $ poster_path           <chr> \"/tnftmLMemPLduW6MRyZE0ZUD19z.jpg\", \"/9MgBNBqlH1…\n## $ production_companies  <chr> NA, \"[{'name': 'Woolner Brothers Pictures Inc.',…\n## $ production_countries  <chr> \"[{'iso_3166_1': 'JP', 'name': 'Japan'}, {'iso_3…\n## $ release_date          <chr> \"7/14/07\", \"5/19/58\", \"5/23/97\", \"9/4/10\", \"2/11…\n## $ runtime               <dbl> 90, 65, 100, 130, 92, 121, 119, 77, 120, 92, 88,…\n## $ spoken_languages      <chr> \"[{'iso_639_1': 'en', 'name': 'English'}, {'iso_…\n## $ status                <chr> \"Released\", \"Released\", \"Released\", \"Released\", …\n## $ tagline               <chr> \"Somewhere Between Time & Space... A Legend Is B…\n## $ title                 <chr> \"Pokémon: The Rise of Darkrai\", \"Attack of the 5…\n## $ Keywords              <chr> \"[{'id': 11451, 'name': 'pok√©mon'}, {'id': 1155…\n## $ cast                  <chr> \"[{'cast_id': 3, 'character': 'Tonio', 'credit_i…\n## $ crew                  <chr> \"[{'credit_id': '52fe44e7c3a368484e03d683', 'dep…"},{"path":"kaggle.html","id":"train-set-verschlanken","chapter":"Kapitel 12 Kaggle","heading":"12.3.4 Train-Set verschlanken","text":"Da wir aus Gründen der Einfachheit einige Spalten nicht berücksichtigen,\nentfernen wir diese Spalten,\ndie Größe des Datensatzes massiv reduziert.","code":""},{"path":"kaggle.html","id":"datensatz-kennenlernen","chapter":"Kapitel 12 Kaggle","heading":"12.3.5 Datensatz kennenlernen","text":"","code":""},{"path":"kaggle.html","id":"fehlende-werte-prüfen","chapter":"Kapitel 12 Kaggle","heading":"12.3.6 Fehlende Werte prüfen","text":"Welche Spalten haben viele fehlende Werte?Mit VIM kann man einen Datensatz gut auf fehlende Werte hin untersuchen:","code":""},{"path":"kaggle.html","id":"rezept","chapter":"Kapitel 12 Kaggle","heading":"12.4 Rezept","text":"","code":""},{"path":"kaggle.html","id":"rezept-definieren-3","chapter":"Kapitel 12 Kaggle","heading":"12.4.1 Rezept definieren","text":"","code":"## Recipe\n## \n## Inputs:\n## \n##       role #variables\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Variable mutation for if_else(budget < 10, 10, budget)\n## Log transformation on budget\n## Variable mutation for mdy(release_date)\n## Date features from release_date\n## K-nearest neighbor imputation for all_predictors()\n## Dummy variables from all_nominal()## # A tibble: 6 × 6\n##   number operation type       trained skip  id              \n##    <int> <chr>     <chr>      <lgl>   <lgl> <chr>           \n## 1      1 step      mutate     FALSE   FALSE mutate_NPsDj    \n## 2      2 step      log        FALSE   FALSE log_uY0o5       \n## 3      3 step      mutate     FALSE   FALSE mutate_5L3cX    \n## 4      4 step      date       FALSE   FALSE date_fbNLA      \n## 5      5 step      impute_knn FALSE   FALSE impute_knn_qa3u3\n## 6      6 step      dummy      FALSE   FALSE dummy_pzH9K"},{"path":"kaggle.html","id":"check-das-rezept","chapter":"Kapitel 12 Kaggle","heading":"12.4.2 Check das Rezept","text":"Wir definieren eine Helper-Funktion:Und wenden diese auf jede Spalte :Keine fehlenden Werte mehr den Prädiktoren.Nach fehlenden Werten könnte man z.B. auch suchen:","code":"## oper 1 step mutate [training] \n## oper 2 step log [training] \n## oper 3 step mutate [training] \n## oper 4 step date [training] \n## oper 5 step impute knn [training] \n## oper 6 step dummy [training] \n## The retained training set is ~ 0.38 Mb  in memory.## Recipe\n## \n## Inputs:\n## \n##       role #variables\n##    outcome          1\n##  predictor          4\n## \n## Training data contained 3000 data points and 2 incomplete rows. \n## \n## Operations:\n## \n## Variable mutation for ~if_else(budget < 10, 10, budget) [trained]\n## Log transformation on budget [trained]\n## Variable mutation for ~mdy(release_date) [trained]\n## Date features from release_date [trained]\n## K-nearest neighbor imputation for runtime, budget, release_date_year, release_da... [trained]\n## Dummy variables from release_date_month [trained]## # A tibble: 3,000 × 16\n##    popularity runtime budget  revenue release_date_year release_date_month_Feb\n##         <dbl>   <dbl>  <dbl>    <dbl>             <dbl>                  <dbl>\n##  1      6.58       93  16.5  12314651              2015                      1\n##  2      8.25      113  17.5  95149435              2004                      0\n##  3     64.3       105  15.0  13092000              2014                      0\n##  4      3.17      122  14.0  16000000              2012                      0\n##  5      1.15      118   2.30  3923970              2009                      1\n##  6      0.743      83  15.9   3261638              1987                      0\n##  7      7.29       92  16.5  85446075              2012                      0\n##  8      1.95       84   2.30  2586511              2004                      0\n##  9      6.90      100   2.30 34327391              1996                      1\n## 10      4.67       91  15.6  18750246              2003                      0\n## # … with 2,990 more rows, and 10 more variables: release_date_month_Mar <dbl>,\n## #   release_date_month_Apr <dbl>, release_date_month_May <dbl>,\n## #   release_date_month_Jun <dbl>, release_date_month_Jul <dbl>,\n## #   release_date_month_Aug <dbl>, release_date_month_Sep <dbl>,\n## #   release_date_month_Oct <dbl>, release_date_month_Nov <dbl>,\n## #   release_date_month_Dec <dbl>## # A tibble: 1 × 16\n##   popularity runtime budget revenue release_date_year release_date_month_Feb\n##        <int>   <int>  <int>   <int>             <int>                  <int>\n## 1          0       0      0       0                 0                      0\n## # … with 10 more variables: release_date_month_Mar <int>,\n## #   release_date_month_Apr <int>, release_date_month_May <int>,\n## #   release_date_month_Jun <int>, release_date_month_Jul <int>,\n## #   release_date_month_Aug <int>, release_date_month_Sep <int>,\n## #   release_date_month_Oct <int>, release_date_month_Nov <int>,\n## #   release_date_month_Dec <int>"},{"path":"kaggle.html","id":"variable-mean-sd-iqr-range-skewness-kurtosis-n-n_missing","chapter":"Kapitel 12 Kaggle","heading":"12.5 Variable | Mean | SD | IQR | Range | Skewness | Kurtosis | n | n_Missing","text":"popularity | 8.46 | 12.10 | 6.88 | [1.00e-06, 294.34] | 14.38 | 280.10 | 3000 | 0\nruntime | 107.86 | 22.09 | 24.00 | [0.00, 338.00] | 1.02 | 8.19 | 2998 | 2\nrevenue | 6.67e+07 | 1.38e+08 | 6.66e+07 | [1.00, 1.52e+09] | 4.54 | 27.78 | 3000 | 0\nbudget | 2.25e+07 | 3.70e+07 | 2.90e+07 | [0.00, 3.80e+08] | 3.10 | 13.23 | 3000 | 0So bekommt man gleich noch ein paar Infos über die Verteilung der Variablen. Praktische Sache.","code":""},{"path":"kaggle.html","id":"check-test-sample","chapter":"Kapitel 12 Kaggle","heading":"12.5.1 Check Test-Sample","text":"Das Test-Sample backen wir auch mal.Wichtig: Wir preppen den Datensatz mit dem Train-Sample.","code":"## # A tibble: 6 × 15\n##   popularity runtime budget release_date_year release_date_mon… release_date_mo…\n##        <dbl>   <dbl>  <dbl>             <dbl>             <dbl>            <dbl>\n## 1       3.85      90   2.30              2007                 0                0\n## 2       3.56      65  11.4               2058                 0                0\n## 3       8.09     100   2.30              1997                 0                0\n## 4       8.60     130  15.7               2010                 0                0\n## 5       3.22      92  14.5               2005                 1                0\n## 6       8.68     121   2.30              1996                 1                0\n## # … with 9 more variables: release_date_month_Apr <dbl>,\n## #   release_date_month_May <dbl>, release_date_month_Jun <dbl>,\n## #   release_date_month_Jul <dbl>, release_date_month_Aug <dbl>,\n## #   release_date_month_Sep <dbl>, release_date_month_Oct <dbl>,\n## #   release_date_month_Nov <dbl>, release_date_month_Dec <dbl>"},{"path":"kaggle.html","id":"kreuzvalidierung","chapter":"Kapitel 12 Kaggle","heading":"12.6 Kreuzvalidierung","text":"","code":""},{"path":"kaggle.html","id":"modelle-1","chapter":"Kapitel 12 Kaggle","heading":"12.7 Modelle","text":"","code":""},{"path":"kaggle.html","id":"baum-1","chapter":"Kapitel 12 Kaggle","heading":"12.7.1 Baum","text":"","code":""},{"path":"kaggle.html","id":"random-forest","chapter":"Kapitel 12 Kaggle","heading":"12.7.2 Random Forest","text":"","code":""},{"path":"kaggle.html","id":"xgboost-2","chapter":"Kapitel 12 Kaggle","heading":"12.7.3 XGBoost","text":"","code":""},{"path":"kaggle.html","id":"lm","chapter":"Kapitel 12 Kaggle","heading":"12.7.4 LM","text":"","code":""},{"path":"kaggle.html","id":"workflows-2","chapter":"Kapitel 12 Kaggle","heading":"12.8 Workflows","text":"","code":""},{"path":"kaggle.html","id":"fitten-und-tunen","chapter":"Kapitel 12 Kaggle","heading":"12.9 Fitten und tunen","text":"Man kann sich das Ergebnisobjekt abspeichern,\num künftig Rechenzeit zu sparen:Professioneller ist der Ansatz mit dem R-Paket target.","code":""},{"path":"kaggle.html","id":"finalisieren","chapter":"Kapitel 12 Kaggle","heading":"12.10 Finalisieren","text":"","code":""},{"path":"kaggle.html","id":"welcher-algorithmus-schneidet-am-besten-ab","chapter":"Kapitel 12 Kaggle","heading":"12.10.1 Welcher Algorithmus schneidet am besten ab?","text":"Genauer geagt, welches Modell, denn es ist ja nicht nur ein Algorithmus,\nsondern ein Algorithmus plus ein Rezept plus die Parameterinstatiierung plus\nein spezifischer Datensatz.R-Quadrat ist nicht entscheidend; rmse ist wichtiger.Die Ergebnislage ist nicht ganz klar, aber\neiniges spricht für das Boosting-Modell, rec1_boost1.","code":"## # A tibble: 10 × 9\n##    wflow_id    .config     preproc model .metric .estimator   mean     n std_err\n##    <chr>       <chr>       <chr>   <chr> <chr>   <chr>       <dbl> <int>   <dbl>\n##  1 rec1_lm1    Preprocess… recipe  line… rmse    standard   1.15e8    15  2.20e6\n##  2 rec1_tree1  Preprocess… recipe  deci… rmse    standard   1.12e8    15  2.67e6\n##  3 rec1_rf1    Preprocess… recipe  rand… rmse    standard   1.10e8    15  2.64e6\n##  4 rec1_tree1  Preprocess… recipe  deci… rmse    standard   9.46e7    15  2.30e6\n##  5 rec1_tree1  Preprocess… recipe  deci… rmse    standard   9.33e7    15  2.26e6\n##  6 rec1_boost1 Preprocess… recipe  boos… rmse    standard   9.30e7    15  1.91e6\n##  7 rec1_boost1 Preprocess… recipe  boos… rmse    standard   9.27e7    15  2.13e6\n##  8 rec1_tree1  Preprocess… recipe  deci… rmse    standard   9.21e7    15  1.91e6\n##  9 rec1_tree1  Preprocess… recipe  deci… rmse    standard   9.21e7    15  1.91e6\n## 10 rec1_boost1 Preprocess… recipe  boos… rmse    standard   9.21e7    15  1.73e6## # A tibble: 1 × 4\n##    mtry trees min_n .config              \n##   <int> <int> <int> <chr>                \n## 1     6   100     4 Preprocessor1_Model04## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: boost_tree()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 6 Recipe Steps\n## \n## • step_mutate()\n## • step_log()\n## • step_mutate()\n## • step_date()\n## • step_impute_knn()\n## • step_dummy()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Boosted Tree Model Specification (regression)\n## \n## Main Arguments:\n##   mtry = 6\n##   trees = 100\n##   min_n = 4\n## \n## Engine-Specific Arguments:\n##   nthreads = parallel::detectCores()\n## \n## Computational engine: xgboost"},{"path":"kaggle.html","id":"final-fit-1","chapter":"Kapitel 12 Kaggle","heading":"12.10.2 Final Fit","text":"","code":"## [15:37:43] WARNING: amalgamation/../src/learner.cc:576: \n## Parameters: { \"nthreads\" } might not be used.\n## \n##   This could be a false alarm, with some parameters getting used by language bindings but\n##   then being mistakenly passed down to XGBoost core, or some parameter actually being used\n##   but getting flagged wrongly here. Please open an issue if you find any such cases.## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: boost_tree()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 6 Recipe Steps\n## \n## • step_mutate()\n## • step_log()\n## • step_mutate()\n## • step_date()\n## • step_impute_knn()\n## • step_dummy()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## ##### xgb.Booster\n## raw: 351.6 Kb \n## call:\n##   xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n##     colsample_bytree = 1, colsample_bynode = 0.4, min_child_weight = 4L, \n##     subsample = 1, objective = \"reg:squarederror\"), data = x$data, \n##     nrounds = 100L, watchlist = x$watchlist, verbose = 0, nthreads = 8L, \n##     nthread = 1)\n## params (as set within xgb.train):\n##   eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"0.4\", min_child_weight = \"4\", subsample = \"1\", objective = \"reg:squarederror\", nthreads = \"8\", nthread = \"1\", validate_parameters = \"TRUE\"\n## xgb.attributes:\n##   niter\n## callbacks:\n##   cb.evaluation.log()\n## # of features: 15 \n## niter: 100\n## nfeatures : 15 \n## evaluation_log:\n##     iter training_rmse\n##        1     121004840\n##        2     100166656\n## ---                   \n##       99      27574408\n##      100      27248100"},{"path":"kaggle.html","id":"submission","chapter":"Kapitel 12 Kaggle","heading":"12.11 Submission","text":"","code":""},{"path":"kaggle.html","id":"submission-vorbereiten","chapter":"Kapitel 12 Kaggle","heading":"12.11.1 Submission vorbereiten","text":"Abspeichern und einreichen:Diese CSV-Datei reichen wir dann bei Kagglei ein.","code":""},{"path":"kaggle.html","id":"kaggle-score","chapter":"Kapitel 12 Kaggle","heading":"12.11.2 Kaggle Score","text":"Diese Submission erzielte einen Score von 4.79227 (RMSLE).","code":""},{"path":"kaggle.html","id":"aufgaben-10","chapter":"Kapitel 12 Kaggle","heading":"12.12 Aufgaben","text":"Arbeiten Sie sich gut als möglich durch diese Analyse zum Verlauf von Covid-FällenFallstudie zur Modellierung einer logististischen Regression mit tidymodelsFallstudie zu VulkanausbrüchenFallstudie Himalaya","code":""},{"path":"kaggle.html","id":"vertiefung-5","chapter":"Kapitel 12 Kaggle","heading":"12.13 Vertiefung","text":"Fields arranged purity, xkcd 435","code":""},{"path":"der-rote-faden.html","id":"der-rote-faden","chapter":"Kapitel 13 Der rote Faden","heading":"Kapitel 13 Der rote Faden","text":"Mittlerweile haben wir einiges zum Thema Data Science bzw. maschinelles Lernen behandelt (und sie hoffentlich viel gelernt).Da ist es der Zeit, einen Schritt zurück zu treten,\num sich einen Überblick über den gegangenen Weg zu verschaffen,\nden berühmten “roten Faden” zu sehen, den zurückgelegten Weg nachzuzeichnen\nden groben Linien, um einen (klareren) Überblick über das Terrain zu bekommen.diesem Kapitel werden wir verschiedene “Aussichtspfade” suchen, um im Bild zu bleiben,\ndie uns einen Überblick über das Gelände versprechen.","code":""},{"path":"der-rote-faden.html","id":"aussichtspunkt-1-blick-vom-hohen-berg","chapter":"Kapitel 13 Der rote Faden","heading":"13.1 Aussichtspunkt 1: Blick vom hohen Berg","text":"Und zeigt sich ein “Flussbild” (Abb. 13.1).\nFigure 13.1: Ein Flussbild des maschinellen Lernens\nDer Reiseführer erzählt uns zu diesem Bild folgende Geschichte:Video-Geschichte","code":""},{"path":"der-rote-faden.html","id":"aussichtspunkt-2-blick-in-den-hof-der-handwerker","chapter":"Kapitel 13 Der rote Faden","heading":"13.2 Aussichtspunkt 2: Blick in den Hof der Handwerker","text":"Wenn man auf einem hohen Berg gestanden ist, hat man zwar einen guten Überblick über das Land bekommen,\naber das konkrete Tun bleibt auf solchen Höhen verborgen.Möchte man wissen, wie das geschäftige Leben abläuft, muss man also den tätigen Menschen über die Schulter schauen.\nWerfen wir also einen Blick den “Hof der Handwerker”,\nwo grundlegende Werkstücke gefertigt werden,\nund wir jeden Handgriff aus der Nähe mitverfolgen können.Weniger blumig ausgedrückt: Schauen wir uns ein maximal einfaches Beispiel ,\nwie man mit Tidymodels Vorhersagen tätigt.","code":""},{"path":"der-rote-faden.html","id":"ein-maximale-einfaches-werkstück-mit-tidymodels","chapter":"Kapitel 13 Der rote Faden","heading":"13.2.1 Ein maximale einfaches Werkstück mit Tidymodels","text":"","code":""},{"path":"der-rote-faden.html","id":"ein-immer-noch-recht-einfaches-werkstück-mit-tidymodels","chapter":"Kapitel 13 Der rote Faden","heading":"13.2.2 Ein immer noch recht einfaches Werkstück mit Tidymodels","text":"","code":""},{"path":"der-rote-faden.html","id":"aussichtspunkt-3-der-nebelberg-quiz","chapter":"Kapitel 13 Der rote Faden","heading":"13.3 Aussichtspunkt 3: Der Nebelberg (Quiz)","text":"Da der “Nebelberg” zumeist Wolken verhüllt ist, muss man, wenn man ihn ersteigt und ins Land hinunterschaut,\nerraten, welche Teile zu sehen sind. Sozusagen eine Art Landschafts-Quiz.Voilà, hier ist es, das Quiz zum maschinellen Lernen:","code":""},{"path":"der-rote-faden.html","id":"aussichtspunkt-4-der-exerzitien-park","chapter":"Kapitel 13 Der rote Faden","heading":"13.4 Aussichtspunkt 4: Der Exerzitien-Park","text":"Wir stehen vor dem Eingang zu einem Park,\ndem sich viele Menschen merkwürdigen Übungen, Exerzitien, befleißigen.\nVielleicht wollen Sie sich auch einigen Übungen abhärten? Bitte schön,\nlassen Sie sich nicht von mir aufhalten.","code":""},{"path":"der-rote-faden.html","id":"yacsda-collection","chapter":"Kapitel 13 Der rote Faden","heading":"13.5 YACSDA-Collection","text":"YACSDA: Yet Another Case Study Data Analysis…Experimenting machine learning R tidymodels Kaggle titanic datasetTutorial tidymodels Machine LearningClassification Tidymodels, Workflows RecipesA (mostly!) tidyverse tour TitanicPersonalised Medicine - EDA tidy RTidy TitaRnicFallstudie: Modellierung von FlugverspätungenFallstudie SeegurkenSehr einfache Fallstudie zur Modellierung einer Regression mit tidymodelsFallstudie zur linearen Regression mit TidymodelsAnalyse zum Verlauf von Covid-FällenFallstudie zur Modellierung einer logististischen Regression mit tidymodelsFallstudie zu VulkanausbrüchenFallstudie HimalayaFallstudien zu Studiengebühren1. Modell der Fallstudie Hotel BookingsAufgaben zur logistischen Regression, PDFFallstudie Oregon SchoolsFallstudie WindturbinenFallstudie ChurnEinfache Durchführung eines Modellierung mit XGBoost\nFallstudie Oregon SchoolsFallstudie ChurnFallstudie IkeaFallstudie Wasserquellen Sierra LeoneFallstudie Bäume San FranciscoFallstudie VulkanausbrücheFallstudie Brettspiele mit XGBoostFallstudie Serie OfficeFallstudie NBER PapersFallstudie Einfache lineare Regression mit Tidymodels, Kaggle-Competition TMDBFallstudie Einfaches Random-Forest-Modell mit Tidymodels, Kaggle-Competition TMDBFallstudie Einfache lineare Regression Base-R, Anfängerniveau, Kaggle-Competition TMDBFallstudie Workflow-Set mit Tidymodels, Kaggle-Competition TMDBFallstudie Titanic mit Tidymodels bei KaggleEinfache Fallstudie mit Tidymodels bei Kaggle","code":""},{"path":"der-rote-faden.html","id":"aussichtspunkt-5-in-der-bibliothek","chapter":"Kapitel 13 Der rote Faden","heading":"13.6 Aussichtspunkt 5: In der Bibliothek","text":"Einen Überblick über eine Landschaft gewinnt man nicht nur von ausgesetzten Wegpunkten aus,\nsondern auch, manchmal, aus Schriftstücken.\nHier ist eine Auswahl Literatur,\ndie Grundlagen zu unserem Landstrich erläutert.Rhys (2020)Silge Kuhn (2022)Etwas weiter leiten uns diese Erzähler:James et al. (2021)Kuhn Johnson (2013)","code":""},{"path":"dimensionsreduktion.html","id":"dimensionsreduktion","chapter":"Kapitel 14 Dimensionsreduktion","heading":"Kapitel 14 Dimensionsreduktion","text":"","code":""},{"path":"dimensionsreduktion.html","id":"lernsteuerung-10","chapter":"Kapitel 14 Dimensionsreduktion","heading":"14.1 Lernsteuerung","text":"","code":""},{"path":"dimensionsreduktion.html","id":"lernziele-11","chapter":"Kapitel 14 Dimensionsreduktion","heading":"14.1.1 Lernziele","text":"Sie kennen den Principal-Components-Algorithmus (PCA) und können ihn Grundzügen erläuternSie können eine PCA R berechnenSie wissen um Vorteile und Beschränkungen dieses Algorithmus","code":""},{"path":"dimensionsreduktion.html","id":"literatur-10","chapter":"Kapitel 14 Dimensionsreduktion","heading":"14.1.2 Literatur","text":"Rhys, Kap. 13","code":""},{"path":"dimensionsreduktion.html","id":"vorbereitung-10","chapter":"Kapitel 14 Dimensionsreduktion","heading":"14.2 Vorbereitung","text":"diesem Kapitel werden folgende R-Pakete benötigt:","code":"\nlibrary(tidymodels)\nlibrary(tidyverse)  "},{"path":"dimensionsreduktion.html","id":"dimensionsreduktion-mit-der-hauptkomponentenanalyse","chapter":"Kapitel 14 Dimensionsreduktion","heading":"14.3 Dimensionsreduktion mit der Hauptkomponentenanalyse","text":"Sagen wir, Sie möchten das Körpergewicht einer Person vorhersagen und haben dafür mehrere Prädiktoren zur Verfügung, vgl. Abb. 14.1.\nFigure 14.1: Vorhersage von Gewicht mit mehreren hochkorrelierten Prädiktoren\nSicherlich sind die ganzen “XXX_Länge-Prädiktoren” alle gut mit einer Variablen Körpergröße zusammenzufassen.\nMan kann also die Komplexität der Vorhersage deutlich reduzieren,\nindem man die Prädiktoren zu einer bzw. zumindest weniger neuen Dimensionen zusammenfasst.\nEine Methode dazu ist die Hauptkomponentenanalyse (Principal Component Analysis, PCA).","code":""},{"path":"dimensionsreduktion.html","id":"wozu-dimensionsreduktion","chapter":"Kapitel 14 Dimensionsreduktion","heading":"14.3.1 Wozu Dimensionsreduktion?","text":"Einerseits ist es nützlich, zusätzliche Prädiktoren zu einem prädiktiven Modell hinzuzufügen -\nvorausgesetzt sie sind mit der Outcome-Variablen korreliert.\nAuf der anderen Seite steigt der Stichprobenbedarf exponenziell der der Anzahl der Prädiktoren,\nvgl. Abb. 14.2, man spricht vom Fluch der Dimension.\nFigure 14.2: Illustration des Fluchs der Dimension\nAnders formuliert: Bei steigender Dimensionszahl sind die einzelnen Datenpunkte immer weiter voneinander entfernt,\ndie Datenlage wird “spärlicher” (sparse), vgl. Abb. 14.3 aus Altman Krzywinski (2018).\nFigure 14.3: Je mehr Dimensionen, desto weniger Daten pro Einheit\nMan könnte also sagen: Das Hinzufügen von Prädiktoren ist sinnvoll, wenn sie prädiktiv und die Stichprobe groß genug ist.Außerdem ist es schwierig, sich (als Mensch, nicht unbedingt wenn Sie eine Maschine sind) im hochdimensionalen Raum zu orientieren.\nMan könnte sogar zugespitzt behaupten, dass das Maschinelle Lernen nur deswegen erfunden wurde,\nweil sich Menschen nur im 3D-Raum orientieren können.Sagen wir, Sie haben \\(p=10\\) Prädiktoren, das ergibt dann \\({p \\choose 2} = p(p-1)/2\\) Möglichkeiten, also 45 bei \\(p=10\\).Im Datensatz mtcars sieht das aus, nur mal zur Verdeutlichung, s. Abb. 14.4.\nFigure 14.4: Viele Streudiagramm\n","code":"\nlibrary(GGally)\ndata(mtcars)\nggpairs(mtcars)"},{"path":"dimensionsreduktion.html","id":"pca-ungeleitetes-verfahren","chapter":"Kapitel 14 Dimensionsreduktion","heading":"14.3.2 PCA: ungeleitetes Verfahren","text":"PCA ist ein ungeleitetes bzw. unüberwachtes Verfahren, es gibt also mehrere Variablen, aber keine “Outcome-Variable”.\nEs geht daher nicht um Vorhersage - die ist nicht möglich, da es keine Zielvariable gibt.\nStattdessen kann das Ziel nur sein, Muster den Variablen zu finden,\ndass man die Anzahl der Variablen reduzieren kann.Die PCA versucht also, eine niedrig dimensionale Repräsentation der Datenmatrix \\(\\boldsymbol{X}\\) zu erstellen,\neine “Informationsverdichtung”, wenn es gut läuft.Aus den \\(p\\) Dimensionen von \\(\\boldsymbol{X}\\) suchen wir eine kleine Zahl zusammengefassten interessanten Prädiktoren.","code":""},{"path":"dimensionsreduktion.html","id":"pca-veranschaulicht","chapter":"Kapitel 14 Dimensionsreduktion","heading":"14.3.3 PCA veranschaulicht","text":"PCA wird zur Dimensionsreduktion verschiedenen Anwendungsbereichen verwendet, zum Beispiel zur Datenkompression, etwa bei Bildern, wie hier anschaulich dargestellt.Interessant wird der PCA operationalisiert als die neuen Dimensionen,\nentlang derer die Daten meisten variieren.Angenommen, wir haben eine Datenmatrix mit \\(p=2\\) und die beiden (metrischen) Variablen sind korreliert, vgl. Abb. 14.5, Quelle.\nFigure 14.5: Zwei korrelierte, metrische Variablen\nWir könnten argumentieren, dass diese 2D-Daten mit wenig Informationsverlust anhand einer Dimension\nbeschrieben werden können. Diese Dimension ist die Daten “gelegt”, dass ihr Vektor die Richtung zeigt,\nder die Varianz maximiert.\nGleichzeitig ist die Streuung der Daten innerhalb dieser Dimension minimiert,\ns. Abb. 14.6.\nder Abbildung ist die Dimension,\ndie Richtung der maximalen Streuung der Daten zeigt, mit \\(u_1\\) beschrieben.\neinem 2D-System kann es maximale zwei (orthogonale) Dimensionen geben.\nDie zweite Dimension ist mit \\(u_2\\) bezeichnet und bindet (relativ zu \\(u_1\\))\nwenig Streuung auf sich.\nFigure 14.6: Zwei korrelierte, metrische Variablen\nGeometrisch betrachtet ist die PCA also ein Rotationsverfahren,\ndas neue Achsen findet und zwar ,\ndass die Achsen die Daten “gut beschreiben”, “interessant sind”,\nalso die Richtung der maximalen Varianz zeigen.\nGeometrisch kann man das für 2-3 Dimensionen gut veranschaulichen,\naber dankbarerweise funktioniert die Algebra auch bei \\(p\\) Dimension ohne Murren.","code":""},{"path":"dimensionsreduktion.html","id":"was-sind-hauptkomponenten","chapter":"Kapitel 14 Dimensionsreduktion","heading":"14.3.4 Was sind Hauptkomponenten?","text":"Hauptkomponenten (Principal Components, PC) nennt man die Dimensionen,\ndie die \\(p\\) Variablen des Datensatzes zusammenfassen sollen.\nJede dieser Dimensionen ist eine Linearkombination der \\(p\\) Variablen (James et al. 2021).Die erste Hauptkomponente kann man dabei darstellen:\\[\\underbrace{Z_1}_{Score} = \\underbrace{\\phi_{11}}_{Ladung 1}\\underbrace{X_1}_{Prädiktor 1} + \\phi_{21}X_2 + \\cdots + \\phi_{p1}X_p\\]Dabei wird \\(Z_1\\) gewählt, dass die Varianz maximal ist.\nAußerdem gilt die Nebenbedingung, dass \\(\\sum_{j=1}^p \\phi^2_{j1}= 1\\).\nAnders gesagt müssen die Ladungen gewählt werden, dass die Summe ihrer Quadrate 1 ergibt.\nAndernfalls gäbe es beliebig viele Lösungen (mit beliebig großen Ladungen).\nBildlich gesprochen wird die Varianz der auf die Hauptkomponente projizierten Daten maximiert (Rhys 2020),\nvgl. Abb. 14.7.\nDa wir nicht Mittelwert, sondern nur den Streuungen interessiert sind,\ngehen wir von zentrierten Daten aus.\nFigure 14.7: Die Hauptkomponente maximiert die Varianz\nAnders gesagt optimiert die erste Hauptkomponente das folgende Optimierungsproblem:\\[\n\\begin{equation}\n\\underbrace{\\operatorname{maximize}}_{\\phi_11, \\ldots, \\phi_1p}\\left\\{  n^{-1}\\sum_{=1}^n \\left( \\sum_{j=1}^p \\phi_{j1}x_{ij} \\right)^2 \\right\\}.\n\\end{equation}\n\\]","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
