[["entscheidungsb√§ume.html", "Kapitel 10 Entscheidungsb√§ume 10.1 Vorbereitung 10.2 Anatomie eines Baumes 10.3 B√§ume als Regelmaschinen rekursiver Partionierung 10.4 Klassifikation 10.5 Gini als Optimierungskriterium 10.6 Metrische Pr√§diktoren 10.7 Regressionb√§ume 10.8 Baum beschneiden 10.9 Das Rechteck schl√§gt zur√ºck 10.10 Tidymodels", " Kapitel 10 Entscheidungsb√§ume 10.1 Vorbereitung In diesem Kapitel werden folgende R-Pakete ben√∂tigt: library(titanic) #library(rpart) # Berechnung von Entscheidungsb√§umen library(tidymodels) library(tictoc) # Zeitmessung 10.2 Anatomie eines Baumes Ein Baum üå≥ hat (u.a.): Wurzel Bl√§tter √Ñste In einem Entscheidungsbaum ist die Terminologie √§hnlich, s. Abb. 10.1. Allgemein gesagt, kann ein Entscheidungsbaum in einem baum√§hnlichen Graphen visualisiert werden. Dort gibt es Knoten, die durch Kanten verbunden sind, wobei zu einem Knoten genau ein Kanten f√ºhrt. Ein Beispiel f√ºr einen einfachen Baum sowie die zugeh√∂rige rekursive Partionierung ist in Abb. 10.1 dargestellt; man erkennt \\(R=3\\) Regionen bzw. Bl√§tter (James et al. 2021). Figure 10.1: Einfaches Beispiel f√ºr einen Baum sowie der zugeh√∂rigen rekursiven Partionierung In Abb. ?? wird der Knoten an der Spitze auch als Wurzel(knoten) bezeichnet. Von diesem Knoten entspringen alle Pfade. Ein Pfad ist die geordnete Menge der Pfade mit ihren Knoten ausgehend von der Wurzel bis zu einem Blatt. Knoten, aus denen kein Kanten mehr wegf√ºhrt (‚ÄúEndknoten‚Äù) werden als Bl√§tter bezeichnet. Von einem Knoten gehen zwei Kanten aus (oder gar keine). Knoten, von denen zwei Kanten ausgehen, spiegeln eine Bedingung (Pr√ºfung) wider, im Sinne einer Aussage, die mit ja oder nein beantwortet werden kann. Die Anzahl der Knoten eines Pfads entsprechen den Ebenen bzw. der Tiefe des Baumes. Von der obersten Ebene (Wurzelknoten) kann man die \\(e\\) Ebenen aufsteigend durchnummerieren, beginnend bei 1: \\(1,2,\\ldots,e\\). 10.3 B√§ume als Regelmaschinen rekursiver Partionierung Ein Baum kann man als eine Menge von Regeln, im Sinne von Wenn-dann-sonst-Aussagen, sehen: Wenn Pr√§diktor A = 1 ist dann | Wenn Pr√§diktor B = 0 ist dann p = 10% | sonst p = 30% sonst p = 50% In diesem Fall, zwei Pr√§diktoren, ist der Pr√§diktorenraum in drei Regionen unterteilt: Der Baum hat drei Bl√§tter. F√ºr Abb. 10.2 ergibt sich eine komplexere Aufteilung, s. auch Abb. 10.3. Figure 10.2: Beispiel f√ºr einen Entscheidungsbaum Kleine Lesehilft f√ºr Abb. 10.2: F√ºr jeden Knoten steht in der ersten Zeile der vorhergesagte Wert, z.B. 0 im Wurzelknoten darunter steht der Anteil (die Wahrscheinlichkeit) f√ºr die in diesem Knoten vorhergesagte Kategorie (0 oder 1) darunter (3. Zeile) steht der Anteil der F√§lle (am Gesamt-Datensatz) in diesem Knoten, z.B. 100% Figure 10.3: Partionierung in Rechtecke durch Entscheidungsb√§ume Wie der Algorithmus oben zeigt, wird der Pr√§diktorraum wiederholt (rekursiv) aufgeteilt, und zwar in Rechtecke,s. Abb. 10.3. Man nennt (eine Implementierung) dieses Algorithmus auch rpart. Das Regelwerk zum Baum aus Abb. 10.2 sieht so aus: titanic_train$Survived = as.factor(titanic_train$Survived) ti_tree &lt;- decision_tree() %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;classification&quot;) %&gt;% fit(Survived ~ Pclass + Age, data = titanic_train) ti_tree ## parsnip model object ## ## n= 891 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 891 342 0 (0.61616162 0.38383838) ## 2) Pclass&gt;=2.5 491 119 0 (0.75763747 0.24236253) ## 4) Age&gt;=6.5 461 102 0 (0.77874187 0.22125813) * ## 5) Age&lt; 6.5 30 13 1 (0.43333333 0.56666667) * ## 3) Pclass&lt; 2.5 400 177 1 (0.44250000 0.55750000) ## 6) Age&gt;=17.5 365 174 1 (0.47671233 0.52328767) ## 12) Pclass&gt;=1.5 161 66 0 (0.59006211 0.40993789) * ## 13) Pclass&lt; 1.5 204 79 1 (0.38725490 0.61274510) ## 26) Age&gt;=44.5 67 32 0 (0.52238806 0.47761194) ## 52) Age&gt;=60.5 14 3 0 (0.78571429 0.21428571) * ## 53) Age&lt; 60.5 53 24 1 (0.45283019 0.54716981) ## 106) Age&lt; 47.5 13 3 0 (0.76923077 0.23076923) * ## 107) Age&gt;=47.5 40 14 1 (0.35000000 0.65000000) * ## 27) Age&lt; 44.5 137 44 1 (0.32116788 0.67883212) * ## 7) Age&lt; 17.5 35 3 1 (0.08571429 0.91428571) * Kleine Lesehilfe: Ander Wurzel root des Baumes, Knoten 1)haben wir 891 F√§lle, von denen 342 nicht unserer Vorhersage yval entsprechen, also loss sind, das ist ein Anteil, (yprob) von 0.38. Unsere Vorhersage ist 0, da das die Mehrheit in diesem Knoten ist, dieser Anteil betr√§gt ca. 61%. In der Klammer stehen also die Wahrscheinlichkeiten f√ºr alle Auspr√§gungen von Y:, 0 und 1, in diesem Fall. Entsprechendes gilt f√ºr jeden weiteren Knoten. Ein kurzer Check der H√§ufigkeit am Wurzelknoten: ## Survived n ## 1 0 549 ## 2 1 342 Solche Entscheidungsb√§ume zu erstellen, ist nichts neues. Man kann sie mit einer einfachen Checkliste oder Entscheidungssystem vergleichen. Der Unterschied zu Entscheidungsb√§umen im maschinellen Lernen ist nur, dass die Regeln aus den Daten gelernt werden, man muss sie nicht vorab kennen. Noch ein Beispiel ist in Abb. 10.4 gezeigt (James et al. 2021): Oben links zeigt eine unm√∂gliche Partionierung (f√ºr einen Entscheidungsbaum). Oben rechts zeigt die Regionen, die sich durch den Entscheidungsbaum unten links ergeben. Untenrechts ist der Baum in 3D dargestellt. Figure 10.4: Ein weiteres Beispiel zur Darstellung von Entscheidungsb√§umen 10.4 Klassifikation B√§ume k√∂nnen f√ºr Zwecke der Klassifikation (nominal skalierte AV) oder Regression (numerische AV) verwendet werden. Betrachten wir zun√§chst die bin√§re Klassifikation, also f√ºr eine zweistufige (nominalskalierte) AV. Das Ziel des Entscheidungsmodel-Algorithmus ist es, zu Bl√§ttern zu kommen, die m√∂glichst ‚Äúsortenrein‚Äù sind, sich also m√∂glichst klar f√ºr eine (der beiden) Klassen \\(A\\) oder \\(B\\) aussprechen. Nach dem Motto: ‚ÄúWenn Pr√§diktor 1 kleiner \\(x\\) und wenn Pr√§diktor 2 gleich \\(y\\), dann handelt es sich beim vorliegenden Fall ziemlich sicher um Klasse \\(A\\).‚Äù Je homogener die Verteilung der AV pro Blatt, desto genauer die Vorhersagen. Unsere Vorhersage in einem Blatt entspricht der Merheit bzw. der h√§ufigsten Kategorie in diesem Blatt. 10.5 Gini als Optimierungskriterium Es gibt mehrere Kennzahlen, die zur Optimierung bzw. zur Entscheidung zum Aufbau des Entscheidungsbaum herangezogen werden. Zwei √ºbliche sind der Gini-Koeffizient und die Entropie. Bei Kennzahlen sind Ma√ü f√ºr die Homogenit√§t oder ‚ÄúSortenreinheit‚Äù (vs.¬†Heterogenit√§t, engl. auch impurity). Den Algorithmus zur Erzeugung des Baumes kann man so darstellen: Wiederhole f√ºr jede Ebenes | pr√ºfe f√ºr alle Pr√§diktoren alle m√∂glichen Bedingungen | w√§hle denjenigen Pr√§diktor mit derjenigen Bedingung, der die Homogenit√§t maximiert solange bis Abbruchkriterium erreicht ist. Ein Bedingung k√∂nnte sein Age &gt;= 18 oder Years &lt; 4.5. Es kommen mehrere Abbruchkriterium in Frage: Eine Mindestanzahl von Beobachtungen pro Knoten wird unterschritten (minsplit) Die maximale Anzahl an Ebenen ist erreicht (maxdepth) Die minimale Zahl an Beobachtungen eines Blatts wird unterschritten (minbucket) Der Gini-Koeffizient ist im Fall einer UV mit zwei Stufen, \\(c_A\\) und \\(c_B\\), so definiert: \\[G = 1 - \\left(p(c_A)^2 + (1-p(c_A))^2\\right)\\] Der Algorithmus ist ‚Äúgierig‚Äù (greedy): Optimiert werden lokal optimale Aufteilungen, auch wenn das bei sp√§teren Aufteilungen im Baum dann insgesamt zu geringerer Homogenit√§t f√ºhrt. Die Entropie ist definiert als \\[D = - \\sum_{k=1}^K p_k \\cdot log(p_k),\\] wobei \\(K\\) die Anzahl der Kategorien indiziert. Gini-Koeffizient und Entropie kommen oft zu √§hnlichen numerischen Ergebnissen, so dass wir uns im Folgenden auf den Gini-Koeffizienten konzentieren werden. Beispiel Vergleichen wir drei Bedingungen mit jeweils \\(n=20\\) F√§llen, die zu unterschiedlich homogenen Knoten f√ºhren: 10/10 15/5 19/1 Was ist jeweils der Wert des Gini-Koeffizienten? G1 &lt;- 1 - ((10/20)^2 + (10/20)^2) G1 ## [1] 0.5 G2 &lt;- 1 - ((15/20)^2 + (5/20)^2) G2 ## [1] 0.375 G3 &lt;- 1 - ((19/20)^2 + (1/20)^2) G3 ## [1] 0.095 Wie man sieht, sinkt der Wert des Gini-Koeffizienten (‚ÄúG-Wert‚Äù), je homogener die Verteilung ist. Maximal heterogen (‚Äúgemischt‚Äù) ist die Verteilung, wenn alle Werte gleich oft vorkommen, in diesem Fall also 50%/50%. Neben dem G-Wert f√ºr einzelne Knoten kann man den G-Wert f√ºr eine Aufteilung (‚ÄúSplit‚Äù) berechnen, also die Fraeg beantworten, ob die Aufteilung eines Knoten in zwei zu mehr Homogenit√§t f√ºhrt. Der G-Wert einer Aufteilung ist die gewichtete Summe der G-Werte der beiden Knoten (links, \\(l\\) und rechts, \\(r\\)): \\[G_{split} = p(l) G_{l} + p(r) G_r\\] Der Gewinn (gain) an Homogenit√§t ist dann die Differenz des G-Werts der kleineren Ebene und der Aufteilung: \\[G_{gain} = G - G_{split}\\] Der Algorithmus kann auch bei UV mit mehr als zwei, also \\(K\\) Stufen, \\(c_1, c_2, \\ldots, c_K\\) verwendet werden: \\[G= 1- \\sum_{k=1}^K p(c_k)^2\\] 10.6 Metrische Pr√§diktoren Au√üerdem ist es m√∂glich, Bedingung bei metrischen UV auf ihre Homogenit√§t hin zu bewerten, also Aufteilungen der Art Years &lt; 4.5 zu t√§tigen. Dazu muss man einen Wert identifieren, bei dem man auftrennt. Das geht in etwa so: Sortiere die Werte eines Pr√§diktors (aufsteigend) F√ºr jedes Paar an aufeinanderfolgenden Werten berechne den G-Wert Finde das Paar mit dem h√∂chsten G-Wert aus allen Paaren Nimm den Mittelwert der beiden Werte dieses Paares: Das ist der Aufteilungswert Abbildung 10.5 stellt dieses Vorgehen schematisch dar (Rhys 2020). Figure 10.5: Aufteilungswert bei metrischen Pr√§diktoren 10.7 Regressionb√§ume Bei Regressionsb√§umen wird nicht ein Homogenit√§tsma√ü wie der Gini-Koeffizient als Optimierungskriterium herangezogen, sondern die RSS (Residual Sum of Squares) bietet sich an. Die \\(J\\) Regionen (Partionierungen) des Pr√§diktorraums \\(R_1, R_2, \\ldots, R_J\\) m√ºssen so gew√§hlt werden, dass RSS minimal ist: \\[RSS = \\sum^J_{j=1}\\sum_{i\\in R_j}(u_i - \\hat{y}_{R_j})^2,\\] wobei \\(\\hat{y}\\) der (vom Baum) vorhergesagte Wert ist f√ºr die \\(j\\)-te Region. 10.8 Baum beschneiden Ein Problem mit Entscheidungsb√§umen ist, dass ein zu komplexer Baum, ‚Äúzu ver√§stelt‚Äù sozusagen, in hohem Ma√üe Overfitting ausgesetzt ist: Bei h√∂heren Ebenen im Baum ist die Anzahl der Beobachtungen zwangsl√§ufig klein, was bedeutet, dass viel Rauschen gefittet wird. Um das Overfitting zu vermeiden, gibt es zwei auf der Hand liegende Ma√ünahmen: Den Baum nicht so gro√ü werden lassen Den Baum ‚Äúzur√ºckschneiden‚Äù Die 1. Ma√ünahme beruht auf dem Festlegen einer maximalen Zahl an Ebenen (maxdepth) oder einer minimalen Zahl an F√§llen pro Knoten (minsplit) oder im Blatt (minbucket). Die 2. Ma√ünahme, das Zur√ºckschneiden (pruning) des Baumes hat als Idee, einen ‚ÄúTeilbaum‚Äù \\(T\\) zu finden, der so klein wie m√∂glich ist, aber so gut wie m√∂glich pr√§zise Vorhersagen erlaubt. Dazu belegen wir die RSS eines Teilbaums (subtree) mit einem Strafterm \\(s = \\alpha |T|\\), wobei \\(|T|\\) die Anzahl der Bl√§tter des Baums entspricht. \\(\\alpha\\) ist ein Tuningparameter, also ein Wert, der nicht vom Modell berechnet wird, sondern von uns gesetzt werden muss - zumeist durch schlichtes Ausprobieren. \\(\\alpha\\) w√§gt ab zwischen Komplexit√§t und Fit (geringe RSS). Wenn \\(\\alpha=0\\) haben wir eine normalen, unbeschnittenen Baum \\(T_0\\). Je gr√∂√üer \\(\\alpha\\) wird, desto h√∂her wird der ‚ÄúPreis‚Äù f√ºr viele Bl√§tter, also f√ºr Komplexit√§t und der Baum wird kleiner. Dieses Vorgehen nennt man auch cost complexity pruning. Daher nennt man den zugeh√∂rigen Tuningparameter auch Cost Complexity \\(C_p\\). 10.9 Das Rechteck schl√§gt zur√ºck Entscheidungsb√§ume zeichnen sich durch rechtecke (rekursive) Partionierungen des Pr√§diktorenraums aus. Lineare Modelle durch eine einfache lineare Partionierung (wenn man Klassifizieren m√∂chte), Abb. 10.6 verdeutlicht diesen Unterschied (James et al. 2021). Figure 10.6: Rechteckige vs.¬†lineare Partionierung Jetzt kann sich fragen: Welches Vorgehen ist besser - das rechteckige oder das lineare Partionierungen. Da gibt es eine klare Antwort: Es kommt drauf an. Wie Abb. 10.6 gibt es Datenlagen, in denen das eine Vorgehen zu homogenerer Klassifikation f√ºhrt und Situationen, in denen das andere Vorgehen besser ist, vgl. Abb. 10.7. Figure 10.7: Free Lunch? 10.10 Tidymodels Probieren wir den Algorithmus Entscheidungsb√§ume an einem einfachen Beispiel in R mit Tidymodels aus. Die Aufgabe sei, Spritverbrauch (m√∂glichst exakt) vorzusagen. 10.10.1 Initiale Datenaufteilung library(tidymodels) data(&quot;mtcars&quot;) set.seed(42) # Reproduzierbarkeit d_split &lt;- initial_split(mtcars, strata = mpg) ## Warning: The number of observations in each quantile is below the recommended threshold of 20. ## ‚Ä¢ Stratification will use 1 breaks instead. ## Warning: Too little data to stratify. ## ‚Ä¢ Resampling will be unstratified. d_train &lt;- training(d_split) d_test &lt;- testing(d_split) Die Warnung zeigt uns, dass der Datensatz sehr klein ist; stimmt. Ignorieren wir hier einfach. Wie man auf der Hilfeseite der Funktion sieht, wird per Voreinstellung 3/1 aufgeteilt, also 75% in das Train-Sample, 25% der Daten ins Test-Sample. Bei \\(n=32\\) finden also 8 Autos ihren Weg ins Test-Sample und die √ºbrigen 24 ins Train-Sample. Bei der kleinen Zahl k√∂nnte man sich (berechtigterweise) fragen, ob es Sinn macht, die sp√§rlichen Daten noch mit einem Test-Sample weiter zu dezimieren. Der Einwand ist nicht unberechtigt, allerdings zieht der Verzicht auf ein Test-Sample andere Probleme, Overfitting namentlich, nach sich. 10.10.2 Kreuzvalidierung definieren d_cv &lt;- vfold_cv(d_train, strata = mpg, repeats = 5, folds = 5) d_cv ## # 10-fold cross-validation repeated 5 times using stratification ## # A tibble: 50 √ó 3 ## splits id id2 ## &lt;list&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;split [21/3]&gt; Repeat1 Fold01 ## 2 &lt;split [21/3]&gt; Repeat1 Fold02 ## 3 &lt;split [21/3]&gt; Repeat1 Fold03 ## 4 &lt;split [21/3]&gt; Repeat1 Fold04 ## 5 &lt;split [22/2]&gt; Repeat1 Fold05 ## 6 &lt;split [22/2]&gt; Repeat1 Fold06 ## 7 &lt;split [22/2]&gt; Repeat1 Fold07 ## 8 &lt;split [22/2]&gt; Repeat1 Fold08 ## 9 &lt;split [22/2]&gt; Repeat1 Fold09 ## 10 &lt;split [22/2]&gt; Repeat1 Fold10 ## # ‚Ä¶ with 40 more rows Die Defaults (Voreinstellungen) der Funktion vfold_cv() k√∂nnen, wie immer, auf der Hilfeseite der Funktion nachgelesen werden. Da die Stichprobe sehr klein ist, bietet es sich an, eine kleine Zahl an Faltungen (folds) zu w√§hlen. Bei 10 Faltungen beinhaltete eine Stichprobe gerade 10% der F√§lle in Train-Sample, also etwa ‚Ä¶ 2! 10.10.3 Rezept definieren Hier ein einfaches Rezept: recipe1 &lt;- recipe(mpg ~ ., data = d_train) %&gt;% step_impute_knn() %&gt;% step_normalize() %&gt;% step_dummy() %&gt;% step_other(threshold = .1) 10.10.4 Modell definieren tree_model &lt;- decision_tree( cost_complexity = tune(), tree_depth = tune(), min_n = tune() ) %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;regression&quot;) Wenn Sie sich fragen, woher Sie die Optionen f√ºr die Tuningparameter wissen sollen: Schauen Sie mal in die Hilfeseite des Pakets {{dials}}; das Paket ist Teil von Tidymodels. Die Berechnung des Modells l√§uft √ºber das Paket {{rpart}}, was wir durch set_engine() festgelegt haben. Der Parameter Cost Complexity, \\(C_p\\) oder manchmal auch mit \\(\\alpha\\) bezeichnet, hat einen typischen Wertebereich von \\(10-^{10}\\) bis \\(10^{-1}\\): cost_complexity() ## Cost-Complexity Parameter (quantitative) ## Transformer: log-10 [1e-100, Inf] ## Range (transformed scale): [-10, -1] Hier ist der Wert in Log-Einheiten angegeben. Wenn Sie sich fragen, woher Sie das bittesch√∂n wissen sollen: Naja, es steht auf der Hilfeseite üòÑ. Unser Modell ist also so definiert: tree_model ## Decision Tree Model Specification (regression) ## ## Main Arguments: ## cost_complexity = tune() ## tree_depth = tune() ## min_n = tune() ## ## Computational engine: rpart Mit tune() weist man den betreffenden Parameter als ‚Äúzu tunen‚Äù aus - gute Werte sollen durch Ausprobieren w√§hrend des Berechnens bestimmt werden. Genauer gesagt soll das Modell f√ºr jeden Wert (oder jede Kombination an Werten von Tuningparametern) berechnet werden. Eine Kombination an Tuningparameter-Werten, die ein Modell spezifizieren, sozusagen erst ‚Äúfertig definieren‚Äù, nennen wir einen Modellkandidaten. Definieren wir also eine Tabelle (grid) mit Werten, die ausprobiert, ‚Äúgetuned‚Äù werden sollen. Wir haben oben dre Tuningparameter bestimmt. Sagen wir, wir h√§tten gerne jeweils 5 Werte pro Parameter. tree_grid &lt;- grid_regular( cost_complexity(), tree_depth(), min_n(), levels = 4 ) F√ºr jeden Parameter sind Wertebereiche definiert; dieser Wertebereich wird gleichm√§√üig (daher grid regular) aufgeteilt; die Anzahl der verschiedenen Werte pro Parameter wird druch levels gegeben. Mehr dazu findet sich auf der Hilfeseite zu grid_regular(). Wenn man die alle miteinander durchprobiert, entstehen \\(4^3\\) Kombinationen, also Modellkandidaten. Allgemeiner gesagt sind das bei \\(n\\) Tuningparametern mit jeweils \\(m\\) verschiedenen Werten \\(m^n\\) M√∂glichkeiten, spricht Modellkandidaten. Um diesen Faktor erh√∂ht sich die Rechenzeit im Vergleich zu einem Modell ohne Tuning. Man sieht gleich, dass die Rechenzeit schnell unangenehm lang werden kann. Entsprechend hat unsere Tabelle diese Zahl an Zeilen. Jede Zeile definiert einen Modellkandidaten, also eine Berechnung des Modells. dim(tree_grid) ## [1] 64 3 head(tree_grid) ## # A tibble: 6 √ó 3 ## cost_complexity tree_depth min_n ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 0.0000000001 1 2 ## 2 0.0000001 1 2 ## 3 0.0001 1 2 ## 4 0.1 1 2 ## 5 0.0000000001 5 2 ## 6 0.0000001 5 2 Man beachte, dass au√üer Definitionen bisher nichts passiert ist ‚Äì vor allem haben wir noch nichts berechnet. Sie scharren mit den Hufen? Wollen endlich loslegen? Also gut. 10.10.5 Workflow definieren Fast vergessen: Wir brauchen noch einen Workflow. tree_wf &lt;- workflow() %&gt;% add_model(tree_model) %&gt;% add_recipe(recipe1) 10.10.6 Modell tunen und berechnen Achtung: Das Modell zu berechnen kann etwas dauern. Es kann daher Sinn machen, das Modell abzuspeichern, so dass Sie beim erneuten Durchlaufen nicht nochmal berechnen m√ºssen, sondern einfach von der Festplatte laden k√∂nnen; das setzt nat√ºrlich voraus, dass sich am Modell nichts ge√§ndert hat. doParallel::registerDoParallel() set.seed(42) tic() trees_tuned &lt;- tune_grid( object = tree_wf, grid = tree_grid, resamples = d_cv ) toc() Es bietet sich in dem Fall an, ein Objekt als R Data serialized (rds) abzuspeichern: write_rds(trees_tuned, &quot;objects/trees1.rds&quot;) Bzw. so wieder aus der RDS-Datei zu importieren: trees_tuned &lt;- read_rds(&quot;objects/trees1.rds&quot;) trees_tuned ## # Tuning results ## # 10-fold cross-validation repeated 10 times using stratification ## # A tibble: 100 √ó 5 ## splits id id2 .metrics .notes ## &lt;list&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [21/3]&gt; Repeat01 Fold01 &lt;tibble [250 √ó 7]&gt; &lt;tibble [50 √ó 3]&gt; ## 2 &lt;split [21/3]&gt; Repeat01 Fold02 &lt;tibble [250 √ó 7]&gt; &lt;tibble [50 √ó 3]&gt; ## 3 &lt;split [21/3]&gt; Repeat01 Fold03 &lt;tibble [250 √ó 7]&gt; &lt;tibble [51 √ó 3]&gt; ## 4 &lt;split [21/3]&gt; Repeat01 Fold04 &lt;tibble [250 √ó 7]&gt; &lt;tibble [50 √ó 3]&gt; ## 5 &lt;split [22/2]&gt; Repeat01 Fold05 &lt;tibble [250 √ó 7]&gt; &lt;tibble [51 √ó 3]&gt; ## 6 &lt;split [22/2]&gt; Repeat01 Fold06 &lt;tibble [250 √ó 7]&gt; &lt;tibble [51 √ó 3]&gt; ## 7 &lt;split [22/2]&gt; Repeat01 Fold07 &lt;tibble [250 √ó 7]&gt; &lt;tibble [51 √ó 3]&gt; ## 8 &lt;split [22/2]&gt; Repeat01 Fold08 &lt;tibble [250 √ó 7]&gt; &lt;tibble [50 √ó 3]&gt; ## 9 &lt;split [22/2]&gt; Repeat01 Fold09 &lt;tibble [250 √ó 7]&gt; &lt;tibble [51 √ó 3]&gt; ## 10 &lt;split [22/2]&gt; Repeat01 Fold10 &lt;tibble [250 √ó 7]&gt; &lt;tibble [50 √ó 3]&gt; ## # ‚Ä¶ with 90 more rows ## ## There were issues with some computations: ## ## - Warning(s) x1000: 30 samples were requested but there were 21 rows in the data. 21 ... - Warning(s) x1500: 30 samples were requested but there were 21 rows in the data. 21 ... - Warning(s) x1000: 30 samples were requested but there were 21 rows in the data. 21 ... - Warning(s) x1500: 30 samples were requested but there were 21 rows in the data. 21 ... - Warning(s) x45: 30 samples were requested but there were 21 rows in the data. 21 ... ## ## Use `collect_notes(object)` for more information. Die Warnhinweise kann man sich so ausgeben lassen: collect_notes(trees_tuned) ## # A tibble: 5,045 √ó 5 ## id id2 location type note ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Repeat01 Fold01 preprocessor 1/1, model 76/125 warning 30 samples were reque‚Ä¶ ## 2 Repeat01 Fold01 preprocessor 1/1, model 77/125 warning 30 samples were reque‚Ä¶ ## 3 Repeat01 Fold01 preprocessor 1/1, model 78/125 warning 30 samples were reque‚Ä¶ ## 4 Repeat01 Fold01 preprocessor 1/1, model 79/125 warning 30 samples were reque‚Ä¶ ## 5 Repeat01 Fold01 preprocessor 1/1, model 80/125 warning 30 samples were reque‚Ä¶ ## 6 Repeat01 Fold01 preprocessor 1/1, model 81/125 warning 30 samples were reque‚Ä¶ ## 7 Repeat01 Fold01 preprocessor 1/1, model 82/125 warning 30 samples were reque‚Ä¶ ## 8 Repeat01 Fold01 preprocessor 1/1, model 83/125 warning 30 samples were reque‚Ä¶ ## 9 Repeat01 Fold01 preprocessor 1/1, model 84/125 warning 30 samples were reque‚Ä¶ ## 10 Repeat01 Fold01 preprocessor 1/1, model 85/125 warning 30 samples were reque‚Ä¶ ## # ‚Ä¶ with 5,035 more rows Wie gesagt, in diesem Fall war die Stichprobengr√∂√üe sehr klein. 10.10.7 Modellg√ºte evaluieren collect_metrics(trees_tuned) ## # A tibble: 250 √ó 9 ## cost_complexity tree_depth min_n .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.0000000001 1 2 rmse standard 3.31 100 0.161 ## 2 0.0000000001 1 2 rsq standard 0.881 55 0.0284 ## 3 0.0000000178 1 2 rmse standard 3.31 100 0.161 ## 4 0.0000000178 1 2 rsq standard 0.881 55 0.0284 ## 5 0.00000316 1 2 rmse standard 3.31 100 0.161 ## 6 0.00000316 1 2 rsq standard 0.881 55 0.0284 ## 7 0.000562 1 2 rmse standard 3.31 100 0.161 ## 8 0.000562 1 2 rsq standard 0.881 55 0.0284 ## 9 0.1 1 2 rmse standard 3.31 100 0.161 ## 10 0.1 1 2 rsq standard 0.881 55 0.0284 ## # ‚Ä¶ with 240 more rows, and 1 more variable: .config &lt;chr&gt; Praktischerweise gibt es eine Autoplot-Funktion, um die besten Modellparameter auszulesen: autoplot(trees_tuned) 10.10.8 Bestes Modell ausw√§hlen Aus allen Modellkandidaten w√§hlen wir jetzt das beste Modell aus: select_best(trees_tuned) ## # A tibble: 1 √ó 4 ## cost_complexity tree_depth min_n .config ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 0.0000000001 4 2 Preprocessor1_Model006 Mit diesem besten Kandidaten definieren wir jetzt das ‚Äúfinale‚Äù Modell, wir ‚Äúfinalisieren‚Äù das Modell mit den besten Modellparametern: tree_final &lt;- finalize_model(tree_model, parameters = select_best(trees_tuned)) tree_final ## Decision Tree Model Specification (regression) ## ## Main Arguments: ## cost_complexity = 1e-10 ## tree_depth = 4 ## min_n = 2 ## ## Computational engine: rpart Hier ist, unser finaler Baum üå≥. Schlie√ülich updaten wir mit dem finalen Baum noch den Workflow: final_wf &lt;- tree_wf %&gt;% update_model(tree_final) 10.10.9 Final Fit Jetzt fitten wir dieses Modell auf das ganze Train-Sample und predicten auf das Test-Sample: fit_final &lt;- final_wf %&gt;% last_fit(d_split) fit_final ## # Resampling results ## # Manual resampling ## # A tibble: 1 √ó 6 ## splits id .metrics .notes .predictions .workflow ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [24/8]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble [8 √ó 4]&gt; &lt;workflow&gt; collect_metrics(fit_final) ## # A tibble: 2 √ó 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 3.90 Preprocessor1_Model1 ## 2 rsq standard 0.687 Preprocessor1_Model1 Voil√†: Die Modellg√ºte f√ºr das Test-Sample. References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
