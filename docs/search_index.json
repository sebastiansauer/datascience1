[["ensemble-lerner.html", "Kapitel 11 Ensemble Lerner 11.1 Lernsteuerung 11.2 Vorbereitung 11.3 Hinweise zur Literatur 11.4 Wir brauchen einen Wald 11.5 Was ist ein Ensemble-Lerner? 11.6 Bagging 11.7 Bagging-Algorithmus 11.8 Random Forests 11.9 Boosting 11.10 Tidymodels 11.11 Aufgaben", " Kapitel 11 Ensemble Lerner 11.1 Lernsteuerung 11.1.1 Lernziele Sie kÃ¶nnen Algorithmen fÃ¼r Ensemble-Lernen erklÃ¤ren, d.i. Bagging, AdaBoost, XGBoost, Random Forest Sie wissen, anhand welche Tuningparamter man Overfitting bei diesen Algorithmen begrenzen kann Sie kÃ¶nnen diese Verfahren in R berechnen 11.1.2 Literatur Rhys, Kap. 8 11.2 Vorbereitung In diesem Kapitel werden folgende R-Pakete benÃ¶tigt: library(tidymodels) library(tictoc) # Zeitmessung library(vip) # Variable importance plot 11.3 Hinweise zur Literatur Die folgenden AusfÃ¼hrungen basieren primÃ¤r auf Rhys (2020), aber auch auf James et al. (2021) und (weniger) Kuhn and Johnson (2013). 11.4 Wir brauchen einen Wald Ein Pluspunkt von EntscheidungsbÃ¤umen ist ihre gute Interpretierbarkeit. Man kÃ¶nnte behaupten, dass BÃ¤ume eine typische Art des menschlichen Entscheidungsverhalten nachahmen: â€œWenn A, dann tue B, ansonsten tue Câ€ (etc.). Allerdings: Einzelne EntscheidungsbÃ¤ume haben oft keine so gute Prognosegenauigkeit. Der oder zumindest ein Grund ist, dass sie (zwar wenig Bias aber) viel Varianz aufweisen. Das sieht man z.B. daran, dass die Vorhersagegenauigkeit stark schwankt, wÃ¤hlt man eine andere Aufteilung von Train- vs.Â Test-Sample. Anders gesagt: BÃ¤ume overfitten ziemlich schnell. Und obwohl das No-Free-Lunch-Theorem zu den Grundfesten des maschinellen Lernens (oder zu allem wissenschaftlichen Wissen) gehÃ¶rt, kann man festhalten, dass sog. Ensemble-Lernen fast immer besser sind als einzelne Baummodelle. Kurz gesagt: Wir brauchen einen Wald: ğŸŒ³ğŸŒ³ğŸŒ³1 11.5 Was ist ein Ensemble-Lerner? Ensemble-Lerner kombinieren mehrere schwache Lerner zu einem starken Lerner. Das Paradebeispiel sind baumbasierte Modelle; darauf wird sich die folgende AusfÃ¼hrung auch begrenzen. Aber theoretisch kann man jede Art von Lerner kombinieren. Bei numerischer PrÃ¤diktion wird bei Ensemble-Lerner zumeist der Mittelwert als Optmierungskriterium herangezogen; bei Klassifikation (nominaler PrÃ¤diktion) hingegen die modale Klasse (also die hÃ¤ufigste). Warum hilft es, mehrere Modelle (Lerner) zu einem zu aggregieren? Die Antwort lautet, dass die Streuung der Mittelwerte sinkt, wenn die StichprobengrÃ¶ÃŸe steigt. Zieht man Stichproben der GrÃ¶ÃŸe 1, werden die Mittelwerte stark variieren, aber bei grÃ¶ÃŸeren Stichproben (z.B. GrÃ¶ÃŸe 100) deutlich weniger2. Die Streuung der Mittelwerte in den Stichproben nennt man bekanntlich Standardefehler (se). Den se des Mittelwerts (\\(se_M\\)) fÃ¼r eine normalverteilte Variable \\(X \\sim \\mathcal{N}(\\mu, \\sigma)\\) gilt: \\(se_{M} = \\sigma / \\sqrt(n)\\), wobei \\(\\sigma\\) die SD der Verteilung und \\(\\mu\\) den Erwartungswert (â€œMittelwertâ€) meint, und \\(n\\) ist die StichprobengrÃ¶ÃŸe. Je grÃ¶ÃŸer die Stichprobe, desto kleiner die Varianz des SchÃ¤tzers (ceteris paribus). Anders gesagt: GrÃ¶ÃŸere Stichproben schÃ¤tzen genauer als kleine Stichproben. Aus diesem Grund bietet es sich an, schwache Lerner mit viel Varianz zu kombinieren, da die Varianz so verringert wird. 11.6 Bagging 11.6.1 Bootstrapping Das erste baumbasierte Modell, was vorgestellt werden soll, basiert auf sog. Bootstrapping, ein Standardverfahren in der Statistik (James et al. 2021). Bootstrapping ist eine Nachahmung fÃ¼r folgende Idee: HÃ¤tte man viele Stichproben aus der relevanten Verteilung, so kÃ¶nnte man z.B. die Genauigkeit eines Modells \\(\\hat{f}_{\\bar{X}}\\) zur SchÃ¤tzung des Erwartungswertes \\(\\mu\\) einfach dadurch bestimmen, indem man se berechnet, also die Streuung der Mitterwerte \\(\\bar{X}\\) berechnet. AuÃŸerdem gilt, dass die PrÃ¤zision der SchÃ¤tzung des Erwartungswerts steigt mit steigendem Stichprobenumfang \\(n\\). Wir kÃ¶nnten also fÃ¼r jede der \\(B\\) Stichproben, \\(b=1,\\ldots, B\\), ein (Baum-)Modell berechnen und dann deren Vorhersagen aggregieren (zum Mittelwert oder Modalwert). Das kann man formal so darstellen (James et al. 2021): \\[\\hat{f}_{\\bar{X}} = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{f}^b\\] Mit diesem Vorgehen kann die Varianz des Modells \\(\\hat{f}_{\\bar{X}}\\) verringert werden; die Vorhersagegenauigkeit steigt. Leider haben wir in der Regel nicht viele (\\(B\\)) DatensÃ¤tze. Daher â€œbauenâ€ wir uns aus dem einzelnen Datensatz, der uns zur VerfÃ¼gung steht, viele DatensÃ¤tze. Das hÃ¶rt sich nach â€œtoo good to be trueâ€ an3 Weil es sich unglaubwÃ¼rdig anhÃ¶rt, nennt man das entsprechende Verfahren (gleich kommt es!) auch â€œMÃ¼nchhausen-Methodeâ€, nach dem berÃ¼hmten LÃ¼bgenbaron. Die Amerikaner ziehen sich Ã¼brigens nicht am Schopf aus dem Sumpf, sondern mit den Stiefelschlaufen (die Cowboys wieder), daher spricht man im Amerikanischen auch von der â€œBoostrapping-Methodeâ€. Diese â€œPseudo-Stichprobenâ€ oder â€œBootstrapping-Stichprobenâ€ sind aber recht einfach zu gewinnen.. Gegeben sei Stichprobe der GrÃ¶ÃŸe \\(n\\): Ziehe mit ZurÃ¼cklegen (ZmZ) aus der Stichprobe \\(n\\) Beobachtungen Fertig ist die Bootstrapping-Stichprobe. Abb. 11.1 verdeutlicht das Prinzip des ZMZ, d.h. des Bootstrappings. Wie man sieht, sind die Bootstrap-Stichproben (rechts) vom gleichen Umfang \\(n\\) wie die Originalstichprobe (links). Allerdins kommen nicht alle FÃ¤lle (in der Regel) in den â€œBoostrap-Beutelâ€ (in bag), sondern einige FÃ¤lle werden oft mehrfach gezogen, so dass einige FÃ¤lle nicht gezogen werden (out of bag). Figure 11.1: Bootstrapping: Der Topf links symbolisiert die Original-Stichprobe, aus der wir hier mehrere ZMZ-Stichproben ziehen (Rechts), dargestellt mit â€˜in bagâ€™ Man kann zeigen, dass ca. 2/3 der FÃ¤lle gezogen werden, bzw. ca. 1/3 nicht gezogen werden. Die nicht gezogenen FÃ¤lle nennt man auch out of bag (OOB). FÃ¼r die Entwicklung des Bootstrapping wurde der Autor, Bradley Efron, im Jahr 2018 mit dem internationalen Preis fÃ¼r Statistik ausgezeichnet; â€œWhile statistics offers no magic pill for quantitative scientific investigations, the bootstrap is the best statistical pain reliever ever produced,â€ says Xiao-Li Meng, Whipple V. N. Jones Professor of Statistics at Harvard University.â€œ 11.7 Bagging-Algorithmus Bagging, die Kurzform fÃ¼r Bootstrap-Aggregation ist wenig mehr als die Umsetzung des Boostrappings. Der Algorithmus von Bagging kann so beschrieben werden: WÃ¤hle \\(B\\), die Anzahl der Boostrap-Stichproben und damit auch Anzahl der Submodelle (Lerner) Ziehe \\(B\\) Boostrap-Stichproben Berechne das Modell \\(\\hat{f}^{*b}\\) fÃ¼r jede der \\(B\\) Stichproben (typischerweise ein einfacher Baum) Schicke die Test-Daten durch jedes Sub-Modell Aggregiere ihre Vorhersage zu einem Wert (Modus bzw. Mittelwert) pro Fall aus dem Test-Sample, zu \\(\\hat{f}_{\\text{bag}}\\) Anders gesagt: \\[\\hat{f}_{\\text{bag}} = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{f}^{*b}\\] Der Bagging-Algorithmus ist in Abbildung 11.2 dargestellt. Figure 11.2: Bagging schematisch illustriert Die Anzahl der BÃ¤ume (allgemeiner: Submodelle) \\(B\\) ist hÃ¤ufig im oberen drei- oder niedrigem vierstelligen Bereich, z.B. \\(B=1000\\). Eine gute Nachricht ist, dass Bagging nicht Ã¼beranpasst, wenn \\(B\\) groÃŸ wird. 11.7.1 Variablenrelevanz Man kann die Relevanz der PrÃ¤diktoren in einem Bagging-Modell auf mehrere Arten schÃ¤tzen. Ein Weg (bei numerischer PrÃ¤diktion) ist, dass man die RSS-Verringerung, die durch Aufteilung anhand eines PrÃ¤diktors erzeugt wird, mittelt Ã¼ber alle beteiligten BÃ¤ume (Modelle). Bei Klassifikation kann man die analog die Reduktion des Gini-Wertes Ã¼ber alle BÃ¤ume mitteln und als SchÃ¤tzwert fÃ¼r die Relevanz des PrÃ¤diktors heranziehen. 11.7.2 Out of Bag Vorhersagen Da nicht alle FÃ¤lle der Stichprobe in das Modell einflieÃŸen (sondern nur ca. 2/3), kann der Rest der FÃ¤lle zur Vorhersage genutzt werden. Bagging erzeugt sozusagen innerhalb der Stichprobe selbstÃ¤ndig ein Train- und ein Test-Sample. Man spricht von Out-of-Bag-SchÃ¤tzung (OOB-SchÃ¤tzung). Der OOB-Fehler (z.B. MSE bei numerischen Modellen und Genauigkeit bei nominalen) ist eine valide SchÃ¤tzung des typischen Test-Sample-Fehlers. Hat man aber Tuningparameter, so wird man dennoch auf die typische Train-Test-Aufteilung zurÃ¼ckgreifen, um Overfitting durch das Ausprobieren der Tuning-Kandidaten zu vermeiden (was sonst zu Zufallstreffern fÃ¼hren wÃ¼rde bei genÃ¼gend vielen Modellkandidaten). 11.8 Random Forests Random Forests (â€œZufallswÃ¤lderâ€) sind eine Weiterentwicklung von Bagging-Modellen. Sie sind Bagging-Modelle, aber haben noch ein Ass im Ã„rmel: Und zwar wird an jedem Slit (Astgabel, Aufteilung) nur eine Zufallsauswahl an \\(m\\) PrÃ¤diktoren berÃ¼cksichtigt. Das hÃ¶rt sich verrÃ¼ckt an: â€œWie, mit weniger PrÃ¤diktoren soll eine bessere Vorhersage erreicht werden?!â€ Ja, genau so ist es! Nehmen Sie an, es gibt im Datensatz einen sehr starken und ein paar mittelstarke PrÃ¤diktoren; der Rest der PrÃ¤diktoren ist wenig relevant. Wenn Sie jetzt viele â€œgebootstrapteâ€4 ziehen, werden diese BÃ¤ume sehr Ã¤hnlich sein: Der stÃ¤rkste PrÃ¤diktor steht vermutlich immer ob an der Wurzel, dann kommen die mittelstarken PrÃ¤diktoren. Jeder zusÃ¤tzliche Baum trÃ¤gt dann wenig neue Information bei. Anders gesagt: Die Vorhersagen der BÃ¤ume sind dann sehr Ã¤hnlich bzw. hoch korreliert. Bildet man den Mittelwert von hoch korrelierten Variablen, verringert sich leider die Varianzu nur wenig im Vergleich zu nicht oder gering korrelierten Variablen (James et al. 2021). Dadurch dass Random Forests nur \\(m\\) der \\(p\\) PrÃ¤diktoren pro Split zulassen, werden die BÃ¤ume unterschiedlicher. Wir â€œdekorrelierenâ€ die BÃ¤ume. Bildet man den Mittelwert von gering(er) korrelierten Variablen, so ist die Varianzreduktion hÃ¶her - und die Vohersage genauer. LÃ¤sst man pro Split \\(m=p\\) PrÃ¤diktoren zu, so gleicht Bagging dem Random Forest. Die Anzahl \\(m\\) der erlaubten PrÃ¤diktoren werden als Zufallstichprobe aus den \\(p\\) PrÃ¤diktoren des Datensatzes gezogen (ohne ZurÃ¼cklegen). \\(m\\) ist ein Tuningparameter; \\(m=\\sqrt(p)\\) ist ein beliebter Startwert. In den meisten Implementationen wird \\(m\\) mit mtry bezeichnet (so auch in Tidymodels). Der Random-Forest-Algorithmus ist in Abb. 11.3 illustriert. Figure 11.3: ZufallswÃ¤lder durch Ziehen mit ZurÃ¼cklegen (zmz) und Ziehen ohne ZurÃ¼cklegen (ZoZ) Abb. 11.4 vergleicht die Test-Sample-VorhersagegÃ¼te von Bagging- und Random-Forest-Algorithmen aus James et al. (2021). In diesem Fall ist die VorhersagegÃ¼te deutlich unter der OOB-GÃ¼te; laut James et al. (2021) ist dies hier â€œZufallâ€. Figure 11.4: Test-Sample-VorhersagegÃ¼te von Bagging- und Random-Forest-Algorithmen Den Effekt von \\(m\\) (Anzahl der PrÃ¤diktoren pro Split) ist in Abb. 11.5 dargestellt (James et al. 2021). Man erkennt, dass der Zusatznutzen an zusÃ¤tzlichen BÃ¤umen, \\(B\\), sich abschwÃ¤cht. \\(m=\\sqrt{p}\\) schneidet wie erwartet am besten ab. Figure 11.5: Test-Sample-VorhersagegÃ¼te von Bagging- und Random-Forest-Algorithmen 11.9 Boosting Im Unterschied zu Bagging und Random-Forest-Modellen wird beim Boosting der â€œWaldâ€ sequenziell entwickelt, nicht gleichzeitig wie bei den anderen vorgestellten â€œWald-Modellenâ€. Die zwei bekanntesten Implementierungen bzw. Algorithmus-Varianten sind AdaBoost und XGBoost. Gerade XGBoost hat den Ruf, hervorragende Vorhersagen zu leisten. Auf Kaggle gewinnt nach einigen Berichten oft XGBoost. Nur neuronale Netze schneiden besser ab. Random-Forest-Modelle kommen nach diesem Bereich auf Platz 3. Allerdings benÃ¶tigen neuronale Netzen oft riesige StichprobengrÃ¶ÃŸen und bei spielen ihre Nuanciertheit vor allem bei komplexen Daten wie Bildern oder Sprache aus. FÃ¼r â€œrechteckigeâ€ Daten (also aus einfachen, normalen Tabellen) wird ein baumbasiertes Modell oft besser abschneiden. Die Idee des Boosting ist es, anschaulich gesprochen, aus Fehlern zu lernen: Fitte einen Baum, schau welche FÃ¤lle er schlecht vorhergesagt hat, konzentriere dich beim nÃ¤chsten Baum auf diese FÃ¤lle und so weiter. Wie andere Ensemble-Methoden auch kann Boosting theoretisch fÃ¼r beliebige Algorithmen eingesetzt werden. Es macht aber Sinn, Boosting bei â€œschwachen Lernernâ€ einzusetzen. Typisches Beispiel ist ein einfacher Baum; â€œeinfachâ€ soll heiÃŸen, der Baum hat nur wenig Gabeln oder vielleicht sogar nur eine einzige. Dann spricht man von einem Stumpf, was intuitiv gut passt. 11.9.1 AdaBoost Der AdaBoost-Algorithmus funktioniert, einfach dargestellt, wie folgt. Zuerst hat jeder Fall im Datensatz des gleiche Gewicht. Die erste (und alle weiteren) Stichprobe werden per Bootstrapping aus dem Datensatz gezogen. Dabei ist die Wahrscheinlichkeit, gezogen zu werden, proportional zum Gewicht. Da im ersten Durchgang die Gewichte identisch sind, haben zunÃ¤chst alle FÃ¤lle die gleiche Wahrscheinlichkeit, in das Bootstrap-Sample gezogen zu werden. Nach Berechnung des Baumen und der Vorhersagen werden die richtig klassifizierten FÃ¤lle heruntergewichtet und die falsch klassifizierten FÃ¤lle hoch gewichtet, also stÃ¤rker gewichtet (bleiben wir aus GrÃ¼nden der Einfachheit zunÃ¤chst bei der Klassifikation). Dieses Vorgehen folgt dem Gedanken, dass man sich seine Fehler genauer anschauen muss, die falsch klassifizierten FÃ¤lle sozusagen mehr Aufmerksamkeit bedÃ¼rfen. Das nÃ¤chste (zweite) Modell zieht ein weiteres Bootstrap-Sample. Jetzt sind allerdings die Gewichte schon angepasst, so dass mehr FÃ¤lle, die im vorherigen Modell falsch klassifiziert wurden, in den neuen (zweiten) Baum gezogen werden. Das neue Modell hat also bessere Chancen, die Aspekte, die das VorgÃ¤nger-Modell Ã¼bersah zu korrigieren bzw. zu lernen. Jetzt haben wir zwei Modelle. Die kÃ¶nnen wir aggregieren, genau wie beim Bagging: Der Modus der Vorhersage Ã¼ber alle (beide) BÃ¤ume hinwig ist dann die Vorhersage fÃ¼r einen bestimmten Fall (â€œFallâ€ und â€œBeobachtungâ€ sind stets synonym fÃ¼r \\(y_i\\) zu verstehen). So wiederholt sich das Vorgehen: Die Gewichte werden angepasst, das neue Modell wird berechnet, alle Modelle machen ihre Vorhersagen, per Mehrheitsbeschluss wird die Vorhersage bestimmt pro Fall. Irgendwann erreichen wir die vorab definierte Maximalzahl an BÃ¤umen, \\(B\\), und das Modell kommt zu einem Ende. Da das Modell die Fehler seiner VorgÃ¤nger reduziert, wird der Bias im Gesamtmodell verringert. Da wir gleichzeitig auch Bagging vornehmen, wird aber die Varianz auch verringert. Klingt schon wieder (fast) nach Too-Good-to-be-True! Das Gewicht \\(w_i^b\\) des \\(b\\)ten Falls im \\(b\\)ten Modell von \\(B\\) berechnet sich wie folgt (Rhys 2020): \\[ w_i^b = \\begin{cases} w_i^{b-1} \\cdot e^{-\\text{model weight}} \\qquad \\text{wenn korrekt klassifiziert} \\\\ w_i^{b-1} \\cdot e^{\\text{model weight}} \\qquad \\text{wenn inkorrekt klassifiziert} \\\\ \\end{cases}\\] Das Modellgewicht \\(mw\\) berechnet sich dabei so (Rhys 2020): \\[mw = 0.5 \\cdot log\\left( \\frac{1-p(\\text{inkorrect})}{p(\\text{korrekt})} \\right) \\propto \\mathcal{L(p)} \\] Das Modellgewicht ist ein Faktor, dass schlechtere Modelle bestraft. 11.9.2 XGBoost XGBoost ist ein Gradientenverfahren, eine Methode also, die die Richtung des parziellen Ableitungskoeffizienten als Optimierungskriterium heranzieht. XGBoost ist Ã¤hnlich zu AdaBoost, nur dass Residuen modelliert werden, nicht \\(y\\). Die Vorhersagefehler von \\(\\hat{f}^b\\) werden die Zielvariable von \\(\\hat{f}^{b+1}\\). Ein Residuum ist der Vorhersagefehler, bei metrischen Modellen etwa RMSE, oder schlicht \\(r_i = y_i - \\hat{y}_i\\). Details finden sich z.B. hier, dem Original XGBoost-Paper (Chen and Guestrin 2016). Die hohe VorhersagegÃ¼te von Boosting-Modellen ist exemplarisch in Abb. 11.6 dargestellt (James et al. 2021, S. 358ff). Allerdings verwenden die Autoren Friedmans (2001) Gradient Boosting Machine, eine weitere Variante des Boosting . Figure 11.6: VorhersagegÃ¼te von Boosting und Random Forest 11.10 Tidymodels 11.10.1 Datensatz Churn Wir betrachten einen Datensatz zur Kundenabwanderung (Churn) aus dieser Quelle. churn_df &lt;- read_rds(&#39;https://gmudatamining.com/data/churn_data.rds&#39;) Ein Blick in die Daten churn_df %&gt;% head() %&gt;% gt::gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #kuhgnnqrfx .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #kuhgnnqrfx .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kuhgnnqrfx .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #kuhgnnqrfx .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #kuhgnnqrfx .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kuhgnnqrfx .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kuhgnnqrfx .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #kuhgnnqrfx .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #kuhgnnqrfx .gt_column_spanner_outer:first-child { padding-left: 0; } #kuhgnnqrfx .gt_column_spanner_outer:last-child { padding-right: 0; } #kuhgnnqrfx .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #kuhgnnqrfx .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #kuhgnnqrfx .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #kuhgnnqrfx .gt_from_md > :first-child { margin-top: 0; } #kuhgnnqrfx .gt_from_md > :last-child { margin-bottom: 0; } #kuhgnnqrfx .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #kuhgnnqrfx .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #kuhgnnqrfx .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kuhgnnqrfx .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #kuhgnnqrfx .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kuhgnnqrfx .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #kuhgnnqrfx .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #kuhgnnqrfx .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kuhgnnqrfx .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kuhgnnqrfx .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #kuhgnnqrfx .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kuhgnnqrfx .gt_sourcenote { font-size: 90%; padding: 4px; } #kuhgnnqrfx .gt_left { text-align: left; } #kuhgnnqrfx .gt_center { text-align: center; } #kuhgnnqrfx .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #kuhgnnqrfx .gt_font_normal { font-weight: normal; } #kuhgnnqrfx .gt_font_bold { font-weight: bold; } #kuhgnnqrfx .gt_font_italic { font-style: italic; } #kuhgnnqrfx .gt_super { font-size: 65%; } #kuhgnnqrfx .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } canceled_service enrollment_discount spouse_partner dependents phone_service internet_service online_security online_backup device_protection tech_support streaming_tv streaming_movies contract paperless_bill payment_method months_with_company monthly_charges late_payments yes no no no multiple_lines fiber_optic yes yes yes no no no one_year no credit_card 30 51.01440 3 yes no yes yes multiple_lines fiber_optic no yes yes yes yes no two_year yes electronic_check 39 80.42466 4 yes yes no no single_line fiber_optic no no no no yes yes month_to_month yes mailed_check 1 75.88737 3 yes no yes yes single_line fiber_optic yes no no no yes no two_year no credit_card 29 81.96467 3 yes yes no no single_line digital no no no no yes yes month_to_month yes bank_draft 9 101.34257 5 yes no yes no single_line fiber_optic yes yes no yes yes yes month_to_month no mailed_check 14 72.01285 4 11.10.2 Data Splitting Das Kreuzvalidieren fassen wir auch unter diesen Punkt. churn_split &lt;- initial_split(churn_df, prop = 0.75, strata = canceled_service) churn_training &lt;- churn_split %&gt;% training() churn_test &lt;- churn_split %&gt;% testing() churn_folds &lt;- vfold_cv(churn_training, v = 5) 11.10.3 Feature Engineering churn_recipe1 &lt;- recipe(canceled_service ~ ., data = churn_training) %&gt;% step_normalize(all_numeric(), -all_outcomes()) %&gt;% step_dummy(all_nominal(), -all_outcomes()) churn_recipe2 &lt;- recipe(canceled_service ~ ., data = churn_training) %&gt;% step_YeoJohnson(all_numeric(), -all_outcomes()) %&gt;% step_normalize(all_numeric(), -all_outcomes()) %&gt;% step_dummy(all_nominal(), -all_outcomes()) step_YeoJohnson() reduziert Schiefe in der Verteilung. 11.10.4 Modelle tree_model &lt;- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %&gt;% set_engine(&#39;rpart&#39;) %&gt;% set_mode(&#39;classification&#39;) rf_model &lt;- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %&gt;% set_engine(&#39;ranger&#39;) %&gt;% set_mode(&#39;classification&#39;) boost_model &lt;- boost_tree(mtry = tune(), min_n = tune(), trees = tune()) %&gt;% set_engine(&quot;xgboost&quot;, nthreads = parallel::detectCores()) %&gt;% set_mode(&quot;classification&quot;) glm_model &lt;- logistic_reg() 11.10.5 Workflows Wir definieren ein Workflow-Set: preproc &lt;- list(rec1 = churn_recipe1, rec2 = churn_recipe2) models &lt;- list(tree1 = tree_model, rf1 = rf_model, boost1 = boost_model, glm1 = glm_model) all_workflows &lt;- workflow_set(preproc, models) Infos zu workflow_set bekommt man wie gewohnt mit workflow_set. Im Standard werden alle Rezepte und Modelle miteinander kombiniert (cross = TRUE), also preproc * models Modelle gefittet. 11.10.6 Modelle berechnen mit Tuning, einzeln Wir kÃ¶nnten jetzt jedes Modell einzeln tunen, wenn wir wollen. 11.10.6.1 Baum tree_wf &lt;- workflow() %&gt;% add_model(tree_model) %&gt;% add_recipe(churn_recipe1) tic() tree_fit &lt;- tree_wf %&gt;% tune_grid( resamples = churn_folds, metrics = metric_set(roc_auc, sens, spec) ) toc() ## 17.51 sec elapsed Im Standard werden 10 Modellkandidaten getuned. tree_fit ## # Tuning results ## # 5-fold cross-validation ## # A tibble: 5 Ã— 4 ## splits id .metrics .notes ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2393/599]&gt; Fold1 &lt;tibble [30 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; ## 2 &lt;split [2393/599]&gt; Fold2 &lt;tibble [30 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; ## 3 &lt;split [2394/598]&gt; Fold3 &lt;tibble [30 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; ## 4 &lt;split [2394/598]&gt; Fold4 &lt;tibble [30 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; ## 5 &lt;split [2394/598]&gt; Fold5 &lt;tibble [30 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; show_best(tree_fit) ## # A tibble: 5 Ã— 9 ## cost_complexity tree_depth min_n .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.000419 15 28 roc_auc binary 0.917 5 0.00233 ## 2 0.00000000466 11 19 roc_auc binary 0.916 5 0.00345 ## 3 0.000000211 7 21 roc_auc binary 0.915 5 0.00268 ## 4 0.0000000271 12 13 roc_auc binary 0.906 5 0.00157 ## 5 0.00000522 5 3 roc_auc binary 0.904 5 0.00422 ## # â€¦ with 1 more variable: .config &lt;chr&gt; autoplot(tree_fit) 11.10.6.2 RF Was fÃ¼r Tuningparameter hat den der Algorithmus bzw. seine Implementierung? show_model_info(&quot;rand_forest&quot;) ## Information for `rand_forest` ## modes: unknown, classification, regression, censored regression ## ## engines: ## classification: randomForest, ranger, spark ## regression: randomForest, ranger, spark ## ## arguments: ## ranger: ## mtry --&gt; mtry ## trees --&gt; num.trees ## min_n --&gt; min.node.size ## randomForest: ## mtry --&gt; mtry ## trees --&gt; ntree ## min_n --&gt; nodesize ## spark: ## mtry --&gt; feature_subset_strategy ## trees --&gt; num_trees ## min_n --&gt; min_instances_per_node ## ## fit modules: ## engine mode ## ranger classification ## ranger regression ## randomForest classification ## randomForest regression ## spark classification ## spark regression ## ## prediction modules: ## mode engine methods ## classification randomForest class, prob, raw ## classification ranger class, conf_int, prob, raw ## classification spark class, prob ## regression randomForest numeric, raw ## regression ranger conf_int, numeric, raw ## regression spark numeric Da die Berechnung einiges an Zeit braucht, kann man das (schon frÃ¼her einmal berechnete) Ergebnisobjekt von der Festplatte lesen (sofern es existiert). Ansonsten berechnet man neu: if (file.exists(&quot;objects/rf_fit1.rds&quot;)){ rf_fit1 &lt;- read_rds(&quot;objects/rf_fit1.rds&quot;) } else { rf_wf1 &lt;- workflow() %&gt;% add_model(rf_model) %&gt;% add_recipe(churn_recipe1) tic() rf_fit1 &lt;- rf_wf1 %&gt;% tune_grid( resamples = churn_folds, metrics = metric_set(roc_auc, sens, spec) ) toc() } So kann man das berechnete Objekt abspeichern auf Festplatte, um kÃ¼nftig Zeit zu sparen: write_rds(rf_fit1, file = &quot;objects/rf_fit1.rds&quot;) rf_fit1 ## # Tuning results ## # 5-fold cross-validation ## # A tibble: 5 Ã— 4 ## splits id .metrics .notes ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2393/599]&gt; Fold1 &lt;tibble [30 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; ## 2 &lt;split [2393/599]&gt; Fold2 &lt;tibble [30 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; ## 3 &lt;split [2394/598]&gt; Fold3 &lt;tibble [30 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; ## 4 &lt;split [2394/598]&gt; Fold4 &lt;tibble [30 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; ## 5 &lt;split [2394/598]&gt; Fold5 &lt;tibble [30 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; show_best(rf_fit1) ## # A tibble: 5 Ã— 9 ## mtry trees min_n .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 6 1686 18 roc_auc binary 0.958 5 0.00330 Preprocessor1_Model03 ## 2 5 747 34 roc_auc binary 0.958 5 0.00324 Preprocessor1_Model10 ## 3 10 818 22 roc_auc binary 0.956 5 0.00378 Preprocessor1_Model01 ## 4 8 342 2 roc_auc binary 0.955 5 0.00361 Preprocessor1_Model09 ## 5 13 1184 25 roc_auc binary 0.954 5 0.00423 Preprocessor1_Model08 11.10.6.3 XGBoost boost_wf1 &lt;- workflow() %&gt;% add_model(boost_model) %&gt;% add_recipe(churn_recipe1) tic() boost_fit1 &lt;- boost_wf1 %&gt;% tune_grid( resamples = churn_folds, metrics = metric_set(roc_auc, sens, spec) ) toc() Wieder auf Festplatte speichern: write_rds(boost_fit1, file = &quot;objects/boost_fit1.rds&quot;) Und so weiter. 11.10.7 Workflow-Set tunen if (file.exists(&quot;objects/churn_model_set.rds&quot;)) { churn_model_set &lt;- read_rds(&quot;objects/churn_model_set.rds&quot;) } else { tic() churn_model_set &lt;- all_workflows %&gt;% workflow_map( resamples = churn_folds, grid = 20, metrics = metric_set(roc_auc), seed = 42, # reproducibility verbose = TRUE) toc() } Da die Berechnung schon etwas Zeit braucht, macht es Sinn, das Modell (bzw. das Ergebnisobjekt) auf Festplatte zu speichern: write_rds(churn_model_set, file = &quot;objects/churn_model_set.rds&quot;) Entsprechend kann man das Modellobjekt wieder importieren, wenn einmal abgespeichert: churn_model_set &lt;- read_rds(file = &quot;objects/churn_model_set.rds&quot;) 11.10.8 Ergebnisse im Train-Sest Hier ist die Rangfolge der Modelle, geordnet nach mittlerem ROC AUC: rank_results(churn_model_set, rank_metric = &quot;roc_auc&quot;) ## # A tibble: 122 Ã— 9 ## wflow_id .config .metric mean std_err n preprocessor model rank ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 rec2_boost1 Preprocessoâ€¦ roc_auc 0.963 0.00104 5 recipe boosâ€¦ 1 ## 2 rec1_boost1 Preprocessoâ€¦ roc_auc 0.963 0.00104 5 recipe boosâ€¦ 2 ## 3 rec2_boost1 Preprocessoâ€¦ roc_auc 0.961 0.00106 5 recipe boosâ€¦ 3 ## 4 rec1_boost1 Preprocessoâ€¦ roc_auc 0.961 0.00106 5 recipe boosâ€¦ 4 ## 5 rec2_glm1 Preprocessoâ€¦ roc_auc 0.961 0.00272 5 recipe logiâ€¦ 5 ## 6 rec1_boost1 Preprocessoâ€¦ roc_auc 0.961 0.00102 5 recipe boosâ€¦ 6 ## 7 rec2_boost1 Preprocessoâ€¦ roc_auc 0.961 0.00102 5 recipe boosâ€¦ 7 ## 8 rec2_boost1 Preprocessoâ€¦ roc_auc 0.960 0.00120 5 recipe boosâ€¦ 8 ## 9 rec1_boost1 Preprocessoâ€¦ roc_auc 0.960 0.00120 5 recipe boosâ€¦ 9 ## 10 rec1_rf1 Preprocessoâ€¦ roc_auc 0.960 0.00278 5 recipe randâ€¦ 10 ## # â€¦ with 112 more rows autoplot(churn_model_set, metric = &quot;roc_auc&quot;) 11.10.9 Bestes Modell Und hier nur der beste Kandidat pro Algorithmus: autoplot(churn_model_set, metric = &quot;roc_auc&quot;, select_best = &quot;TRUE&quot;) + geom_text(aes(y = mean - .01, label = wflow_id), angle = 90, hjust = 1) + theme(legend.position = &quot;none&quot;) + lims(y = c(0.85, 1)) Boosting hat - knapp - am besten abgeschnitten. Allerdings sind Random Forest und die schlichte, einfache logistische Regression auch fast genau so gut. Das wÃ¤re ein Grund fÃ¼r das einfachste Modell, das GLM, zu votieren. Zumal die Interpretierbarkeit am besten ist. Alternativ kÃ¶nnte man sich fÃ¼r das Boosting-Modell aussprechen. Man kann sich das beste Submodell auch von Tidymodels bestimmen lassen. Das scheint aber (noch) nicht fÃ¼r ein Workflow-Set zu funktionieren, sondern nur fÃ¼r das Ergebnisobjekt von tune_grid. select_best(churn_model_set, metric = &quot;roc_auc&quot;) ## Error in `is_metric_maximize()`: ## ! Please check the value of `metric`. rf_fit1 haben wir mit tune_grid() berechnet; mit diesem Modell kann select_best() arbeiten: select_best(rf_fit1) ## # A tibble: 1 Ã— 4 ## mtry trees min_n .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 6 1686 18 Preprocessor1_Model03 Aber wir kÃ¶nnen uns hÃ¤ndisch behelfen. Schauen wir uns mal die Metriken (VorhersagegÃ¼te) an: churn_model_set %&gt;% collect_metrics() %&gt;% arrange(-mean) ## # A tibble: 122 Ã— 9 ## wflow_id .config preproc model .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 rec1_boost1 Preprocessoâ€¦ recipe boosâ€¦ roc_auc binary 0.963 5 0.00104 ## 2 rec2_boost1 Preprocessoâ€¦ recipe boosâ€¦ roc_auc binary 0.963 5 0.00104 ## 3 rec1_boost1 Preprocessoâ€¦ recipe boosâ€¦ roc_auc binary 0.961 5 0.00106 ## 4 rec2_boost1 Preprocessoâ€¦ recipe boosâ€¦ roc_auc binary 0.961 5 0.00106 ## 5 rec2_glm1 Preprocessoâ€¦ recipe logiâ€¦ roc_auc binary 0.961 5 0.00272 ## 6 rec1_boost1 Preprocessoâ€¦ recipe boosâ€¦ roc_auc binary 0.961 5 0.00102 ## 7 rec2_boost1 Preprocessoâ€¦ recipe boosâ€¦ roc_auc binary 0.961 5 0.00102 ## 8 rec1_boost1 Preprocessoâ€¦ recipe boosâ€¦ roc_auc binary 0.960 5 0.00120 ## 9 rec2_boost1 Preprocessoâ€¦ recipe boosâ€¦ roc_auc binary 0.960 5 0.00120 ## 10 rec1_rf1 Preprocessoâ€¦ recipe randâ€¦ roc_auc binary 0.960 5 0.00278 ## # â€¦ with 112 more rows rec1_boost1 scheint das beste Modell zu sein. best_model_params &lt;- extract_workflow_set_result(churn_model_set, &quot;rec1_boost1&quot;) %&gt;% select_best() best_model_params ## # A tibble: 1 Ã— 4 ## mtry trees min_n .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 6 80 21 Preprocessor1_Model05 11.10.10 Finalisisieren Wir entscheiden uns mal fÃ¼r das Boosting-Modell, rec1_boost1. Diesen Workflow, in finalisierter Form, brauchen wir fÃ¼r den â€œfinal Fitâ€. Finalisierte Form heiÃŸt: Schritt 1: Nimm den passenden Workflow, hier rec1 und boost1; das hatte uns oben rank_results() verraten. Schritt 2: Update (Finalisiere) ihn mit den besten Tuningparameter-Werten # Schritt 1: best_wf &lt;- all_workflows %&gt;% extract_workflow(&quot;rec1_boost1&quot;) best_wf ## â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ## Preprocessor: Recipe ## Model: boost_tree() ## ## â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ## 2 Recipe Steps ## ## â€¢ step_normalize() ## â€¢ step_dummy() ## ## â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ## Boosted Tree Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = tune() ## min_n = tune() ## ## Engine-Specific Arguments: ## nthreads = parallel::detectCores() ## ## Computational engine: xgboost Jetzt finalisieren wir den Workflow, d.h. wir setzen die Parameterwerte des besten Submodells ein: # Schritt 2: best_wf_finalized &lt;- best_wf %&gt;% finalize_workflow(best_model_params) best_wf_finalized ## â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ## Preprocessor: Recipe ## Model: boost_tree() ## ## â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ## 2 Recipe Steps ## ## â€¢ step_normalize() ## â€¢ step_dummy() ## ## â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ## Boosted Tree Model Specification (classification) ## ## Main Arguments: ## mtry = 6 ## trees = 80 ## min_n = 21 ## ## Engine-Specific Arguments: ## nthreads = parallel::detectCores() ## ## Computational engine: xgboost 11.10.11 Last Fit fit_final &lt;- best_wf_finalized %&gt;% last_fit(churn_split) fit_final ## # Resampling results ## # Manual resampling ## # A tibble: 1 Ã— 6 ## splits id .metrics .notes .predictions .workflow ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2992/998]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt; &lt;workflow&gt; collect_metrics(fit_final) ## # A tibble: 2 Ã— 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.903 Preprocessor1_Model1 ## 2 roc_auc binary 0.966 Preprocessor1_Model1 11.10.12 Variablenrelevanz Um die Variablenrelevanz zu plotten, mÃ¼ssen wir aus dem Tidymodels-Ergebnisobjekt das eigentliche Ergebnisobjekt herausziehen, von der R-Funktion, die die eigentliche Berechnung durchfÃ¼hrt, das wÃ¤re glm() bei einer logistischen Regression oder xgboost::xgb.train() bei XGBoost: fit_final %&gt;% extract_fit_parsnip() ## parsnip model object ## ## ##### xgb.Booster ## raw: 114.1 Kb ## call: ## xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, ## colsample_bytree = 1, colsample_bynode = 0.285714285714286, ## min_child_weight = 21L, subsample = 1, objective = &quot;binary:logistic&quot;), ## data = x$data, nrounds = 80L, watchlist = x$watchlist, verbose = 0, ## nthreads = 8L, nthread = 1) ## params (as set within xgb.train): ## eta = &quot;0.3&quot;, max_depth = &quot;6&quot;, gamma = &quot;0&quot;, colsample_bytree = &quot;1&quot;, colsample_bynode = &quot;0.285714285714286&quot;, min_child_weight = &quot;21&quot;, subsample = &quot;1&quot;, objective = &quot;binary:logistic&quot;, nthreads = &quot;8&quot;, nthread = &quot;1&quot;, validate_parameters = &quot;TRUE&quot; ## xgb.attributes: ## niter ## callbacks: ## cb.evaluation.log() ## # of features: 21 ## niter: 80 ## nfeatures : 21 ## evaluation_log: ## iter training_logloss ## 1 0.600389 ## 2 0.495920 ## --- ## 79 0.197130 ## 80 0.196439 Dieses Objekt Ã¼bergeben wir dann an {vip}: fit_final %&gt;% extract_fit_parsnip() %&gt;% vip() 11.10.13 ROC-Curve Eine ROC-Kurve berechnet SensitivitÃ¤t und SpezifitÃ¤t aus den Vorhersagen, bzw. aus dem Vergleich von Vorhersagen und wahrem Wert (d.h. der beobachtete Wert). Ziehen wir also zuerst die Vorhersagen heraus: fit_final %&gt;% collect_predictions() ## # A tibble: 998 Ã— 7 ## id .pred_yes .pred_no .row .pred_class canceled_service .config ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; ## 1 train/test splâ€¦ 0.00669 0.993 1 no yes Preproâ€¦ ## 2 train/test splâ€¦ 0.915 0.0847 6 yes yes Preproâ€¦ ## 3 train/test splâ€¦ 0.959 0.0408 15 yes yes Preproâ€¦ ## 4 train/test splâ€¦ 0.997 0.00291 16 yes yes Preproâ€¦ ## 5 train/test splâ€¦ 0.901 0.0994 17 yes yes Preproâ€¦ ## 6 train/test splâ€¦ 0.992 0.00765 18 yes yes Preproâ€¦ ## 7 train/test splâ€¦ 0.965 0.0349 28 yes yes Preproâ€¦ ## 8 train/test splâ€¦ 0.941 0.0589 31 yes yes Preproâ€¦ ## 9 train/test splâ€¦ 0.703 0.297 32 yes yes Preproâ€¦ ## 10 train/test splâ€¦ 0.988 0.0115 41 yes yes Preproâ€¦ ## # â€¦ with 988 more rows Praktischerweise werden die â€œwahren Werteâ€ (also die beobachtaten Werte), canceled_service, ausch angegeben. Dann berechnen wir die roc_curve und autoplotten sie. fit_final %&gt;% collect_predictions() %&gt;% roc_curve(canceled_service, .pred_yes) %&gt;% autoplot() 11.11 Aufgaben Einfache DurchfÃ¼hrung eines Modellierung mit XGBoost Fallstudie Oregon Schools Fallstudie Churn Fallstudie Ikea Fallstudie Wasserquellen in Sierra Leone Fallstudie BÃ¤ume in San Francisco Fallstudie VulkanausbrÃ¼che Fallstudie Brettspiele mit XGBoost References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
