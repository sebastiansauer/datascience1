[["ensemble-lerner.html", "Kapitel 11 Ensemble Lerner 11.1 Lernsteuerung 11.2 Vorbereitung 11.3 Hinweise zur Literatur 11.4 Wir brauchen einen Wald 11.5 Was ist ein Ensemble-Lerner? 11.6 Bagging 11.7 Bagging-Algorithmus 11.8 Random Forests 11.9 Boosting 11.10 Tidymodels 11.11 Aufgaben", " Kapitel 11 Ensemble Lerner 11.1 Lernsteuerung 11.1.1 Lernziele Sie k√∂nnen Algorithmen f√ºr Ensemble-Lernen erkl√§ren, d.i. Bagging, AdaBoost, XGBoost, Random Forest Sie wissen, anhand welche Tuningparamter man Overfitting bei diesen Algorithmen begrenzen kann Sie k√∂nnen diese Verfahren in R berechnen 11.1.2 Literatur Rhys, Kap. 8 11.2 Vorbereitung In diesem Kapitel werden folgende R-Pakete ben√∂tigt: library(tidymodels) library(tictoc) # Zeitmessung library(vip) # Variable importance plot 11.3 Hinweise zur Literatur Die folgenden Ausf√ºhrungen basieren prim√§r auf Rhys (2020), aber auch auf James et al. (2021) und (weniger) Kuhn and Johnson (2013). 11.4 Wir brauchen einen Wald Ein Pluspunkt von Entscheidungsb√§umen ist ihre gute Interpretierbarkeit. Man k√∂nnte behaupten, dass B√§ume eine typische Art des menschlichen Entscheidungsverhalten nachahmen: ‚ÄúWenn A, dann tue B, ansonsten tue C‚Äù (etc.). Allerdings: Einzelne Entscheidungsb√§ume haben oft keine so gute Prognosegenauigkeit. Der oder zumindest ein Grund ist, dass sie (zwar wenig Bias aber) viel Varianz aufweisen. Das sieht man z.B. daran, dass die Vorhersagegenauigkeit stark schwankt, w√§hlt man eine andere Aufteilung von Train- vs.¬†Test-Sample. Anders gesagt: B√§ume overfitten ziemlich schnell. Und obwohl das No-Free-Lunch-Theorem zu den Grundfesten des maschinellen Lernens (oder zu allem wissenschaftlichen Wissen) geh√∂rt, kann man festhalten, dass sog. Ensemble-Lernen fast immer besser sind als einzelne Baummodelle. Kurz gesagt: Wir brauchen einen Wald: üå≥üå≥üå≥1 11.5 Was ist ein Ensemble-Lerner? Ensemble-Lerner kombinieren mehrere schwache Lerner zu einem starken Lerner. Das Paradebeispiel sind baumbasierte Modelle; darauf wird sich die folgende Ausf√ºhrung auch begrenzen. Aber theoretisch kann man jede Art von Lerner kombinieren. Bei numerischer Pr√§diktion wird bei Ensemble-Lerner zumeist der Mittelwert als Optmierungskriterium herangezogen; bei Klassifikation (nominaler Pr√§diktion) hingegen die modale Klasse (also die h√§ufigste). Warum hilft es, mehrere Modelle (Lerner) zu einem zu aggregieren? Die Antwort lautet, dass die Streuung der Mittelwerte sinkt, wenn die Stichprobengr√∂√üe steigt. Zieht man Stichproben der Gr√∂√üe 1, werden die Mittelwerte stark variieren, aber bei gr√∂√üeren Stichproben (z.B. Gr√∂√üe 100) deutlich weniger2. Die Streuung der Mittelwerte in den Stichproben nennt man bekanntlich Standardefehler (se). Den se des Mittelwerts (\\(se_M\\)) f√ºr eine normalverteilte Variable \\(X \\sim \\mathcal{N}(\\mu, \\sigma)\\) gilt: \\(se_{M} = \\sigma / \\sqrt(n)\\), wobei \\(\\sigma\\) die SD der Verteilung und \\(\\mu\\) den Erwartungswert (‚ÄúMittelwert‚Äù) meint, und \\(n\\) ist die Stichprobengr√∂√üe. Je gr√∂√üer die Stichprobe, desto kleiner die Varianz des Sch√§tzers (ceteris paribus). Anders gesagt: Gr√∂√üere Stichproben sch√§tzen genauer als kleine Stichproben. Aus diesem Grund bietet es sich an, schwache Lerner mit viel Varianz zu kombinieren, da die Varianz so verringert wird. 11.6 Bagging 11.6.1 Bootstrapping Das erste baumbasierte Modell, was vorgestellt werden soll, basiert auf sog. Bootstrapping, ein Standardverfahren in der Statistik (James et al. 2021). Bootstrapping ist eine Nachahmung f√ºr folgende Idee: H√§tte man viele Stichproben aus der relevanten Verteilung, so k√∂nnte man z.B. die Genauigkeit eines Modells \\(\\hat{f}_{\\bar{X}}\\) zur Sch√§tzung des Erwartungswertes \\(\\mu\\) einfach dadurch bestimmen, indem man se berechnet, also die Streuung der Mitterwerte \\(\\bar{X}\\) berechnet. Au√üerdem gilt, dass die Pr√§zision der Sch√§tzung des Erwartungswerts steigt mit steigendem Stichprobenumfang \\(n\\). Wir k√∂nnten also f√ºr jede der \\(B\\) Stichproben, \\(b=1,\\ldots, B\\), ein (Baum-)Modell berechnen und dann deren Vorhersagen aggregieren (zum Mittelwert oder Modalwert). Das kann man formal so darstellen (James et al. 2021): \\[\\hat{f}_{\\bar{X}} = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{f}^b\\] Mit diesem Vorgehen kann die Varianz des Modells \\(\\hat{f}_{\\bar{X}}\\) verringert werden; die Vorhersagegenauigkeit steigt. Leider haben wir in der Regel nicht viele (\\(B\\)) Datens√§tze. Daher ‚Äúbauen‚Äù wir uns aus dem einzelnen Datensatz, der uns zur Verf√ºgung steht, viele Datens√§tze. Das h√∂rt sich nach ‚Äútoo good to be true‚Äù an3 Weil es sich unglaubw√ºrdig anh√∂rt, nennt man das entsprechende Verfahren (gleich kommt es!) auch ‚ÄúM√ºnchhausen-Methode‚Äù, nach dem ber√ºhmten L√ºbgenbaron. Die Amerikaner ziehen sich √ºbrigens nicht am Schopf aus dem Sumpf, sondern mit den Stiefelschlaufen (die Cowboys wieder), daher spricht man im Amerikanischen auch von der ‚ÄúBoostrapping-Methode‚Äù. Diese ‚ÄúPseudo-Stichproben‚Äù oder ‚ÄúBootstrapping-Stichproben‚Äù sind aber recht einfach zu gewinnen.. Gegeben sei Stichprobe der Gr√∂√üe \\(n\\): Ziehe mit Zur√ºcklegen (ZmZ) aus der Stichprobe \\(n\\) Beobachtungen Fertig ist die Bootstrapping-Stichprobe. Abb. 11.1 verdeutlicht das Prinzip des ZMZ, d.h. des Bootstrappings. Wie man sieht, sind die Bootstrap-Stichproben (rechts) vom gleichen Umfang \\(n\\) wie die Originalstichprobe (links). Allerdins kommen nicht alle F√§lle (in der Regel) in den ‚ÄúBoostrap-Beutel‚Äù (in bag), sondern einige F√§lle werden oft mehrfach gezogen, so dass einige F√§lle nicht gezogen werden (out of bag). Figure 11.1: Bootstrapping: Der Topf links symbolisiert die Original-Stichprobe, aus der wir hier mehrere ZMZ-Stichproben ziehen (Rechts), dargestellt mit ‚Äòin bag‚Äô Man kann zeigen, dass ca. 2/3 der F√§lle gezogen werden, bzw. ca. 1/3 nicht gezogen werden. Die nicht gezogenen F√§lle nennt man auch out of bag (OOB). F√ºr die Entwicklung des Bootstrapping wurde der Autor, Bradley Efron, im Jahr 2018 mit dem internationalen Preis f√ºr Statistik ausgezeichnet; ‚ÄúWhile statistics offers no magic pill for quantitative scientific investigations, the bootstrap is the best statistical pain reliever ever produced,‚Äù says Xiao-Li Meng, Whipple V. N. Jones Professor of Statistics at Harvard University.‚Äú 11.7 Bagging-Algorithmus Bagging, die Kurzform f√ºr Bootstrap-Aggregation ist wenig mehr als die Umsetzung des Boostrappings. Der Algorithmus von Bagging kann so beschrieben werden: W√§hle \\(B\\), die Anzahl der Boostrap-Stichproben und damit auch Anzahl der Submodelle (Lerner) Ziehe \\(B\\) Boostrap-Stichproben Berechne das Modell \\(\\hat{f}^{*b}\\) f√ºr jede der \\(B\\) Stichproben (typischerweise ein einfacher Baum) Schicke die Test-Daten durch jedes Sub-Modell Aggregiere ihre Vorhersage zu einem Wert (Modus bzw. Mittelwert) pro Fall aus dem Test-Sample, zu \\(\\hat{f}_{\\text{bag}}\\) Anders gesagt: \\[\\hat{f}_{\\text{bag}} = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{f}^{*b}\\] Der Bagging-Algorithmus ist in Abbildung 11.2 dargestellt. Figure 11.2: Bagging schematisch illustriert Die Anzahl der B√§ume (allgemeiner: Submodelle) \\(B\\) ist h√§ufig im oberen drei- oder niedrigem vierstelligen Bereich, z.B. \\(B=1000\\). Eine gute Nachricht ist, dass Bagging nicht √ºberanpasst, wenn \\(B\\) gro√ü wird. 11.7.1 Variablenrelevanz Man kann die Relevanz der Pr√§diktoren in einem Bagging-Modell auf mehrere Arten sch√§tzen. Ein Weg (bei numerischer Pr√§diktion) ist, dass man die RSS-Verringerung, die durch Aufteilung anhand eines Pr√§diktors erzeugt wird, mittelt √ºber alle beteiligten B√§ume (Modelle). Bei Klassifikation kann man die analog die Reduktion des Gini-Wertes √ºber alle B√§ume mitteln und als Sch√§tzwert f√ºr die Relevanz des Pr√§diktors heranziehen. 11.7.2 Out of Bag Vorhersagen Da nicht alle F√§lle der Stichprobe in das Modell einflie√üen (sondern nur ca. 2/3), kann der Rest der F√§lle zur Vorhersage genutzt werden. Bagging erzeugt sozusagen innerhalb der Stichprobe selbst√§ndig ein Train- und ein Test-Sample. Man spricht von Out-of-Bag-Sch√§tzung (OOB-Sch√§tzung). Der OOB-Fehler (z.B. MSE bei numerischen Modellen und Genauigkeit bei nominalen) ist eine valide Sch√§tzung des typischen Test-Sample-Fehlers. Hat man aber Tuningparameter, so wird man dennoch auf die typische Train-Test-Aufteilung zur√ºckgreifen, um Overfitting durch das Ausprobieren der Tuning-Kandidaten zu vermeiden (was sonst zu Zufallstreffern f√ºhren w√ºrde bei gen√ºgend vielen Modellkandidaten). 11.8 Random Forests Random Forests (‚ÄúZufallsw√§lder‚Äù) sind eine Weiterentwicklung von Bagging-Modellen. Sie sind Bagging-Modelle, aber haben noch ein Ass im √Ñrmel: Und zwar wird an jedem Slit (Astgabel, Aufteilung) nur eine Zufallsauswahl an \\(m\\) Pr√§diktoren ber√ºcksichtigt. Das h√∂rt sich verr√ºckt an: ‚ÄúWie, mit weniger Pr√§diktoren soll eine bessere Vorhersage erreicht werden?!‚Äù Ja, genau so ist es! Nehmen Sie an, es gibt im Datensatz einen sehr starken und ein paar mittelstarke Pr√§diktoren; der Rest der Pr√§diktoren ist wenig relevant. Wenn Sie jetzt viele ‚Äúgebootstrapte‚Äù4 ziehen, werden diese B√§ume sehr √§hnlich sein: Der st√§rkste Pr√§diktor steht vermutlich immer ob an der Wurzel, dann kommen die mittelstarken Pr√§diktoren. Jeder zus√§tzliche Baum tr√§gt dann wenig neue Information bei. Anders gesagt: Die Vorhersagen der B√§ume sind dann sehr √§hnlich bzw. hoch korreliert. Bildet man den Mittelwert von hoch korrelierten Variablen, verringert sich leider die Varianzu nur wenig im Vergleich zu nicht oder gering korrelierten Variablen (James et al. 2021). Dadurch dass Random Forests nur \\(m\\) der \\(p\\) Pr√§diktoren pro Split zulassen, werden die B√§ume unterschiedlicher. Wir ‚Äúdekorrelieren‚Äù die B√§ume. Bildet man den Mittelwert von gering(er) korrelierten Variablen, so ist die Varianzreduktion h√∂her - und die Vohersage genauer. L√§sst man pro Split \\(m=p\\) Pr√§diktoren zu, so gleicht Bagging dem Random Forest. Die Anzahl \\(m\\) der erlaubten Pr√§diktoren werden als Zufallstichprobe aus den \\(p\\) Pr√§diktoren des Datensatzes gezogen (ohne Zur√ºcklegen). \\(m\\) ist ein Tuningparameter; \\(m=\\sqrt(p)\\) ist ein beliebter Startwert. In den meisten Implementationen wird \\(m\\) mit mtry bezeichnet (so auch in Tidymodels). Der Random-Forest-Algorithmus ist in Abb. 11.3 illustriert. Figure 11.3: Zufallsw√§lder durch Ziehen mit Zur√ºcklegen (zmz) und Ziehen ohne Zur√ºcklegen (ZoZ) Abb. 11.4 vergleicht die Test-Sample-Vorhersageg√ºte von Bagging- und Random-Forest-Algorithmen aus James et al. (2021). In diesem Fall ist die Vorhersageg√ºte deutlich unter der OOB-G√ºte; laut James et al. (2021) ist dies hier ‚ÄúZufall‚Äù. Figure 11.4: Test-Sample-Vorhersageg√ºte von Bagging- und Random-Forest-Algorithmen Den Effekt von \\(m\\) (Anzahl der Pr√§diktoren pro Split) ist in Abb. 11.5 dargestellt (James et al. 2021). Man erkennt, dass der Zusatznutzen an zus√§tzlichen B√§umen, \\(B\\), sich abschw√§cht. \\(m=\\sqrt{p}\\) schneidet wie erwartet am besten ab. Figure 11.5: Test-Sample-Vorhersageg√ºte von Bagging- und Random-Forest-Algorithmen 11.9 Boosting Im Unterschied zu Bagging und Random-Forest-Modellen wird beim Boosting der ‚ÄúWald‚Äù sequenziell entwickelt, nicht gleichzeitig wie bei den anderen vorgestellten ‚ÄúWald-Modellen‚Äù. Die zwei bekanntesten Implementierungen bzw. Algorithmus-Varianten sind AdaBoost und XGBoost. Gerade XGBoost hat den Ruf, hervorragende Vorhersagen zu leisten. Auf Kaggle gewinnt nach einigen Berichten oft XGBoost. Nur neuronale Netze schneiden besser ab. Random-Forest-Modelle kommen nach diesem Bereich auf Platz 3. Allerdings ben√∂tigen neuronale Netzen oft riesige Stichprobengr√∂√üen und bei spielen ihre Nuanciertheit vor allem bei komplexen Daten wie Bildern oder Sprache aus. F√ºr ‚Äúrechteckige‚Äù Daten (also aus einfachen, normalen Tabellen) wird ein baumbasiertes Modell oft besser abschneiden. Die Idee des Boosting ist es, anschaulich gesprochen, aus Fehlern zu lernen: Fitte einen Baum, schau welche F√§lle er schlecht vorhergesagt hat, konzentriere dich beim n√§chsten Baum auf diese F√§lle und so weiter. Wie andere Ensemble-Methoden auch kann Boosting theoretisch f√ºr beliebige Algorithmen eingesetzt werden. Es macht aber Sinn, Boosting bei ‚Äúschwachen Lernern‚Äù einzusetzen. Typisches Beispiel ist ein einfacher Baum; ‚Äúeinfach‚Äù soll hei√üen, der Baum hat nur wenig Gabeln oder vielleicht sogar nur eine einzige. Dann spricht man von einem Stumpf, was intuitiv gut passt. 11.9.1 AdaBoost Der AdaBoost-Algorithmus funktioniert, einfach dargestellt, wie folgt. Zuerst hat jeder Fall im Datensatz des gleiche Gewicht. Die erste (und alle weiteren) Stichprobe werden per Bootstrapping aus dem Datensatz gezogen. Dabei ist die Wahrscheinlichkeit, gezogen zu werden, proportional zum Gewicht. Da im ersten Durchgang die Gewichte identisch sind, haben zun√§chst alle F√§lle die gleiche Wahrscheinlichkeit, in das Bootstrap-Sample gezogen zu werden. Nach Berechnung des Baumen und der Vorhersagen werden die richtig klassifizierten F√§lle heruntergewichtet und die falsch klassifizierten F√§lle hoch gewichtet, also st√§rker gewichtet (bleiben wir aus Gr√ºnden der Einfachheit zun√§chst bei der Klassifikation). Dieses Vorgehen folgt dem Gedanken, dass man sich seine Fehler genauer anschauen muss, die falsch klassifizierten F√§lle sozusagen mehr Aufmerksamkeit bed√ºrfen. Das n√§chste (zweite) Modell zieht ein weiteres Bootstrap-Sample. Jetzt sind allerdings die Gewichte schon angepasst, so dass mehr F√§lle, die im vorherigen Modell falsch klassifiziert wurden, in den neuen (zweiten) Baum gezogen werden. Das neue Modell hat also bessere Chancen, die Aspekte, die das Vorg√§nger-Modell √ºbersah zu korrigieren bzw. zu lernen. Jetzt haben wir zwei Modelle. Die k√∂nnen wir aggregieren, genau wie beim Bagging: Der Modus der Vorhersage √ºber alle (beide) B√§ume hinwig ist dann die Vorhersage f√ºr einen bestimmten Fall (‚ÄúFall‚Äù und ‚ÄúBeobachtung‚Äù sind stets synonym f√ºr \\(y_i\\) zu verstehen). So wiederholt sich das Vorgehen: Die Gewichte werden angepasst, das neue Modell wird berechnet, alle Modelle machen ihre Vorhersagen, per Mehrheitsbeschluss wird die Vorhersage bestimmt pro Fall. Irgendwann erreichen wir die vorab definierte Maximalzahl an B√§umen, \\(B\\), und das Modell kommt zu einem Ende. Da das Modell die Fehler seiner Vorg√§nger reduziert, wird der Bias im Gesamtmodell verringert. Da wir gleichzeitig auch Bagging vornehmen, wird aber die Varianz auch verringert. Klingt schon wieder (fast) nach Too-Good-to-be-True! Das Gewicht \\(w_i^b\\) des \\(b\\)ten Falls im \\(b\\)ten Modell von \\(B\\) berechnet sich wie folgt (Rhys 2020): \\[ w_i^b = \\begin{cases} w_i^{b-1} \\cdot e^{-\\text{model weight}} \\qquad \\text{wenn korrekt klassifiziert} \\\\ w_i^{b-1} \\cdot e^{\\text{model weight}} \\qquad \\text{wenn inkorrekt klassifiziert} \\\\ \\end{cases}\\] Das Modellgewicht \\(mw\\) berechnet sich dabei so (Rhys 2020): \\[mw = 0.5 \\cdot log\\left( \\frac{1-p(\\text{inkorrect})}{p(\\text{korrekt})} \\right) \\propto \\mathcal{L(p)} \\] Das Modellgewicht ist ein Faktor, dass schlechtere Modelle bestraft. 11.9.2 XGBoost XGBoost ist ein Gradientenverfahren, eine Methode also, die die Richtung des parziellen Ableitungskoeffizienten als Optimierungskriterium heranzieht. XGBoost ist √§hnlich zu AdaBoost, nur dass Residuen modelliert werden, nicht \\(y\\). Die Vorhersagefehler von \\(\\hat{f}^b\\) werden die Zielvariable von \\(\\hat{f}^{b+1}\\). Ein Residuum ist der Vorhersagefehler, bei metrischen Modellen etwa RMSE, oder schlicht \\(r_i = y_i - \\hat{y}_i\\). Details finden sich z.B. hier, dem Original XGBoost-Paper (Chen and Guestrin 2016). Die hohe Vorhersageg√ºte von Boosting-Modellen ist exemplarisch in Abb. 11.6 dargestellt (James et al. 2021, S. 358ff). Allerdings verwenden die Autoren Friedmans (2001) Gradient Boosting Machine, eine weitere Variante des Boosting . Figure 11.6: Vorhersageg√ºte von Boosting und Random Forest 11.10 Tidymodels 11.10.1 Datensatz Churn Wir betrachten einen Datensatz zur Kundenabwanderung (Churn) aus dieser Quelle. churn_df &lt;- read_rds(&#39;https://gmudatamining.com/data/churn_data.rds&#39;) Ein Blick in die Daten churn_df %&gt;% head() %&gt;% gt::gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #kuhgnnqrfx .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #kuhgnnqrfx .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kuhgnnqrfx .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #kuhgnnqrfx .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #kuhgnnqrfx .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kuhgnnqrfx .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kuhgnnqrfx .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #kuhgnnqrfx .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #kuhgnnqrfx .gt_column_spanner_outer:first-child { padding-left: 0; } #kuhgnnqrfx .gt_column_spanner_outer:last-child { padding-right: 0; } #kuhgnnqrfx .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #kuhgnnqrfx .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #kuhgnnqrfx .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #kuhgnnqrfx .gt_from_md > :first-child { margin-top: 0; } #kuhgnnqrfx .gt_from_md > :last-child { margin-bottom: 0; } #kuhgnnqrfx .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #kuhgnnqrfx .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #kuhgnnqrfx .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kuhgnnqrfx .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #kuhgnnqrfx .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kuhgnnqrfx .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #kuhgnnqrfx .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #kuhgnnqrfx .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kuhgnnqrfx .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kuhgnnqrfx .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #kuhgnnqrfx .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kuhgnnqrfx .gt_sourcenote { font-size: 90%; padding: 4px; } #kuhgnnqrfx .gt_left { text-align: left; } #kuhgnnqrfx .gt_center { text-align: center; } #kuhgnnqrfx .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #kuhgnnqrfx .gt_font_normal { font-weight: normal; } #kuhgnnqrfx .gt_font_bold { font-weight: bold; } #kuhgnnqrfx .gt_font_italic { font-style: italic; } #kuhgnnqrfx .gt_super { font-size: 65%; } #kuhgnnqrfx .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } canceled_service enrollment_discount spouse_partner dependents phone_service internet_service online_security online_backup device_protection tech_support streaming_tv streaming_movies contract paperless_bill payment_method months_with_company monthly_charges late_payments yes no no no multiple_lines fiber_optic yes yes yes no no no one_year no credit_card 30 51.01440 3 yes no yes yes multiple_lines fiber_optic no yes yes yes yes no two_year yes electronic_check 39 80.42466 4 yes yes no no single_line fiber_optic no no no no yes yes month_to_month yes mailed_check 1 75.88737 3 yes no yes yes single_line fiber_optic yes no no no yes no two_year no credit_card 29 81.96467 3 yes yes no no single_line digital no no no no yes yes month_to_month yes bank_draft 9 101.34257 5 yes no yes no single_line fiber_optic yes yes no yes yes yes month_to_month no mailed_check 14 72.01285 4 11.10.2 Data Splitting Das Kreuzvalidieren fassen wir auch unter diesen Punkt. churn_split &lt;- initial_split(churn_df, prop = 0.75, strata = canceled_service) churn_training &lt;- churn_split %&gt;% training() churn_test &lt;- churn_split %&gt;% testing() churn_folds &lt;- vfold_cv(churn_training, v = 5) 11.10.3 Feature Engineering churn_recipe1 &lt;- recipe(canceled_service ~ ., data = churn_training) %&gt;% step_normalize(all_numeric(), -all_outcomes()) %&gt;% step_dummy(all_nominal(), -all_outcomes()) churn_recipe2 &lt;- recipe(canceled_service ~ ., data = churn_training) %&gt;% step_YeoJohnson(all_numeric(), -all_outcomes()) %&gt;% step_normalize(all_numeric(), -all_outcomes()) %&gt;% step_dummy(all_nominal(), -all_outcomes()) step_YeoJohnson() reduziert Schiefe in der Verteilung. 11.10.4 Modelle tree_model &lt;- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %&gt;% set_engine(&#39;rpart&#39;) %&gt;% set_mode(&#39;classification&#39;) rf_model &lt;- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %&gt;% set_engine(&#39;ranger&#39;) %&gt;% set_mode(&#39;classification&#39;) boost_model &lt;- boost_tree(mtry = tune(), min_n = tune(), trees = tune()) %&gt;% set_engine(&quot;xgboost&quot;, nthreads = parallel::detectCores()) %&gt;% set_mode(&quot;classification&quot;) glm_model &lt;- logistic_reg() 11.10.5 Workflows Wir definieren ein Workflow-Set: preproc &lt;- list(rec1 = churn_recipe1, rec2 = churn_recipe2) models &lt;- list(tree1 = tree_model, rf1 = rf_model, boost1 = boost_model, glm1 = glm_model) all_workflows &lt;- workflow_set(preproc, models) Infos zu workflow_set bekommt man wie gewohnt mit workflow_set. Im Standard werden alle Rezepte und Modelle miteinander kombiniert (cross = TRUE), also preproc * models Modelle gefittet. 11.10.6 Modelle berechnen mit Tuning, einzeln Wir k√∂nnten jetzt jedes Modell einzeln tunen, wenn wir wollen. 11.10.6.1 Baum tree_wf &lt;- workflow() %&gt;% add_model(tree_model) %&gt;% add_recipe(churn_recipe1) tic() tree_fit &lt;- tree_wf %&gt;% tune_grid( resamples = churn_folds, metrics = metric_set(roc_auc, sens, spec) ) toc() ## 17.51 sec elapsed Im Standard werden 10 Modellkandidaten getuned. tree_fit ## # Tuning results ## # 5-fold cross-validation ## # A tibble: 5 √ó 4 ## splits id .metrics .notes ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2393/599]&gt; Fold1 &lt;tibble [30 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; ## 2 &lt;split [2393/599]&gt; Fold2 &lt;tibble [30 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; ## 3 &lt;split [2394/598]&gt; Fold3 &lt;tibble [30 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; ## 4 &lt;split [2394/598]&gt; Fold4 &lt;tibble [30 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; ## 5 &lt;split [2394/598]&gt; Fold5 &lt;tibble [30 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; show_best(tree_fit) ## # A tibble: 5 √ó 9 ## cost_complexity tree_depth min_n .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.000419 15 28 roc_auc binary 0.917 5 0.00233 ## 2 0.00000000466 11 19 roc_auc binary 0.916 5 0.00345 ## 3 0.000000211 7 21 roc_auc binary 0.915 5 0.00268 ## 4 0.0000000271 12 13 roc_auc binary 0.906 5 0.00157 ## 5 0.00000522 5 3 roc_auc binary 0.904 5 0.00422 ## # ‚Ä¶ with 1 more variable: .config &lt;chr&gt; autoplot(tree_fit) 11.10.6.2 RF Was f√ºr Tuningparameter hat den der Algorithmus bzw. seine Implementierung? show_model_info(&quot;rand_forest&quot;) ## Information for `rand_forest` ## modes: unknown, classification, regression, censored regression ## ## engines: ## classification: randomForest, ranger, spark ## regression: randomForest, ranger, spark ## ## arguments: ## ranger: ## mtry --&gt; mtry ## trees --&gt; num.trees ## min_n --&gt; min.node.size ## randomForest: ## mtry --&gt; mtry ## trees --&gt; ntree ## min_n --&gt; nodesize ## spark: ## mtry --&gt; feature_subset_strategy ## trees --&gt; num_trees ## min_n --&gt; min_instances_per_node ## ## fit modules: ## engine mode ## ranger classification ## ranger regression ## randomForest classification ## randomForest regression ## spark classification ## spark regression ## ## prediction modules: ## mode engine methods ## classification randomForest class, prob, raw ## classification ranger class, conf_int, prob, raw ## classification spark class, prob ## regression randomForest numeric, raw ## regression ranger conf_int, numeric, raw ## regression spark numeric Da die Berechnung einiges an Zeit braucht, kann man das (schon fr√ºher einmal berechnete) Ergebnisobjekt von der Festplatte lesen (sofern es existiert). Ansonsten berechnet man neu: if (file.exists(&quot;objects/rf_fit1.rds&quot;)){ rf_fit1 &lt;- read_rds(&quot;objects/rf_fit1.rds&quot;) } else { rf_wf1 &lt;- workflow() %&gt;% add_model(rf_model) %&gt;% add_recipe(churn_recipe1) tic() rf_fit1 &lt;- rf_wf1 %&gt;% tune_grid( resamples = churn_folds, metrics = metric_set(roc_auc, sens, spec) ) toc() } So kann man das berechnete Objekt abspeichern auf Festplatte, um k√ºnftig Zeit zu sparen: write_rds(rf_fit1, file = &quot;objects/rf_fit1.rds&quot;) rf_fit1 ## # Tuning results ## # 5-fold cross-validation ## # A tibble: 5 √ó 4 ## splits id .metrics .notes ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2393/599]&gt; Fold1 &lt;tibble [30 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; ## 2 &lt;split [2393/599]&gt; Fold2 &lt;tibble [30 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; ## 3 &lt;split [2394/598]&gt; Fold3 &lt;tibble [30 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; ## 4 &lt;split [2394/598]&gt; Fold4 &lt;tibble [30 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; ## 5 &lt;split [2394/598]&gt; Fold5 &lt;tibble [30 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; show_best(rf_fit1) ## # A tibble: 5 √ó 9 ## mtry trees min_n .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 6 1686 18 roc_auc binary 0.958 5 0.00330 Preprocessor1_Model03 ## 2 5 747 34 roc_auc binary 0.958 5 0.00324 Preprocessor1_Model10 ## 3 10 818 22 roc_auc binary 0.956 5 0.00378 Preprocessor1_Model01 ## 4 8 342 2 roc_auc binary 0.955 5 0.00361 Preprocessor1_Model09 ## 5 13 1184 25 roc_auc binary 0.954 5 0.00423 Preprocessor1_Model08 11.10.6.3 XGBoost boost_wf1 &lt;- workflow() %&gt;% add_model(boost_model) %&gt;% add_recipe(churn_recipe1) tic() boost_fit1 &lt;- boost_wf1 %&gt;% tune_grid( resamples = churn_folds, metrics = metric_set(roc_auc, sens, spec) ) toc() Wieder auf Festplatte speichern: write_rds(boost_fit1, file = &quot;objects/boost_fit1.rds&quot;) Und so weiter. 11.10.7 Workflow-Set tunen if (file.exists(&quot;objects/churn_model_set.rds&quot;)) { churn_model_set &lt;- read_rds(&quot;objects/churn_model_set.rds&quot;) } else { tic() churn_model_set &lt;- all_workflows %&gt;% workflow_map( resamples = churn_folds, grid = 20, metrics = metric_set(roc_auc), seed = 42, # reproducibility verbose = TRUE) toc() } Da die Berechnung schon etwas Zeit braucht, macht es Sinn, das Modell (bzw. das Ergebnisobjekt) auf Festplatte zu speichern: write_rds(churn_model_set, file = &quot;objects/churn_model_set.rds&quot;) Entsprechend kann man das Modellobjekt wieder importieren, wenn einmal abgespeichert: churn_model_set &lt;- read_rds(file = &quot;objects/churn_model_set.rds&quot;) 11.10.8 Ergebnisse im Train-Sest Hier ist die Rangfolge der Modelle, geordnet nach mittlerem ROC AUC: rank_results(churn_model_set, rank_metric = &quot;roc_auc&quot;) ## # A tibble: 122 √ó 9 ## wflow_id .config .metric mean std_err n preprocessor model rank ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 rec2_boost1 Preprocesso‚Ä¶ roc_auc 0.963 0.00104 5 recipe boos‚Ä¶ 1 ## 2 rec1_boost1 Preprocesso‚Ä¶ roc_auc 0.963 0.00104 5 recipe boos‚Ä¶ 2 ## 3 rec2_boost1 Preprocesso‚Ä¶ roc_auc 0.961 0.00106 5 recipe boos‚Ä¶ 3 ## 4 rec1_boost1 Preprocesso‚Ä¶ roc_auc 0.961 0.00106 5 recipe boos‚Ä¶ 4 ## 5 rec2_glm1 Preprocesso‚Ä¶ roc_auc 0.961 0.00272 5 recipe logi‚Ä¶ 5 ## 6 rec1_boost1 Preprocesso‚Ä¶ roc_auc 0.961 0.00102 5 recipe boos‚Ä¶ 6 ## 7 rec2_boost1 Preprocesso‚Ä¶ roc_auc 0.961 0.00102 5 recipe boos‚Ä¶ 7 ## 8 rec2_boost1 Preprocesso‚Ä¶ roc_auc 0.960 0.00120 5 recipe boos‚Ä¶ 8 ## 9 rec1_boost1 Preprocesso‚Ä¶ roc_auc 0.960 0.00120 5 recipe boos‚Ä¶ 9 ## 10 rec1_rf1 Preprocesso‚Ä¶ roc_auc 0.960 0.00278 5 recipe rand‚Ä¶ 10 ## # ‚Ä¶ with 112 more rows autoplot(churn_model_set, metric = &quot;roc_auc&quot;) 11.10.9 Bestes Modell Und hier nur der beste Kandidat pro Algorithmus: autoplot(churn_model_set, metric = &quot;roc_auc&quot;, select_best = &quot;TRUE&quot;) + geom_text(aes(y = mean - .01, label = wflow_id), angle = 90, hjust = 1) + theme(legend.position = &quot;none&quot;) + lims(y = c(0.85, 1)) Boosting hat - knapp - am besten abgeschnitten. Allerdings sind Random Forest und die schlichte, einfache logistische Regression auch fast genau so gut. Das w√§re ein Grund f√ºr das einfachste Modell, das GLM, zu votieren. Zumal die Interpretierbarkeit am besten ist. Alternativ k√∂nnte man sich f√ºr das Boosting-Modell aussprechen. Man kann sich das beste Submodell auch von Tidymodels bestimmen lassen. Das scheint aber (noch) nicht f√ºr ein Workflow-Set zu funktionieren, sondern nur f√ºr das Ergebnisobjekt von tune_grid. select_best(churn_model_set, metric = &quot;roc_auc&quot;) ## Error in `is_metric_maximize()`: ## ! Please check the value of `metric`. rf_fit1 haben wir mit tune_grid() berechnet; mit diesem Modell kann select_best() arbeiten: select_best(rf_fit1) ## # A tibble: 1 √ó 4 ## mtry trees min_n .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 6 1686 18 Preprocessor1_Model03 Aber wir k√∂nnen uns h√§ndisch behelfen. Schauen wir uns mal die Metriken (Vorhersageg√ºte) an: churn_model_set %&gt;% collect_metrics() %&gt;% arrange(-mean) ## # A tibble: 122 √ó 9 ## wflow_id .config preproc model .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 rec1_boost1 Preprocesso‚Ä¶ recipe boos‚Ä¶ roc_auc binary 0.963 5 0.00104 ## 2 rec2_boost1 Preprocesso‚Ä¶ recipe boos‚Ä¶ roc_auc binary 0.963 5 0.00104 ## 3 rec1_boost1 Preprocesso‚Ä¶ recipe boos‚Ä¶ roc_auc binary 0.961 5 0.00106 ## 4 rec2_boost1 Preprocesso‚Ä¶ recipe boos‚Ä¶ roc_auc binary 0.961 5 0.00106 ## 5 rec2_glm1 Preprocesso‚Ä¶ recipe logi‚Ä¶ roc_auc binary 0.961 5 0.00272 ## 6 rec1_boost1 Preprocesso‚Ä¶ recipe boos‚Ä¶ roc_auc binary 0.961 5 0.00102 ## 7 rec2_boost1 Preprocesso‚Ä¶ recipe boos‚Ä¶ roc_auc binary 0.961 5 0.00102 ## 8 rec1_boost1 Preprocesso‚Ä¶ recipe boos‚Ä¶ roc_auc binary 0.960 5 0.00120 ## 9 rec2_boost1 Preprocesso‚Ä¶ recipe boos‚Ä¶ roc_auc binary 0.960 5 0.00120 ## 10 rec1_rf1 Preprocesso‚Ä¶ recipe rand‚Ä¶ roc_auc binary 0.960 5 0.00278 ## # ‚Ä¶ with 112 more rows rec1_boost1 scheint das beste Modell zu sein. best_model_params &lt;- extract_workflow_set_result(churn_model_set, &quot;rec1_boost1&quot;) %&gt;% select_best() best_model_params ## # A tibble: 1 √ó 4 ## mtry trees min_n .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 6 80 21 Preprocessor1_Model05 11.10.10 Finalisisieren Wir entscheiden uns mal f√ºr das Boosting-Modell, rec1_boost1. Diesen Workflow, in finalisierter Form, brauchen wir f√ºr den ‚Äúfinal Fit‚Äù. Finalisierte Form hei√üt: Schritt 1: Nimm den passenden Workflow, hier rec1 und boost1; das hatte uns oben rank_results() verraten. Schritt 2: Update (Finalisiere) ihn mit den besten Tuningparameter-Werten # Schritt 1: best_wf &lt;- all_workflows %&gt;% extract_workflow(&quot;rec1_boost1&quot;) best_wf ## ‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ## Preprocessor: Recipe ## Model: boost_tree() ## ## ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ## 2 Recipe Steps ## ## ‚Ä¢ step_normalize() ## ‚Ä¢ step_dummy() ## ## ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ## Boosted Tree Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = tune() ## min_n = tune() ## ## Engine-Specific Arguments: ## nthreads = parallel::detectCores() ## ## Computational engine: xgboost Jetzt finalisieren wir den Workflow, d.h. wir setzen die Parameterwerte des besten Submodells ein: # Schritt 2: best_wf_finalized &lt;- best_wf %&gt;% finalize_workflow(best_model_params) best_wf_finalized ## ‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ## Preprocessor: Recipe ## Model: boost_tree() ## ## ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ## 2 Recipe Steps ## ## ‚Ä¢ step_normalize() ## ‚Ä¢ step_dummy() ## ## ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ## Boosted Tree Model Specification (classification) ## ## Main Arguments: ## mtry = 6 ## trees = 80 ## min_n = 21 ## ## Engine-Specific Arguments: ## nthreads = parallel::detectCores() ## ## Computational engine: xgboost 11.10.11 Last Fit fit_final &lt;- best_wf_finalized %&gt;% last_fit(churn_split) fit_final ## # Resampling results ## # Manual resampling ## # A tibble: 1 √ó 6 ## splits id .metrics .notes .predictions .workflow ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2992/998]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt; &lt;workflow&gt; collect_metrics(fit_final) ## # A tibble: 2 √ó 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.903 Preprocessor1_Model1 ## 2 roc_auc binary 0.966 Preprocessor1_Model1 11.10.12 Variablenrelevanz Um die Variablenrelevanz zu plotten, m√ºssen wir aus dem Tidymodels-Ergebnisobjekt das eigentliche Ergebnisobjekt herausziehen, von der R-Funktion, die die eigentliche Berechnung durchf√ºhrt, das w√§re glm() bei einer logistischen Regression oder xgboost::xgb.train() bei XGBoost: fit_final %&gt;% extract_fit_parsnip() ## parsnip model object ## ## ##### xgb.Booster ## raw: 114.1 Kb ## call: ## xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, ## colsample_bytree = 1, colsample_bynode = 0.285714285714286, ## min_child_weight = 21L, subsample = 1, objective = &quot;binary:logistic&quot;), ## data = x$data, nrounds = 80L, watchlist = x$watchlist, verbose = 0, ## nthreads = 8L, nthread = 1) ## params (as set within xgb.train): ## eta = &quot;0.3&quot;, max_depth = &quot;6&quot;, gamma = &quot;0&quot;, colsample_bytree = &quot;1&quot;, colsample_bynode = &quot;0.285714285714286&quot;, min_child_weight = &quot;21&quot;, subsample = &quot;1&quot;, objective = &quot;binary:logistic&quot;, nthreads = &quot;8&quot;, nthread = &quot;1&quot;, validate_parameters = &quot;TRUE&quot; ## xgb.attributes: ## niter ## callbacks: ## cb.evaluation.log() ## # of features: 21 ## niter: 80 ## nfeatures : 21 ## evaluation_log: ## iter training_logloss ## 1 0.600389 ## 2 0.495920 ## --- ## 79 0.197130 ## 80 0.196439 Dieses Objekt √ºbergeben wir dann an {vip}: fit_final %&gt;% extract_fit_parsnip() %&gt;% vip() 11.10.13 ROC-Curve Eine ROC-Kurve berechnet Sensitivit√§t und Spezifit√§t aus den Vorhersagen, bzw. aus dem Vergleich von Vorhersagen und wahrem Wert (d.h. der beobachtete Wert). Ziehen wir also zuerst die Vorhersagen heraus: fit_final %&gt;% collect_predictions() ## # A tibble: 998 √ó 7 ## id .pred_yes .pred_no .row .pred_class canceled_service .config ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; ## 1 train/test spl‚Ä¶ 0.00669 0.993 1 no yes Prepro‚Ä¶ ## 2 train/test spl‚Ä¶ 0.915 0.0847 6 yes yes Prepro‚Ä¶ ## 3 train/test spl‚Ä¶ 0.959 0.0408 15 yes yes Prepro‚Ä¶ ## 4 train/test spl‚Ä¶ 0.997 0.00291 16 yes yes Prepro‚Ä¶ ## 5 train/test spl‚Ä¶ 0.901 0.0994 17 yes yes Prepro‚Ä¶ ## 6 train/test spl‚Ä¶ 0.992 0.00765 18 yes yes Prepro‚Ä¶ ## 7 train/test spl‚Ä¶ 0.965 0.0349 28 yes yes Prepro‚Ä¶ ## 8 train/test spl‚Ä¶ 0.941 0.0589 31 yes yes Prepro‚Ä¶ ## 9 train/test spl‚Ä¶ 0.703 0.297 32 yes yes Prepro‚Ä¶ ## 10 train/test spl‚Ä¶ 0.988 0.0115 41 yes yes Prepro‚Ä¶ ## # ‚Ä¶ with 988 more rows Praktischerweise werden die ‚Äúwahren Werte‚Äù (also die beobachtaten Werte), canceled_service, ausch angegeben. Dann berechnen wir die roc_curve und autoplotten sie. fit_final %&gt;% collect_predictions() %&gt;% roc_curve(canceled_service, .pred_yes) %&gt;% autoplot() 11.11 Aufgaben Einfache Durchf√ºhrung eines Modellierung mit XGBoost Fallstudie Oregon Schools Fallstudie Churn Fallstudie Ikea Fallstudie Wasserquellen in Sierra Leone Fallstudie B√§ume in San Francisco Fallstudie Vulkanausbr√ºche Fallstudie Brettspiele mit XGBoost References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
