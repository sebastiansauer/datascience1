[["statistisches-lernen-2.html", "Kapitel 9 Statistisches Lernen, 2 9.1 Resampling 9.2 Illustration des Resampling 9.3 Gesetz der großen Zahl 9.4 Über- und Unteranpassung an einem Beispiel 9.5 Tuning 9.6 Tuning mit Tidymodels", " Kapitel 9 Statistisches Lernen, 2 Benötigte R-Pakete: library(tidyverse) library(tidymodels) library(tune) library(AmesHousing) library(vip) # Prädiktorenrelevanz Das Paket tune auf CRAN hat einen Bug1, daher empfiehlt der Autor, Max Kuhn, die Version des Pakets von Github zu installieren: devtools::install_github(&quot;tidymodels/tune&quot;) 9.1 Resampling Vergleichen Sie die drei Fälle, die sich in der Nutzung von Train- und Test-Sample unterscheiden: Wir fitten ein Klassifikationsmodell in einer Stichprobe, sagen die Y-Werte dieser Stichprobe “vorher”. Wir finden eine Gesamtgenauigkeit von 80%. Wir fitten ein Klassifikationsmodell in einem Teil der ursprünglichen Stichprobe (Train-Sample) und sagen Y-die Werte im verbleibenden Teil der ursprünglichen Stichprobe vorher (Test-Sample). Wir finden eine Gesamtgenauigkeit von 70%. Wir wiederholen Fall 2 noch drei Mal mit jeweils anderer Zuweisung der Fälle zum Train- bzw. zum Test-Sample. Wir finden insgesamt folgende Werte an Gesamtgenauigkeit: 70%, 70%, 65%, 75%. Welchen der drei Fälle finden Sie am sinnvollsten? Warum? 9.2 Illustration des Resampling Resampling stellt einen Oberbegriff dar; Kreuzvalidierung ist ein Unterbegriff dazu. Es gibt noch andere Arten des Resampling, etwa Bootstrapping oder Leave-One-Out-Cross-Validation (LOOCV). Im Folgenden ist nur die Kreuzvalidierung dargestellt, da es eines der wichtigsten und vielleicht das Wichtigste ist. In vielen Quellen finden sich Erläuterungen anderer Verfahren dargestellt, etwa in Silge and Kuhn (2022), James et al. (2021) oder Rhys (2020). 9.2.1 Einfache v-fache Kreuzvalidierung Abb. 9.1 illustriert die zufällige Aufteilung von \\(n=10\\) Fällen der Originalstrichprobe auf eine Train- bzw. Test-Stichpobe. Man spricht von Kreuzvalidierung (cross validation, CV). In diesem Fall wurden 70% der (\\(n=10\\)) Fälle der Train-Stichprobe zugewiesen (der Rest der Test-Stichprobe); ein willkürlicher, aber nicht unüblicher Anteil. Diese Aufteilung wurde \\(v=3\\) Mal vorgenommen, es resultieren drei “Resampling-Stichproben”, die manchmal auch als “Faltungen” bezeichnet werden. Figure 9.1: Resampling: Eine Stichprobe wird mehrfach (hier 3 Mal) zu 70% in ein Train- und zu 30% in die Test-Stichprobe aufgeteilt Sauer (2019) stellt das Resampling so dar (S. 259), s. Abb. 9.2. Figure 9.2: Kreuzvalidierung, Aufteilung in Train- vs. Testsample Der Gesamtfehler der Vorhersage wird als Mittelwerte der Vorhersagefehler in den einzelnen Faltungen berechnet. Warum ist die Vorhersage besser, wenn man mehrere Faltungen, mehrere Schätzungen für \\(y\\) also, vornimmt? Der Grund ist das Gesetz der großen Zahl, nachdem sich eine Schätzung in Mittelwert und Variabilität stabilisiert mit steigendem Stichprobenumfang, dem wahren Mittelwert also präziser schätzt. Bei Normalverteilungen klappt das gut, bei randlastigen Verteilungen leider nicht mehr (Taleb 2019). Häufig werden \\(v=10\\) Faltungen verwendet, was sich empirisch als guter Kompromiss von Rechenaufwand und Fehlerreduktion herausgestellt hat. 9.2.2 Wiederholte Kreuzvalidierung Die \\(r\\)-fach wiederholte Kreuzvalidierung wiederholte die einfache Kreuzvalidierung mehrfach (nämlich \\(r=4\\) mal), Sauer (2019) stellt das Resampling so dar (S. 259), s. Abb. 9.3. Figure 9.3: Wiederholte Kreuzvalidierung Die wiederholte Kreuzvalidierung reduziert den Standardfehler der Vorhersagen. Silge and Kuhn (2022) zeigen die Verringerung des Schätzfehlers als Funktion der \\(r\\) Wiederholungen dar, s. Abb. 9.4. Figure 9.4: Reduktion des Schätzfehlers als Funktion der r Wiederhoulugen der Kreuzvalidierung Warum ist die Wiederholung der Kreuzvalidierung nützlich? Die Kreuvalidierung liefert einen Schätzwert der Modellparameter, die wahren Modellparameter werden also anhand einer Stichprobe von \\(n=1\\) geschätzt. Mit höherem Stichprobenumfang kann diese Schätzung natürlich präzisiert werden. Da jede Stichprobenverteilung bei \\(n \\rightarrow \\infty\\) normalverteilt ist - ein zentrales Theorem der Statistik, der Zentrale Grenzwertsatz (Central Limit Theorem) - kann man hoffen, dass sich eine bestimmte Stichprobenverteilung bei kleinerem \\(n\\) ebenfalls annähernd normalverteilt2. Dann sind die Quantile bekannt und man kann die Streuung der Schätzers, \\({\\sigma }_{\\bar {x}}\\), z.B. für den Mittelwert, einfach schätzen: \\[{\\displaystyle {\\sigma }_{\\bar {x}}\\ ={\\frac {\\sigma }{\\sqrt {n}}}}\\] 9.2.3 Resampling passiert im Train-Sample Wichtig zu beachten ist, dass die Resampling nur im Train-Sample stattfindet. Das Test-Sample bleibt unangerührt. Dieser Sachverhalt ist in Abb. 9.5, aus Silge and Kuhn (2022), illustriert. Figure 9.5: Resampling im Train-, nicht im Test-Sample Wie in Abb. 9.5 dargestellt, wird das Modell im Analyse-Sample berechnet (gefittet), und im Assessment-Sample auf Modellgüte hin überprüft. Die letztliche Modellgüte ist dann die Zusammenfassung (Mittelwert) der einzelnen Resamples. 9.2.4 Andere Illustrationen Es gibt eine Reihe vergleichbarer Illustrationen in anderen Büchern: Timbers, Campbell &amp; Lee, 2022, Kap. 6 Silge &amp; Kuhn, 2022, Abb. 10.1 Silge &amp; Kuhn, 2022, Abb. 10.2 Silge &amp; Kuhn, 2022, Abb. 10.3 James, Witten, hastie &amp; Tishirani, 2021, Abb. 5.3 9.3 Gesetz der großen Zahl Nach dem Gesetz der großen Zahl (Law of Large Numbers) sollte sich der Mittelwert einer großen Stichprobe dem theoretischen Mittelwert der zugrundeliegenden Verteilung (Population, datengeneriender Prozess) sehr nahe kommen. \\[\\displaystyle \\lim _{n\\to \\infty }\\sum _{i=1}^{n}{\\frac {X_{i}}{n}}={\\overline {X}}\\] David Salazar visualisiert das folgendermaßen in diesem Post seines lesenswerten Blogs, s. Abb. 9.6. # source: https://david-salazar.github.io/2020/04/17/fat-vs-thin-does-lln-work/ samples &lt;- 1000 thin &lt;- rnorm(samples, sd = 20) cumulative_mean &lt;- function(numbers) { x &lt;- seq(1, length(numbers)) cum_mean &lt;- cumsum(numbers)/x cum_mean } thin_cum_mean &lt;- cumulative_mean(thin) thin_cum_mean %&gt;% tibble(running_mean = .) %&gt;% add_rownames(var = &#39;number_samples&#39;) %&gt;% mutate(number_samples = as.double(number_samples)) %&gt;% arrange(number_samples) %&gt;% ggplot(aes(x = number_samples, y = running_mean)) + geom_line(color = &#39;dodgerblue4&#39;) + geom_hline(yintercept = 0, linetype = 2, color = &#39;red&#39;) + hrbrthemes::theme_ipsum_rc(grid = &#39;Y&#39;) + scale_x_continuous(labels = scales::comma) + labs(x = &quot;Stichprobengröße&quot;, title = &quot;Gesetz der großen Zahl&quot;, subtitle = &quot;Kumulierter Mittelwert aus einer Normalverteilung mit sd=20&quot;) Figure 9.6: Gesetz der großen Zahl Wie man sieht, nähert sich der empirische Mittelwert (also in der Stichprobe) immer mehr dem theoretischen Mittelwert, 0, an. Achtung: Bei randlastigen Verteilungen darf man dieses schöne, wohlerzogene Verhalten nicht erwarten (Taleb 2019). 9.4 Über- und Unteranpassung an einem Beispiel Figure 9.7: Welches Modell (Teile C-E) passt am besten zu den Daten (Teil B)? Die ‘wahre Funktion’, der datengenerierende Prozess ist im Teil A dargestellt Abb. 9.7 zeigt: Teil A: Die ‘wahre Funktion’, \\(f\\), die die Daten erzeugt. Man spricht auch von der “datengenerierenden Funktion”. Wir gehen gemeinhin davon aus, dass es eine wahre Funktion gibt. Das heißt nicht, dass die wahre Funktion die Daten perfekt erklärt, schließlich kann die Funktion zwar wahr, aber unvollständig sein oder unsere Messinstrumente sind nicht perfekt präzise. Teil B: Die Daten, erzeugt aus A plus etwas zufälliges Fehler (Rauschen). Teil C: Ein zu einfaches Modell: Unteranpassung. Vorhersagen in einer neuen Stichprobe (basierend auf dem datengenerierenden Prozess aus A) werden nicht so gut sein. Teil D: Ein zu komplexes Modell: Überanpassung. Vorhersagen in einer neuen Stichprobe (basierend auf dem datengenerierenden Prozess aus A) werden nicht so gut sein. Teil E: Ein Modell mittlerer Komplexität. Keine Überanpassung, keine Unteranpassung. Vorhersagen in einer neuen Stichprobe (basierend auf dem datengenerierenden Prozess aus A) werden gut sein. 9.5 Tuning 9.5.1 Grid Search vs. Iterative Search Im K-Nächste-Nachbarn-Modell ist der vorhergesagt Wert, \\(\\hat{y}\\) für eine neue Beobachtung \\(x_0\\) wie folgt definiert: \\[ \\hat y = \\frac{1}{K}\\sum_{\\ell = 1}^K x_\\ell^*, \\] wobei \\(K\\) die Anzahl der zu berücksichtigen nächsten Nachbarn darstellt und \\(x_\\ell^*\\) die Werte dieser berücksichtiggten Nachbarn. Die Wahl von \\(K\\) hat einen gewaltigen Einfluss auf die Vorhersagen und damit auf die Vorhersagegüte. Allerdings wird \\(K\\) nicht vom Modell geschätzt. Es liegt an den Nutzi, diesen Wert zu wählen. Parameter dieser Art (die von den Nutzi zu bestimmen sind, nicht vom Algorithmus), nennt man Tuningparameter. Abbildung 9.8 aus Silge and Kuhn (2022) stellt exemplarisch dar, welchen großen Einfluss die Wahl des Werts eines Tuningparameters auf die Vorhersagen eines Modells haben. Figure 9.8: Overfitting als Funktion der Modellparameter und insofern als Problem de Wahl der Tuningparameter Aber wie wählt man “gute” Werte der Tuningparater? Zwei Ansätze, grob gesprochen, bieten sich an. Grid Search: Probiere viele Werte aus und schaue, welcher der beste ist. Dabei musst du hoffen, dass du die Werte erwischt, die nicht nur im Train-, sondern auch im Test-Sample gut funktionieren werden. Iterative Search: Wenn du einen Wert eines Tuningparameters hast, nutze diesen, um intelligenter einen neuen Wert eines Tuningparameters zu finden. Der Unterschied beider Ansätze ist in Silge and Kuhn (2022) wie in Abb. 9.9 dargestellt. Figure 9.9: Links: Grid Search. Rechts: Iterative Search2 In tidymodels kann man mit tune() angeben, dass man einen bestimmten Parameter tunen möchte. tidymodels führt das dann ohne weiteres Federlesens für uns durch. 9.6 Tuning mit Tidymodels 9.6.1 Tuning definieren mit tidymodels für kNN 9.6.1.1 Datensatz aufteilen ames &lt;- make_ames() set.seed(4595) data_split &lt;- initial_split(ames, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(data_split) ames_test &lt;- testing(data_split) set.seed(2453) rs_splits &lt;- vfold_cv(ames_train, strata = &quot;Sale_Price&quot;) 9.6.1.2 Rezept, Modell und Workflow definieren ames_rec &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;% step_log(Sale_Price, base = 10) %&gt;% step_other(Neighborhood, threshold = .1) %&gt;% step_dummy(all_nominal()) %&gt;% step_zv(all_predictors()) knn_model &lt;- nearest_neighbor( mode = &quot;regression&quot;, neighbors = tune(&quot;K&quot;) ) %&gt;% set_engine(&quot;kknn&quot;) ames_wflow &lt;- workflow() %&gt;% add_recipe(ames_rec) %&gt;% add_model(knn_model) Das kNN-Modell ist noch nicht berechnet, es ist nur ein “Rezept” erstellt: knn_model ## K-Nearest Neighbor Model Specification (regression) ## ## Main Arguments: ## neighbors = tune(&quot;K&quot;) ## ## Computational engine: kknn ames_wflow ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: nearest_neighbor() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 4 Recipe Steps ## ## • step_log() ## • step_other() ## • step_dummy() ## • step_zv() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## K-Nearest Neighbor Model Specification (regression) ## ## Main Arguments: ## neighbors = tune(&quot;K&quot;) ## ## Computational engine: kknn 9.6.1.3 Tuningparameter betrachten Möchte man wissen, welche und wie viele Tuningparameter tidymodels in einem Modell berücksichtigt, kann man extract_parameter_set_dials() aufrufen: extract_parameter_set_dials(ames_wflow) ## Collection of 1 parameters for tuning ## ## identifier type object ## K neighbors nparam[+] Die Ausgabe informiert uns, dass es nur einen Tuningparameter gibt in diesem Modell und dass der Name (Label, ID) des Tuningparameters “K” ist. Außerdem sollen die Anzahl der Nachbarn getunt werden. Der Tuningparameter ist numerisch; das sieht man an nparam[+]. Schauen wir uns mal an, auf welchen Wertebereich tidymodels den Parameter \\(K\\) begrenzt hat: ames_wflow %&gt;% extract_parameter_dials(&quot;K&quot;) ## # Nearest Neighbors (quantitative) ## Range: [1, 15] Wir können auch Einfluss nehmen und angeben, dass die Grenzen des Wertebereichs zwischen 1 und 50 liegen soll (für den Tuningparameter neighbors): ames_set &lt;- extract_parameter_set_dials(ames_wflow) %&gt;% update(K = neighbors(c(1, 50))) ames_set ## Collection of 1 parameters for tuning ## ## identifier type object ## K neighbors nparam[+] 9.6.2 Datenabhängige Tuningparameter Manche Tuningparameter kann man nur bestimmen, wenn man den Datensatz kennt. So ist die Anzahl der Prädiktoren, mtry in einem Random-Forest-Modell sinnvollerweise als Funktion der Prädiktorenzahl zu wählen. Der Workflow kennt aber den Datensatz nicht. Daher muss der Workflow noch “finalisiert” oder “aktualisiert” werden, um den Wertebereich (Unter- und Obergrenze) eines Tuningparameters zu bestimmen. Wenn wir im Rezept aber z.B. die Anzahl der Prädiktoren verändert haben, möchten wir die Grenzen des Wertebereichs für mtry (oder andere Tuningparameter) vielleicht nicht händisch, “hartverdrahtet” selber bestimmen, sondern lieber den Computer anweisen, und sinngemäß sagen: “Warte mal mit der Bestimmung der Werte der Tuningparameter, bis du den Datensatz bzw. dessen Dimensionen kennst. Merk dir, dass du, wenn du den Datensatz kennst, die Werte des Tuningparameter noch ändern musst. Und tu das dann auch.” Dazu später mehr. ames_set &lt;- workflow() %&gt;% add_model(knn_model) %&gt;% add_recipe(ames_rec) %&gt;% extract_parameter_set_dials() %&gt;% finalize(ames_train) 9.6.3 Modelle mit Tuning berechnen Nachdem wir die Tuningwerte bestimmt haben, können wir jetzt das Modell berechnen: Für jeden Wert des Tuningparameters wird ein Modell berechnet: ames_grid_search &lt;- tune_grid( ames_wflow, resamples = rs_splits ) ames_grid_search ## # Tuning results ## # 10-fold cross-validation using stratification ## # A tibble: 10 × 4 ## splits id .metrics .notes ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [1976/221]&gt; Fold01 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 2 &lt;split [1976/221]&gt; Fold02 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 3 &lt;split [1976/221]&gt; Fold03 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 4 &lt;split [1976/221]&gt; Fold04 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 5 &lt;split [1977/220]&gt; Fold05 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 6 &lt;split [1977/220]&gt; Fold06 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 7 &lt;split [1978/219]&gt; Fold07 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 8 &lt;split [1978/219]&gt; Fold08 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 9 &lt;split [1979/218]&gt; Fold09 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 10 &lt;split [1980/217]&gt; Fold10 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt; Im Default berechnet tiymodels 10 Kandidatenmodelle. Die Spalte .metrics beinhaltet die Modellgüte für jedes Kandidatenmodell. ames_grid_search %&gt;% collect_metrics() ## # A tibble: 18 × 7 ## K .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2 rmse standard 0.0946 10 0.00198 Preprocessor1_Model1 ## 2 2 rsq standard 0.716 10 0.00959 Preprocessor1_Model1 ## 3 3 rmse standard 0.0901 10 0.00214 Preprocessor1_Model2 ## 4 3 rsq standard 0.739 10 0.00925 Preprocessor1_Model2 ## 5 5 rmse standard 0.0855 10 0.00234 Preprocessor1_Model3 ## 6 5 rsq standard 0.764 10 0.00961 Preprocessor1_Model3 ## 7 6 rmse standard 0.0842 10 0.00239 Preprocessor1_Model4 ## 8 6 rsq standard 0.772 10 0.00968 Preprocessor1_Model4 ## 9 8 rmse standard 0.0825 10 0.00241 Preprocessor1_Model5 ## 10 8 rsq standard 0.783 10 0.00958 Preprocessor1_Model5 ## 11 9 rmse standard 0.0819 10 0.00241 Preprocessor1_Model6 ## 12 9 rsq standard 0.786 10 0.00952 Preprocessor1_Model6 ## 13 10 rmse standard 0.0814 10 0.00242 Preprocessor1_Model7 ## 14 10 rsq standard 0.790 10 0.00951 Preprocessor1_Model7 ## 15 12 rmse standard 0.0807 10 0.00243 Preprocessor1_Model8 ## 16 12 rsq standard 0.795 10 0.00948 Preprocessor1_Model8 ## 17 14 rmse standard 0.0803 10 0.00245 Preprocessor1_Model9 ## 18 14 rsq standard 0.799 10 0.00950 Preprocessor1_Model9 Das können wir uns einfach visualisieren lassen: autoplot(ames_grid_search) Auf Basis dieser Ergebnisse könnte es Sinn machen, noch größere Werte für \\(K\\) zu überprüfen. 9.6.4 Vorhersage im Test-Sample Welches Modellkandidat war jetzt am besten? show_best(ames_grid_search) ## # A tibble: 5 × 7 ## K .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 14 rmse standard 0.0803 10 0.00245 Preprocessor1_Model9 ## 2 12 rmse standard 0.0807 10 0.00243 Preprocessor1_Model8 ## 3 10 rmse standard 0.0814 10 0.00242 Preprocessor1_Model7 ## 4 9 rmse standard 0.0819 10 0.00241 Preprocessor1_Model6 ## 5 8 rmse standard 0.0825 10 0.00241 Preprocessor1_Model5 Wählen wir jetzt mal das beste Modell aus (im Sinne des Optimierungskriteriusms): select_best(ames_grid_search) ## # A tibble: 1 × 2 ## K .config ## &lt;int&gt; &lt;chr&gt; ## 1 14 Preprocessor1_Model9 Ok, notieren wir uns die Kombination der Tuningparameterwerte im besten Kandiatenmodell. In diesem Fall hat das Modull nur einen Tuningparameter: ames_knn_best_params &lt;- tibble(K = 15) Unser Workflow weiß noch nicht, welche Tuningparameterwerte am besten sind: ames_wflow ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: nearest_neighbor() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 4 Recipe Steps ## ## • step_log() ## • step_other() ## • step_dummy() ## • step_zv() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## K-Nearest Neighbor Model Specification (regression) ## ## Main Arguments: ## neighbors = tune(&quot;K&quot;) ## ## Computational engine: kknn neighbors = tune(\"K\") sagt uns, dass er diesen Parameter tunen will. Das haben wir jetzt ja erledigt. Wir wollen für das Test-Sample nur noch einen Wert, eben aus dem besten Kandidatenmodell, verwenden: ames_final_wflow &lt;- ames_wflow %&gt;% finalize_workflow(ames_knn_best_params) ames_final_wflow ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: nearest_neighbor() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 4 Recipe Steps ## ## • step_log() ## • step_other() ## • step_dummy() ## • step_zv() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## K-Nearest Neighbor Model Specification (regression) ## ## Main Arguments: ## neighbors = 15 ## ## Computational engine: kknn Wie man sieht, steht im Workflow nichts mehr von Tuningparameter. Wir können jetzt das ganze Train-Sample fitten, also das Modell auf das ganze Train-Sample anwenden - nicht nur auf ein Analysis-Sample. Und mit den dann resultierenden Modellkoeffizienten sagen wir das TestSample vorher: final_ames_knn_fit &lt;- last_fit(ames_final_wflow, data_split) final_ames_knn_fit ## # Resampling results ## # Manual resampling ## # A tibble: 1 × 6 ## splits id .metrics .notes .predictions .workflow ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2197/733]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt; &lt;workflow&gt; Holen wir uns die Modellgüte: collect_metrics(final_ames_knn_fit) ## # A tibble: 2 × 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 0.0867 Preprocessor1_Model1 ## 2 rsq standard 0.780 Preprocessor1_Model1 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
