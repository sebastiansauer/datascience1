<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Kapitel 11 Regularisierte Modelle | DataScience1</title>
<meta name="author" content="Sebastian Sauer">
<meta name="description" content="11.1 Lernsteuerung  11.1.1 Lernziele Sie k√∂nnen Algorithmen f√ºr regularisierte lineare Modell erkl√§ren, d.h. Lasso- und Ridge-Regression Sie wissen, anhand welche Tuningparamter man Overfitting...">
<meta name="generator" content="bookdown 0.26.2 with bs4_book()">
<meta property="og:title" content="Kapitel 11 Regularisierte Modelle | DataScience1">
<meta property="og:type" content="book">
<meta property="og:description" content="11.1 Lernsteuerung  11.1.1 Lernziele Sie k√∂nnen Algorithmen f√ºr regularisierte lineare Modell erkl√§ren, d.h. Lasso- und Ridge-Regression Sie wissen, anhand welche Tuningparamter man Overfitting...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kapitel 11 Regularisierte Modelle | DataScience1">
<meta name="twitter:description" content="11.1 Lernsteuerung  11.1.1 Lernziele Sie k√∂nnen Algorithmen f√ºr regularisierte lineare Modell erkl√§ren, d.h. Lasso- und Ridge-Regression Sie wissen, anhand welche Tuningparamter man Overfitting...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><script src="libs/viz-1.8.2/viz.js"></script><link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet">
<script src="libs/grViz-binding-1.0.6.1/grViz.js"></script><script src="libs/es6shim-0.35.6/es6shim.js"></script><script src="libs/es7shim-6.0.0/es7shim.js"></script><script src="libs/graphre-0.1.3/graphre.js"></script><script src="libs/nomnoml-1.3.1/nomnoml.js"></script><script src="libs/nomnoml-binding-0.2.3/nomnoml.js"></script><script src="libs/d3-3.3.8/d3.min.js"></script><script src="libs/dagre-0.4.0/dagre-d3.min.js"></script><link href="libs/mermaid-0.3.0/dist/mermaid.css" rel="stylesheet">
<script src="libs/mermaid-0.3.0/dist/mermaid.slim.min.js"></script><script src="libs/chromatography-0.1/chromatography.js"></script><script src="libs/DiagrammeR-binding-1.0.6.1/DiagrammeR.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style-bs4.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="Grundlagen der Prognosemodellierung üîÆüß∞">DataScience1</a>:
        <small class="text-muted">Grundlagen der Prognosemodellierung üîÆüß∞</small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Zu diesem Buch</a></li>
<li><a class="" href="hinweise.html"><span class="header-section-number">1</span> Hinweise</a></li>
<li><a class="" href="pr%C3%BCfung.html"><span class="header-section-number">2</span> Pr√ºfung</a></li>
<li class="book-part">Themen</li>
<li><a class="" href="statistisches-lernen.html"><span class="header-section-number">3</span> Statistisches Lernen</a></li>
<li><a class="" href="r-zweiter-blick.html"><span class="header-section-number">4</span> R, zweiter Blick</a></li>
<li><a class="" href="tidymodels.html"><span class="header-section-number">5</span> tidymodels</a></li>
<li><a class="" href="knn.html"><span class="header-section-number">6</span> kNN</a></li>
<li><a class="" href="resampling-und-tuning.html"><span class="header-section-number">7</span> Resampling und Tuning</a></li>
<li><a class="" href="logistische-regression.html"><span class="header-section-number">8</span> Logistische Regression</a></li>
<li><a class="" href="entscheidungsb%C3%A4ume.html"><span class="header-section-number">9</span> Entscheidungsb√§ume</a></li>
<li><a class="" href="ensemble-lerner.html"><span class="header-section-number">10</span> Ensemble Lerner</a></li>
<li><a class="active" href="regularisierte-modelle.html"><span class="header-section-number">11</span> Regularisierte Modelle</a></li>
<li><a class="" href="kaggle.html"><span class="header-section-number">12</span> Kaggle</a></li>
<li><a class="" href="der-rote-faden.html"><span class="header-section-number">13</span> Der rote Faden</a></li>
<li><a class="" href="fallstudien.html"><span class="header-section-number">14</span> Fallstudien</a></li>
<li><a class="" href="dimensionsreduktion.html"><span class="header-section-number">15</span> Dimensionsreduktion</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/sebastiansauer/datascience1">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="regularisierte-modelle" class="section level1" number="11">
<h1>
<span class="header-section-number">Kapitel 11</span> Regularisierte Modelle<a class="anchor" aria-label="anchor" href="#regularisierte-modelle"><i class="fas fa-link"></i></a>
</h1>
<div id="lernsteuerung-8" class="section level2" number="11.1">
<h2>
<span class="header-section-number">11.1</span> Lernsteuerung<a class="anchor" aria-label="anchor" href="#lernsteuerung-8"><i class="fas fa-link"></i></a>
</h2>
<div id="lernziele-9" class="section level3" number="11.1.1">
<h3>
<span class="header-section-number">11.1.1</span> Lernziele<a class="anchor" aria-label="anchor" href="#lernziele-9"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Sie k√∂nnen Algorithmen f√ºr regularisierte lineare Modell erkl√§ren, d.h. Lasso- und Ridge-Regression</li>
<li>Sie wissen, anhand welche Tuningparamter man Overfitting bei diesen Algorithmen begrenzen kann</li>
<li>Sie k√∂nnen diese Verfahren in R berechnen</li>
</ul>
</div>
<div id="literatur-9" class="section level3" number="11.1.2">
<h3>
<span class="header-section-number">11.1.2</span> Literatur<a class="anchor" aria-label="anchor" href="#literatur-9"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Rhys, Kap. 11</li>
</ul>
</div>
<div id="hinweise-4" class="section level3" number="11.1.3">
<h3>
<span class="header-section-number">11.1.3</span> Hinweise<a class="anchor" aria-label="anchor" href="#hinweise-4"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Rhys und ISLR sind eine gute Quelle zum Einstieg in das Thema</li>
</ul>
</div>
</div>
<div id="vorbereitung-8" class="section level2" number="11.2">
<h2>
<span class="header-section-number">11.2</span> Vorbereitung<a class="anchor" aria-label="anchor" href="#vorbereitung-8"><i class="fas fa-link"></i></a>
</h2>
<p>In diesem Kapitel werden folgende R-Pakete ben√∂tigt:</p>
<div class="sourceCode" id="cb458"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidymodels.tidymodels.org">tidymodels</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/collectivemedia/tictoc">tictoc</a></span><span class="op">)</span>  <span class="co"># Zeitmessung</span></code></pre></div>
</div>
<div id="regularisierung" class="section level2" number="11.3">
<h2>
<span class="header-section-number">11.3</span> Regularisierung<a class="anchor" aria-label="anchor" href="#regularisierung"><i class="fas fa-link"></i></a>
</h2>
<div id="was-ist-regularisierung" class="section level3" number="11.3.1">
<h3>
<span class="header-section-number">11.3.1</span> Was ist Regularisierung?<a class="anchor" aria-label="anchor" href="#was-ist-regularisierung"><i class="fas fa-link"></i></a>
</h3>
<p>Regularisieren verweist auf ‚Äúregul√§r‚Äù;
laut <a href="">Duden</a> bedeutet das Wort so viel wie ‚Äúden Regeln, Bestimmungen,
Vorschriften entsprechend; vorschriftsm√§√üig, ordnungsgem√§√ü, richtig‚Äù oder ‚Äú√ºblich‚Äù.</p>
<p>Im Englischen spricht man auch von ‚Äúpenalized models‚Äù, ‚Äúbestrafte Modell‚Äù und von ‚Äúshrinkage‚Äù,
von ‚ÄúSchrumpfung‚Äù im Zusammenhang mit dieer Art von Modellen.</p>
<p>Regularisierung ist ein Metalalgorithmus, also ein Verfahren, was als zweiter Schritt ‚Äúauf‚Äù verschiedene
Modelle angewendet werden kann - zumeist aber auf lineare Modelle, worauf
wir uns im Folgenden konzentrieren.</p>
<p>Das Ziel von Regularisierung ist es, Overfitting zu vermeiden,
in dem die Komplexit√§t eines Modells reduziert wird.
Der Effekt von Regularisierung ist,
dass die Varianz der Modelle verringert wird und damit das Overfitting.
Der Preis ist, dass der Bias erh√∂ht wird,
aber oft geht die Rechnung auf, dass der Gewinn gr√∂√üer ist als der Verlust.</p>
<p>Im Kontext von linearen Modellen bedeutet das,
dass die Koeffizienten (<span class="math inline">\(\beta\)</span>s) im Betrag verringert werden durch Regularisierung,
also in Richtung Null ‚Äúgeschrumpft‚Äù werden.</p>
<p>Dem liegt die Idee zugrunde,
dass extreme Werte in den Koeffizienten vermutlich nicht ‚Äúecht‚Äù, sondern durch Rauschen
f√§lschlich vorgegaukelt werden.</p>
<p>Die bekanntesten Vertreter dieser Modellart sind <em>Ridge Regression</em>, <span class="math inline">\(L2\)</span>, das <em>Lasso</em>, <span class="math inline">\(L1\)</span>, sowie <em>Elastic Net</em>.</p>
</div>
<div id="√§hnliche-verfahren" class="section level3" number="11.3.2">
<h3>
<span class="header-section-number">11.3.2</span> √Ñhnliche Verfahren<a class="anchor" aria-label="anchor" href="#%C3%A4hnliche-verfahren"><i class="fas fa-link"></i></a>
</h3>
<p>Ein √§hnliches
Ziel wie der Regulaisierung liegt dem Pruning zugrunde,
dem nachtr√§glichen Beschneiden von Entscheidungsb√§umen.
In beiden F√§llen wird die Komplexit√§t des Modells verringert,
und damit die Varianz auf Kosten eines m√∂glichen Anstiegs der Verzerrung (Bias)
des Modells. Unterm Strich hofft man, dass der Gewinn die Kosten √ºbersteigt
und somit der Fit im Test-Sample besser wird.</p>
<p>Eine Andere Art der Regularisierung wird durch die Verwendung von Bayes-Modellen erreicht:
Setzt man einen konservativen Prior, etwa mit Mittelwert Null und kleiner Streuung,
so werden die Posteriori-Koeffizienten gegen Null hin geschrumpft werden.</p>
<p>Mit Mehrebenen-Modellen (Multi Level Models) l√§sst sich ein √§hnlicher Effekt erreichen.</p>
</div>
<div id="normale-regression-ols" class="section level3" number="11.3.3">
<h3>
<span class="header-section-number">11.3.3</span> Normale Regression (OLS)<a class="anchor" aria-label="anchor" href="#normale-regression-ols"><i class="fas fa-link"></i></a>
</h3>
<p>Man kann sich fragen, warum sollte man an der normalen Least-Square-Regression
(OLS: Ordinary Least Square) weiter herumbasteln wollen,
schlie√ülich garantiert das <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">Gauss-Markov-Theorem</a>, dass eine lineare Regression
den besten linearen unverzerrten Sch√§tzwert (BLUE, best linear unbiased estimator) stellt,
vorausgesetzt die Voraussetzungen der Regression sind erf√ºllt.</p>
<p>Ja, die Sch√§tzwerte (Vorhersagen) der Regression sind BLUE, sch√§tzen also den wahren
Wert korrekt und maximal pr√§zise. Das gilt (nat√ºrlich) nur, wenn die Voraussetzungen der Regression erf√ºllt
sind, also vor allem, dass die Beziehung auch linear-additiv ist.</p>
<p>Zur Erinnerung, mit OLS minimiert man man den quadrierten Fehler, <span class="math inline">\(RSS\)</span>, Residual Sum of Square:</p>
<p><span class="math display">\[RSS = \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)\]</span></p>
<p>Man sucht also diejenigen Koeffizientenwerte <span class="math inline">\(\beta\)</span> (Argumente der Loss-Funktion RSS),
die RSS minimieren:</p>
<p><span class="math display">\[\beta = \underset {\beta}{\operatorname {arg\,min(RSS)}}\]</span></p>
<p>Es handelt sich hier um Sch√§tzwerte, die meist mit dem H√ºtchen <span class="math inline">\(\hat{\beta}\)</span> ausgedr√ºckt werden,
hier aber zur einfacheren Notation weggelassen sind.</p>
<p>Abb. <a href="regularisierte-modelle.html#fig:ols">11.1</a> visualisiert die Optimierung mit OLS <a href="https://www.crumplab.com/rstatsforpsych/regression.html">Quelle</a>.
An <a href="https://www.crumplab.com/rstatsforpsych/regression.html">gleicher Stelle</a> findet sich
eine gute Darstellung zu den (mathematischen) Grundlagen der OLS-Regression.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ols"></span>
<img src="https://www.crumplab.com/rstatsforpsych/imgs/regression_squares.gif" alt="Visualisierung der Minimierung der RSS durch OLS" width="70%"><p class="caption">
Figure 11.1: Visualisierung der Minimierung der RSS durch OLS
</p>
</div>
<p>√úbrigens nennt man Funktionen, die man minimiert mit Hilfe von Methoden des maschinellen Lernens
mit dem Ziel die optimalen Koeffizienten (wie <span class="math inline">\(\beta\)</span>s) zu finden, auch <em>Loss Functions</em> (Kostenfunktion).</p>
<p>Das Problem der Regression ist, dass die sch√∂ne Eigenschaft BLUE nur im <em>Train-Sample</em>, <em>nicht</em> (notwendig)
im Test-Sample gilt.</p>
</div>
</div>
<div id="ridge-regression-l2" class="section level2" number="11.4">
<h2>
<span class="header-section-number">11.4</span> Ridge Regression, L2<a class="anchor" aria-label="anchor" href="#ridge-regression-l2"><i class="fas fa-link"></i></a>
</h2>
<div id="strafterm" class="section level3" number="11.4.1">
<h3>
<span class="header-section-number">11.4.1</span> Strafterm<a class="anchor" aria-label="anchor" href="#strafterm"><i class="fas fa-link"></i></a>
</h3>
<p>Ridge Regression ist sehr √§hnlich zum OLS-Algorithmus,
nur das ein ‚ÄúStrafterm aufgebrummt‚Äù wird, der <span class="math inline">\(RSS\)</span> erh√∂ht.</p>
<p>Der Gesamtterm, der optimiert wird, <span class="math inline">\(L_{L2}\)</span> (Loss Level 2) ist also
die Summe aus RSS und dem Strafterm:</p>
<p><span class="math display">\[L_{L2} = RSS + \text{Strafterm}\]</span></p>
<p>Der Strafterm ist so aufgebaut,
dass (im Absolutbetrag) gr√∂√üere Koeffizienten mehr zum Fehler beitragen,
also eine Funktion der (quadrierten) Summe der Absolutwerte der Koeffizienten:</p>
<p><span class="math display">\[\text{Strafterm} = \lambda \sum_{j=1}^p \beta_j^2\]</span></p>
<p>Man nennt den L2-Strafterm auch L2-Norm<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Streng genommen ist er eine Funktion der L2-Norm bzw. mit Lambda-Gewichtet und ohne die Wurzel, die zur Vektornorm geh√∂rt&lt;/p&gt;"><sup>8</sup></a>.</p>
<p>Dabei ist <span class="math inline">\(\lambda\)</span> (lambda) ein Tuningparameter,
der bestimmt, wie stark die Bestrafung ausf√§llt. Den Wert von <span class="math inline">\(\lambda\)</span> lassen wir durch
Tuning bestimmen, wobei <span class="math inline">\(\lambda \in \mathbb{R}^+\setminus\{0\}\)</span>.
Es gilt: Je gr√∂√üer lambda, desto st√§rker die Schrumpfung der Koeffizienten gegen Null,
da der gesamte zu minimierende Term, <span class="math inline">\(L_{L2}\)</span> entsprechend durch lambda vergr√∂√üert wird.</p>
<p>Der Begriff ‚ÄúL2‚Äù beschreibt dass es sich um eine quadrierte Normierung handelt.</p>
<p>Der Begriff ‚ÄúNorm‚Äù stammt aus der Vektoralgebra. Die L2-Norm eines Vektors <span class="math inline">\(||v||\)</span> mit <span class="math inline">\(k\)</span> Elementen ist so definiert <a href="https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261">Quelle</a>:</p>
<p><span class="math display">\[||v|| = \left(|{v_1}|^2+ |{v_2}|^2+ |{v_i}|^2+ \ldots + |{v_k}|^2 \right)^{1/2} \]</span>
wobei <span class="math inline">\(|{v_i}|\)</span> den Absolutwert (Betrag) meint de Elements <span class="math inline">\(v_i\)</span> meint.
Im Falle von reellen Zahlen und Quadrierung braucht es hier die Absolutfunktion nicht.</p>
<p>Im Falle von zwei Elementen vereinfacht sich obiger Ausdruck zu:</p>
<p><span class="math display">\[||v|| = \sqrt{\left({v_1}^2+ {v_2}^2\right)} \]</span></p>
<p>Das ist nichts anderes als Pythagoras‚Äô Gesetz im euklidischen Raum.</p>
<p>Der Effekt von <span class="math inline">\(\lambda \sum_{j=1}^p \beta_j^2\)</span> ist wie gesagt, dass
die Koeffizienten in Richtung Null geschrumpft werden. Wenn <span class="math inline">\(\lambda = 0\)</span>,
resultiert OLS.
Wenn <span class="math inline">\(\lambda \rightarrow \infty\)</span>, werden alle Koeffizienten auf Null gesch√§tzt werden,
Abb. <a href="regularisierte-modelle.html#fig:l2-shrink">11.2</a> verdeutlicht dies <span class="citation">(<a href="references.html#ref-islr" role="doc-biblioref">James et al. 2021</a>)</span>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:l2-shrink"></span>
<img src="img/6.4.png" alt="Links: Regressionskoeffizienten als Funktion von lambda. Rechts: L2-Norm der Ridge-Regression im Verh√§ltnis zur OLS-Regression" width="70%"><p class="caption">
Figure 11.2: Links: Regressionskoeffizienten als Funktion von lambda. Rechts: L2-Norm der Ridge-Regression im Verh√§ltnis zur OLS-Regression
</p>
</div>
</div>
<div id="standardisierung" class="section level3" number="11.4.2">
<h3>
<span class="header-section-number">11.4.2</span> Standardisierung<a class="anchor" aria-label="anchor" href="#standardisierung"><i class="fas fa-link"></i></a>
</h3>
<p>Die Straftermformel sagt uns, dass die Ridge-Regression abh√§ngig von der Skalierung
der Pr√§diktoren ist.
Daher sollten die Pr√§diktoren vor der Ridge-Regression zun√§chst auf <span class="math inline">\(sd=1\)</span> standardisiert werden.
Da wir <span class="math inline">\(\beta_0\)</span> nicht schrumpfen wollen, sondern nur die Koeffizienten der Pr√§diktoren
bietet es sich an, die Pr√§diktoren dazu noch zu zentieren.
Kurz: Die z-Transformation bietet sich als Vorverarbeitung zur Ridge-Regression an.</p>
</div>
</div>
<div id="lasso-l1" class="section level2" number="11.5">
<h2>
<span class="header-section-number">11.5</span> Lasso, L1<a class="anchor" aria-label="anchor" href="#lasso-l1"><i class="fas fa-link"></i></a>
</h2>
<div id="strafterm-1" class="section level3" number="11.5.1">
<h3>
<span class="header-section-number">11.5.1</span> Strafterm<a class="anchor" aria-label="anchor" href="#strafterm-1"><i class="fas fa-link"></i></a>
</h3>
<p>Der Strafterm in der ‚ÄúLasso-Variante‚Äù der regularisierten Regression lautet so:</p>
<p><span class="math display">\[\text{Strafterm} = \lambda \sum_{j=1}^p |\beta_j|,\]</span></p>
<p>ist also analog zur Ridge-Regression konzipiert.</p>
<p>Genau wie bei der L2-Norm-Regularisierung ist ein ‚Äúguter‚Äù Wert von lambda entscheidend.
Dieser Wert wird, wie bei der Ridge-Regression, durch Tuning bestimmt.</p>
<p>Der Unterschied ist, dass die L1-Norm (Absolutwerte) und nicht die L2-Norm (Quadratwerte)
verwendet werden.</p>
<p>Die L1-Norm eines Vektors ist definiert durch <span class="math inline">\(||\beta||_1 = \sum|\beta_j|\)</span>.</p>
</div>
<div id="variablenselektion" class="section level3" number="11.5.2">
<h3>
<span class="header-section-number">11.5.2</span> Variablenselektion<a class="anchor" aria-label="anchor" href="#variablenselektion"><i class="fas fa-link"></i></a>
</h3>
<p>Genau wie die Ridge-Regression f√ºhrt ein h√∂here lambda-Wert zu einer Regularisierung (Schrumpfung)
der Koeffizienten.
Im Unterschied zur Ridge-Regression hat das Lasso die Eigenschaft,
einzelne Parameter auf <em>exakt</em> Null zu schrumpfen und damit faktisch als Pr√§diktor auszuschlie√üen.
Anders gesagt hat das Lasso die praktische Eigenschaft,
Variablenselektion zu erm√∂glichen.</p>
<p>Abb. <a href="regularisierte-modelle.html#fig:lasso-l1">11.3</a> verdeutlicht den Effekt der Variablenselektion, vgl. <span class="citation">James et al. (<a href="references.html#ref-islr" role="doc-biblioref">2021</a>)</span>, Kap. 6.2.
Die Ellipsen um <span class="math inline">\(\hat{beta}\)</span> herum nent man Kontourlinien. Alle Punkte einer Kontourlinie
haben den gleiche RSS-Wert,
stehen also f√ºr eine gleichwertige OLS-L√∂sung.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:lasso-l1"></span>
<img src="img/6.6.png" alt="lambda in der Lasso-Regression" width="70%"><p class="caption">
Figure 11.3: lambda in der Lasso-Regression
</p>
</div>
<p>Warum erlaubt die L1-Norm Variablenselektion,
die L2-Norm aber nicht?
Abb. <a href="regularisierte-modelle.html#fig:l1l2">11.4</a> verdeutlicht den Unterschied zwischen L1- und L2-Norm.
Es ist eine Regression mit zwei Pr√§diktoren, also den zwei Koeffizienten <span class="math inline">\(\beta1, \beta_2\)</span> dargestellt.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:l1l2"></span>
<img src="img/6.6.png" alt="Verlauf des Strafterms bei der L1-Norm (links) und der L2-Norm (rechts)" width="70%"><p class="caption">
Figure 11.4: Verlauf des Strafterms bei der L1-Norm (links) und der L2-Norm (rechts)
</p>
</div>
<p>Betrachten wir zun√§chst das rechte Teilbild f√ºr die L2-Norm aus Abb. <a href="regularisierte-modelle.html#fig:l1l2">11.4</a>,
das in Abb. <a href="regularisierte-modelle.html#fig:l2-penalty">11.5</a> in den Fokus ger√ºckt wird <span class="citation">(<a href="references.html#ref-rhys" role="doc-biblioref">Rhys 2020</a>)</span>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:l2-penalty"></span>
<img src="img/l2-penalty.png" alt="Verlauf des Strafterms bei der L2-Norm" width="70%"><p class="caption">
Figure 11.5: Verlauf des Strafterms bei der L2-Norm
</p>
</div>
<p>Wenn lambda gleich Null ist, entspricht <span class="math inline">\(L_{L2}\)</span> genau der OLS-L√∂sung.
Vergr√∂√üert man lambda,
so liegt <span class="math inline">\(L_{L2}\)</span> dem Schnittpunkt des OLS-Kreises mit dem zugeh√∂rigen lambda-Kreis.
Wie man sieht, f√ºhrt eine Erh√∂hung von lambda zu einer Reduktion der Absolutwerte von <span class="math inline">\(\beta_1\)</span> und <span class="math inline">\(\beta_2\)</span>.
Allerdings werden, wie man im Diagramm sieht, auch bei hohen lambda-Werten die
Regressionskoeffizienten nicht exakt Null sein.</p>
<p>Warum l√§sst die L2-Norm f√ºr bestimmte lambda-Werte den charakteristischen Kreis entstehen?
Die Antwort ist, dass die L√∂sungen f√ºr <span class="math inline">\(\beta_1^2 + \beta_2^2=1\)</span> (mit <span class="math inline">\(\lambda=1\)</span>) graphisch als Kreis dargestellt werden k√∂nnen.</p>
<p>Anders ist die Situation bei der L1-Norm, dem Lasso, vgl. Abb. <a href="regularisierte-modelle.html#fig:l1-penalty">11.6</a> <span class="citation">(<a href="references.html#ref-rhys" role="doc-biblioref">Rhys 2020</a>)</span>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:l1-penalty"></span>
<img src="img/l1-penalty.png" alt="Verlauf des Strafterms bei der L1-Norm" width="70%"><p class="caption">
Figure 11.6: Verlauf des Strafterms bei der L1-Norm
</p>
</div>
<p>Eine Erh√∂hung von $ f√ºhrt aufgrund der charakteristischen Kontourlinie zu einem Schnittpunkt (von OLS-L√∂sung und lambda-Wert),
der - wenn lambda gro√ü genug ist, stets auf einer der beiden Achsen liegt,
also zu einer Nullsetzung des Parameters f√ºhrt.</p>
<p>Damit kann man argumentieren,
dass das Lasso implizit davon ausgeht,
dass einige Koeffizienten in Wirklichkeit <em>exakt Null</em> sind,
die L2-Norm aber nicht.</p>
</div>
</div>
<div id="l1-vs.-l2" class="section level2" number="11.6">
<h2>
<span class="header-section-number">11.6</span> L1 vs.¬†L2<a class="anchor" aria-label="anchor" href="#l1-vs.-l2"><i class="fas fa-link"></i></a>
</h2>
<div id="wer-ist-st√§rker" class="section level3" number="11.6.1">
<h3>
<span class="header-section-number">11.6.1</span> Wer ist st√§rker?<a class="anchor" aria-label="anchor" href="#wer-ist-st%C3%A4rker"><i class="fas fa-link"></i></a>
</h3>
<p>Man kann nicht sagen, dass die L1- oder die L2-Norm strikt besser sei.
Es kommt auf den Datensatz an.
Wenn man einen Datensatz hat, in dem es eingie wenige starke Pr√§diktoren gibt
und viele sehr schwache (oder exakt irrelevante) Pr√§diktoren gibt,
dann wird L1 tendenziell zu besseren Ergebnissen f√ºhren<span class="citation">(<a href="references.html#ref-islr" role="doc-biblioref">James et al. 2021</a>, S. 246)</span>.
Das Lasso hat noch den Vorteil der Einfachheit, da
weniger Pr√§diktoren im Modell verbleiben.</p>
<p>Ridge-Regression wird dann besser abschneiden (tendenziell),
wenn die Pr√§diktoren etwa alle gleich stark sind.</p>
</div>
<div id="elastic-net-als-kompromiss" class="section level3" number="11.6.2">
<h3>
<span class="header-section-number">11.6.2</span> Elastic Net als Kompromiss<a class="anchor" aria-label="anchor" href="#elastic-net-als-kompromiss"><i class="fas fa-link"></i></a>
</h3>
<p>Das Elastic Net (EN) ist ein Kompromiss zwischen L1- und L2-Norm.
<span class="math inline">\(\lambda\)</span> wird auf einen Wert zwischen 1 und 2 eingestellt;
auch hier wird der Wert f√ºr <span class="math inline">\(\lambda\)</span> wieder per Tuning gefunden.</p>
<p><span class="math display">\[L_{EN} = RSS + \lambda\left((1-\alpha))\cdot \text{L2-Strafterm} + \alpha \cdot  \text{L1-Strafterm}\right)\]</span></p>
<p><span class="math inline">\(\alpha\)</span> ist ein Tuningparameter, der einstellt, wie sehr wir uns Richtung L1- vs.¬†L2-Norm bewegen.
Damit wird sozusagen die ‚ÄúMischung‚Äù eingestellt (von L1- vs.¬†L2).</p>
<p>Spezialf√§lle:</p>
<ul>
<li>Wenn <span class="math inline">\(\alpha=0\)</span> resultiert die Ridge-Regression (L1-Strafterm wird Null)</li>
<li>Wenn <span class="math inline">\(\alpha=1\)</span> resultiert die Lasso-Regression (L2-Strafterm wird Null)</li>
</ul>
<!-- ## Aufgaben und Vertiefung -->
</div>
</div>
<div id="aufgaben-9" class="section level2" number="11.7">
<h2>
<span class="header-section-number">11.7</span> Aufgaben<a class="anchor" aria-label="anchor" href="#aufgaben-9"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li><a href="https://juliasilge.com/blog/lasso-the-office/">Fallstudie Serie The Office</a></li>
<li><a href="https://juliasilge.com/blog/nber-papers/">Fallstudie NBER Papers</a></li>
</ul>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="ensemble-lerner.html"><span class="header-section-number">10</span> Ensemble Lerner</a></div>
<div class="next"><a href="kaggle.html"><span class="header-section-number">12</span> Kaggle</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#regularisierte-modelle"><span class="header-section-number">11</span> Regularisierte Modelle</a></li>
<li>
<a class="nav-link" href="#lernsteuerung-8"><span class="header-section-number">11.1</span> Lernsteuerung</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#lernziele-9"><span class="header-section-number">11.1.1</span> Lernziele</a></li>
<li><a class="nav-link" href="#literatur-9"><span class="header-section-number">11.1.2</span> Literatur</a></li>
<li><a class="nav-link" href="#hinweise-4"><span class="header-section-number">11.1.3</span> Hinweise</a></li>
</ul>
</li>
<li><a class="nav-link" href="#vorbereitung-8"><span class="header-section-number">11.2</span> Vorbereitung</a></li>
<li>
<a class="nav-link" href="#regularisierung"><span class="header-section-number">11.3</span> Regularisierung</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#was-ist-regularisierung"><span class="header-section-number">11.3.1</span> Was ist Regularisierung?</a></li>
<li><a class="nav-link" href="#%C3%A4hnliche-verfahren"><span class="header-section-number">11.3.2</span> √Ñhnliche Verfahren</a></li>
<li><a class="nav-link" href="#normale-regression-ols"><span class="header-section-number">11.3.3</span> Normale Regression (OLS)</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#ridge-regression-l2"><span class="header-section-number">11.4</span> Ridge Regression, L2</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#strafterm"><span class="header-section-number">11.4.1</span> Strafterm</a></li>
<li><a class="nav-link" href="#standardisierung"><span class="header-section-number">11.4.2</span> Standardisierung</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#lasso-l1"><span class="header-section-number">11.5</span> Lasso, L1</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#strafterm-1"><span class="header-section-number">11.5.1</span> Strafterm</a></li>
<li><a class="nav-link" href="#variablenselektion"><span class="header-section-number">11.5.2</span> Variablenselektion</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#l1-vs.-l2"><span class="header-section-number">11.6</span> L1 vs.¬†L2</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#wer-ist-st%C3%A4rker"><span class="header-section-number">11.6.1</span> Wer ist st√§rker?</a></li>
<li><a class="nav-link" href="#elastic-net-als-kompromiss"><span class="header-section-number">11.6.2</span> Elastic Net als Kompromiss</a></li>
</ul>
</li>
<li><a class="nav-link" href="#aufgaben-9"><span class="header-section-number">11.7</span> Aufgaben</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/sebastiansauer/datascience1/blob/master/120-Regularisierte-Modelle.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/sebastiansauer/datascience1/edit/master/120-Regularisierte-Modelle.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>DataScience1</strong>: Grundlagen der Prognosemodellierung üîÆüß∞" was written by Sebastian Sauer. It was last built on 2022-06-09 16:24:59.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
