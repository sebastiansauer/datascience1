# Resampling und Tuning



```{r echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```











## Lernsteuerung


### Lernziele
- Sie verstehen den Nutzen von Resampling und Tuning im maschinellen Nutzen.
- Sie k√∂nnen Methoden des Resampling und Tunings mit Hilfe von Tidymodels anwenden.
    
### Vorbereitung
- Lesen Sie die Literatur.

###  Literatur
- Rhys, Kap. 3
- TMWR, Kap. 10, 12


### Daten


```{r}
data(ames)
```



### Ben√∂tigte R-Pakete

```{r libs-resampling-tuning, echo = TRUE}
library(tidyverse)
library(tidymodels)
library(tictoc)  # Rechenzeit messen, optional
```


## √úberblick


### Train- und Test-Sample vervielfacht

In @sec-overfit haben wir gelernt, dass ein Modell in einem zweiten Datensatz auf seine Modellg√ºte hin √ºberpr√ºft werden und sollte und nicht in dem (ersten) Datensatz, in dem die Modellparameter berechnet wurden.

In diesem Kapitel werden wir wir von einem Modell *mehrere Varianten* berechnen,
daher ben√∂tigen wir f√ºr jeden dieser Varianten oder "Modellkandidaten" eine eigene Train-Test-Aufteilung. 
Zur Klarheit der Begrifflichkeiten nennt man die resultierenden Teile in dem Fall *Analyse- und Assessment-Sample*, s. @fig-analys-assess-test dargestellt aus 
Kap. 10.2 in @silge_tidy_2022 ([Quelle](https://www.tmwr.org/resampling.html)).


![Die Aufteilung der Daten im Falle mehrerer Modellkandidaten](img/resampling.svg){#ig-analys-assess-test width="50%"}


### Standardablauf

Ein Standardablauf des maschinellen Lernens ist in @fig-process1 dargestellt.

```{mermaid}
%%| label: fig-process1
%%| fig-cap: "Standardablauf des maschinellen Lernens mit Tuning und Resampling (S: Sample bzw. Stichprobe)"

flowchart TD
   
Gesamtdatensatz --> Split[In Train- und Test aufteilen]
subgraph Fit[F√ºr jeden Modellkandidaten i]
  subgraph Kand[Modellkandidat i]
  F[Fitte im Train-S] --> T[Teste im Assessment-S]
  end
end
Split --> Fit
Fit --> Best[Bestimmte besten Kandidaten]
Best --> lastFit[Fitte ihn im ganzen Train-S]
lastFit --> test[Teste im Test-S]
```








## tidymodels

Betrachten wir dieses Konzept an einem konkreten Beispiel mit Tidymodels.

#


### Abh√§ngige Variable transformieren

:::{callout-note}
M√∂chte man eine abh√§ngige Variable transformieren, 
so sollte das *au√üerhalb* des Rezepts passieren,
da das "Backen" nicht auf die `outcome`-Variable ausgef√ºhrt wird.$\square$
:::


Aus der [Dokumentation von `step_scale`](https://recipes.tidymodels.org/reference/step_scale.html):

>   skip - A logical. Should the step be skipped when the recipe is baked by bake()? While all operations are baked when prep() is run, some operations may not be able to be conducted on new data (e.g. processing the outcome variable(s)). Care should be taken when using skip = TRUE as it may affect the computations for subsequent operations.


```{r}
ames <-
  ames %>% 
  mutate(Sale_Price = log(Sale_Price, base = 10))
```


[Hier](https://www.tmwr.org/recipes.html#skip-equals-true) finden Sie eine Antwort,
warum tidymodels sich weigert, Informationen √ºber die AV vom Train- in das Test-Sample zu transferieren.




## Datensatz aufteilen


```{r ames-split, echo = TRUE}
set.seed(4595)
data_split <- initial_split(ames, strata = "Sale_Price")

ames_train <- training(data_split)
ames_test <- testing(data_split)
```


### Rezept, Modell und Workflow definieren

In gewohnter Weise definieren wir zun√§chst den Workflow
mit einem kNN-Modell.

```{r ames-wf, echo = TRUE}
ames_rec <-
  recipe(Sale_Price ~ Lot_Area + Fireplaces + Longitude + Latitude,
         data = ames_train) %>%
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_impute_median(all_predictors())

knn_model <-
  nearest_neighbor(
    mode = "regression"
  ) 

ames_wflow1 <-
  workflow() %>%
  add_recipe(ames_rec) %>%
  add_model(knn_model)
```

Mit dem Rezept kNN-Modell ist noch *nicht* *berechnet,
es ist nur ein "Rezept" erstellt.





## Resampling


Vergleichen Sie die drei F√§lle, die sich in der Nutzung von Train- und Test-Sample unterscheiden:

1. Wir fitten ein Klassifikationsmodell in einer Stichprobe, sagen die Y-Werte dieser Stichprobe "vorher". Wir finden eine Gesamtgenauigkeit von 80%.
2. Wir fitten ein Klassifikationsmodell in einem Teil der urspr√ºnglichen Stichprobe (Train-Sample) und sagen Y-die Werte im verbleibenden Teil der urspr√ºnglichen Stichprobe vorher (Test-Sample). Wir finden eine Gesamtgenauigkeit von 70%.
3. Wir wiederholen Fall 2 noch drei Mal mit jeweils anderer Zuweisung der F√§lle zum Train- bzw. zum Test-Sample. Wir finden insgesamt folgende Werte an Gesamtgenauigkeit: 70%, 70%, 65%, 75%.


Welchen der drei F√§lle finden Sie am sinnvollsten? Warum?


:::{.callout-note}
Verschiedene (zuf√§llige) Aufteilung eines Datensatzes in Train- und Test-Sample k√∂nnen zu verschiedenen Modellg√ºten f√ºhren. So k√∂nnten im Train-Sample durch eine bestimmte Zufallsaufteilung relativ viele (oder wenige) schwer zu klassifizierende F√§lle zusammen kommen.$\square$
:::

Fall Nummer 3 bezeichnet man als *Kruezvalidierung*.


## Illustration des Resampling

*Resampling* stellt einen Oberbegriff dar; *Kreuzvalidierung* ist ein Unterbegriff dazu.
Es gibt noch andere Arten des Resampling, etwa *Bootstrapping* oder *Leave-One-Out-Cross-Validation* (LOOCV).

Im Folgenden ist nur die Kreuzvalidierung dargestellt,
da es eines der wichtigsten und vielleicht das am h√§ufigsten verwendete Verfahren des Resampling ist.
In vielen Quellen finden sich Erl√§uterungen anderer Verfahren dargestellt,
etwa in @silge_tidy_2022, @islr oder @rhys.




### Einfache v-fache Kreuzvalidierung

@fig-resampling illustriert die zuf√§llige Aufteilung von $n=10$ F√§llen der Originalstrichprobe auf eine Train- bzw. Test-Stichpobe. 
Man spricht von *Kreuzvalidierung* (cross validation, CV).

In diesem Fall wurden 70% der ($n=10$) F√§lle der Train-Stichprobe zugewiesen (der Rest der Test-Stichprobe);
ein willk√ºrlicher, aber nicht un√ºblicher Anteil.
Diese Aufteilung wurde $v=3$ Mal vorgenommen,
es resultieren drei "Resampling-Stichproben", die
manchmal auch als "Faltungen" bezeichnet werden.


```{r fig.cap = "Resampling: Eine Stichprobe wird mehrfach (hier 3 Mal) zu 70% in ein Train- und zu 30% in die Test-Stichprobe aufgeteilt", echo = FALSE, messagen = FALSE, out.width="100%"}
#| label: fig-resampling
source("children/resampling-plot.R")
illustrate_resampling()
```




@modar stellt das Resampling so dar (S. 259), s. @fig-cvmodar.

```{r cvmodar, echo = FALSE, fig.cap = "Kreuzvalidierung, Aufteilung in Train- vs. Testsample"}
#| label: fig-cvmodar
knitr::include_graphics("img/crossval.png")
```


Der Gesamtfehler der Vorhersage (die Modellg√ºte) wird als *Mittelwert der Vorhersagefehler* in den einzelnen Faltungen berechnet.

Warum ist die Vorhersage besser,
wenn man mehrere Faltungen, mehrere Sch√§tzungen f√ºr $y$ also, vornimmt?

Der Grund ist das Gesetz der gro√üen Zahl,
nachdem sich eine Sch√§tzung in Mittelwert und Variabilit√§t stabilisiert mit steigendem
Stichprobenumfang,
dem wahren Mittelwert also pr√§ziser sch√§tzt.^[Bei Normalverteilungen klappt das gut
bei randlastigen Verteilungen leider nicht mehr [@fattails].]
Mit mehr Faltungen n√§hern wir uns also einem "wahren" Mittelwert der Modellg√ºte (und sonstiger Kennzahlen) n√§her an.


H√§ufig werden $v=10$ Faltungen verwendet,
was sich empirisch als guter Kompromiss von Rechenaufwand und Fehlerreduktion herausgestellt hat.

Die Nachteile der Kreuzvalidierung sind:

1. Die Rechenzeit steigt (in der Regel) etwa proportional zur Anzahl der $v$ Faltungen.
2. Da die Train-Stichprobe kleiner ist (als bei der einfachen Train-Test-Aufteilung), wird die Sch√§tzung der Modellkoeffizienten ungenauer sein und damit die Modellg√ºte geringer.

Insgesamt √ºberwiegen zumeist die Vorteiler eines Resamplings (wie eine Kreuzvalidierung) im Vergleich zu einfachen Train-Test-Aufteilung.




### Wiederholte Kreuzvalidierung


Die $r$-fach wiederholte Kreuzvalidierung wiederholte die einfache Kreuzvalidierung mehrfach (n√§mlich $r=4$ mal),
@modar stellt das Resampling so dar (S. 259), s. @fig-cvrep.


```{r cvrep, echo = FALSE, fig.cap = "Wiederholte Kreuzvalidierung"}
#| label: fig-cvrep
knitr::include_graphics("img/crossval_repeated.png")
```

Die wiederholte Kreuzvalidierung reduziert den Standardfehler der Vorhersagen.

@silge_tidy_2022 zeigen die Verringerung des Sch√§tzfehlers als Funktion der $r$ Wiederholungen dar,
s. @fig-repcvred.


```{r repcvred, echo = FALSE, fig.cap = "Reduktion des Sch√§tzfehlers als Funktion der r Wiederhoulugen der Kreuzvalidierung"}
#| label: fig-repcvred
knitr::include_graphics("https://www.tmwr.org/figures/variance-reduction-1.png")
```


Warum ist die Wiederholung der Kreuzvalidierung n√ºtzlich?

Die Kreuvalidierung liefert einen Sch√§tzwert der Modellparameter,
die wahren Modellparameter werden also anhand einer Stichprobe von $n=1$ gesch√§tzt.
Mit h√∂herem Stichprobenumfang kann diese Sch√§tzung nat√ºrlich pr√§zisiert werden.

Da jede Stichprobenverteilung bei $n \rightarrow \infty$ normalverteilt ist - 
ein zentrales Theorem der Statistik, der *Zentrale Grenzwertsatz* (Central Limit Theorem) - 
kann man hoffen, dass sich eine bestimmte Stichprobenverteilung bei kleinerem $n$ ebenfalls ann√§hernd
normalverteilt^[Das klappt bei randlastigen Verteilungen nicht]. 
Dann sind die Quantile bekannt und man kann die Streuung der Sch√§tzers, 
${\sigma }_{\bar {x}}$, z.B. f√ºr den Mittelwert,
einfach sch√§tzen:

$${\displaystyle {\sigma }_{\bar {x}}\ ={\frac {\sigma }{\sqrt {n}}}}$$


### Resampling passiert im Train-Sample

Wichtig zu beachten ist, dass
die Resampling nur im Train-Sample stattfindet.
Das Test-Sample bleibt unanger√ºhrt.
Dieser Sachverhalt ist in @fig-analys-assess-test, aus @silge_tidy_2022, illustriert.





Wie in @fig-initialsplit dargestellt,
wird das Modell im *Analyse-Sample* berechnet (gefittet),
und im *Assessment-Sample* auf Modellg√ºte hin √ºberpr√ºft.

Die letztliche Modellg√ºte ist dann die Zusammenfassung (Mittelwert) der einzelnen Resamples.


### Andere Illustrationen


Es gibt eine Reihe vergleichbarer Illustrationen in anderen B√ºchern:

- [Timbers, Campbell & Lee, 2022, Kap. 6](https://datasciencebook.ca/img/cv.png)
- [Silge & Kuhn, 2022, 10.1](https://datasciencebook.ca/img/cv.png)
- [Silge & Kuhn, 2022, 10.2](https://www.tmwr.org/premade/three-CV.svg)
- [Silge & Kuhn, 2022, 10.3](https://www.tmwr.org/premade/three-CV-iter.svg)
- James, Witten, hastie & Tishirani, 2021, 5.3



## Gesetz der gro√üen Zahl

Nach dem *Gesetz der gro√üen Zahl* (Law of Large Numbers) sollte sich der Mittelwert einer gro√üen Stichprobe 
dem theoretischen Mittelwert der zugrundeliegenden Verteilung (Population, datengeneriender Prozess) 
sehr nahe kommen.

$$\displaystyle \lim _{n\to \infty }\sum _{i=1}^{n}{\frac {X_{i}}{n}}={\overline {X}}$$

David Salazar visualisiert das folgenderma√üen in [diesem Post](https://david-salazar.github.io/2020/04/17/fat-vs-thin-does-lln-work/) seines lesenswerten [Blogs](https://david-salazar.github.io/), s. @fig-lln).

```{r  lln,  fig.cap = "Gesetz der gro√üen Zahl", fig.width=7}
#| label: fig-lln
#| echo: false
# source: https://david-salazar.github.io/2020/04/17/fat-vs-thin-does-lln-work/
samples <- 1000

thin <- rnorm(samples, sd = 20)

cumulative_mean <- function(numbers) {
    x <- seq(1, length(numbers))
    cum_mean <- cumsum(numbers)/x 
    cum_mean
}

thin_cum_mean <- cumulative_mean(thin)

thin_cum_mean %>%
  tibble(running_mean = .) %>% 
  add_rownames(var = 'number_samples') %>% 
  mutate(number_samples = as.double(number_samples)) %>% 
  arrange(number_samples) %>% 
  ggplot(aes(x = number_samples, y = running_mean)) +
    geom_line(color = 'dodgerblue4') +
    geom_hline(yintercept = 0, linetype = 2, color = 'red') +
  hrbrthemes::theme_ipsum_rc(grid = 'Y') +
  scale_x_continuous(labels = scales::comma) +
  labs(x = "Stichprobengr√∂√üe",
       title = "Gesetz der gro√üen Zahl", 
       subtitle = "Kumulierter Mittelwert aus einer Normalverteilung mit sd=20")
```

Wie man sieht, n√§hert sich der empirische Mittelwert (also in der Stichprobe)
immer mehr dem theoretischen Mittelwert, 0, an.

Achtung: Bei randlastigen Verteilungen darf man dieses sch√∂ne, wohlerzogene Verhalten nicht erwarten [@fattails].








## CV in tidymodels

### CV definieren

So kann man eine *einfache* v-fache Kreuzvalidierung in Tidymodels auszeichnen^[$v=10$ in der Voreinstellung]:

```{r}
set.seed(2453)
ames_folds <- vfold_cv(ames_train, strata = "Sale_Price")
ames_folds
```

Werfen wir einen Blick in die Spalte `splits`, erste Zeile:

```{r}
ames_folds %>% pluck(1, 1)
```


M√∂chte man die Defaults vpn `vfold_cv` wissen, schaut man in der Hilfe nach: `?vfold_cv`:


`vfold_cv(data, v = 10, repeats = 1, strata = NULL, breaks = 4, pool = 0.1, ...)` 


Probieren wir $v=10$ und $r=10$:

```{r}
ames_folds_rep <- vfold_cv(ames_train, 
                           strata = "Sale_Price", 
                           v = 10,
                           repeats = 10)
ames_folds_rep
```


### Resamples fitten


Hat unser Computer mehrere Rechenkerne, dann k√∂nnen wir diese nutzen und die Berechnungen beschleunigen.
Im Standard wird sonst nur ein Kern verwendet.

```{r}
mycores <- parallel::detectCores(logical = FALSE)
mycores
```

Auf Unix/MacOC-Systemen kann man dann die Anzahl der parallen Kerne so einstellen^[In Windows gibt es andere Wege.]:

```{r}
library(doMC)
registerDoMC(cores = mycores)
```



So, und jetzt fitten wir die Resamples und betrachten die Modellg√ºte in den Resamples:


```{r}
tic()
ames_resamples_fit <- 
  ames_wflow1 %>% 
  fit_resamples(ames_folds)
toc()
```


```{r}
 ames_resamples_fit %>%
  collect_metrics()
```



Nat√ºrlich interessiert uns prim√§r die Modellg√ºte im *Test*-Sample:


```{r}
final_ames1 <-
  last_fit(ames_wflow1, data_split)
```

```{r}
final_ames1 %>% 
  collect_metrics()
```



### Streuung in der Modellg√ºte zwischen den Resamples

Betrachten wir die Streuungen der Modellg√ºte (RSMSE) in der 10-fachen, nicht wiederholten Kreuzvalidierung, s. @fig-cv-rmse.


```{r}
rmse_resamples <-
  ames_resamples_fit %>% 
  unnest(.metrics) %>% 
  filter(.metric == "rmse") %>% 
  select(id, .metric, .estimate) 

p_rmse_cv1r <- 
  rmse_resamples %>% 
  ggplot(aes(x = id, y = .estimate)) +
  geom_point() +
  geom_hline(yintercept = mean(rmse_resamples$.estimate), linetype = "dashed") +
  labs(caption = paste0("SD: ", round(sd(rmse_resamples$.estimate), 4)),
       y = "Modellg√ºte: RMSE")
```


Jetzt wiederholen wir die Kreuzvalidierung $r=5$ mal und betrachten wieder die Streuung der Modellg√ºte.
Da wir $r$ mal so viele Modelle berechnen, ben√∂tigen wir - wenn nur ein einzelnen Rechenkern benutzt wird - $r$ mal so viel  Rechenzeit^[theoretisch].


Zuerst berechnen wir die wiederholte Kreuzvalidierung, das kann etwas dauern:

```{r}
#| cache: true
tic()
ames_resamples_fit_rep <- 
  ames_wflow1 %>% 
  fit_resamples(ames_folds_rep)
toc()
```


Dann analysieren wir die Ergebnisse:

```{r}
mse_resamples_rep <-
  ames_resamples_fit_rep %>% 
  unnest(.metrics) %>% 
  filter(.metric == "rmse") %>% 
  select(id, id2, .estimate)

mse_resamples_rep_summ <-
  mse_resamples_rep %>% 
  group_by(id) %>% 
  summarise(
    .estimate_rep_mean = mean(.estimate),
    .estimate_rep_sd = sd(.estimate),
    .estimate = .estimate_rep_mean) 

p_rmse_cv5r <- 
mse_resamples_rep %>% 
  ggplot(aes(x = id)) +
  geom_point(alpha = .7, aes(y = .estimate)) + 
  geom_errorbar(data = mse_resamples_rep_summ,
                aes(ymin = .estimate - .estimate_rep_sd,
                    ymax = .estimate + .estimate_rep_sd)) +
  geom_hline(yintercept = mean(mse_resamples_rep$.estimate), linetype = "dashed") +
  labs(caption = paste0("SD: ", round(sd(mse_resamples_rep$.estimate), 4)),
       y = "Modellg√ºte: RMSE")
```



Wie man sieht, streuen die $v=10$ Faltungen in ihre Modellg√ºte, s. @fig-cv-rmse.
Durch das Wiederholen *sinkt*, nach dem Gesetz der gro√üen Zahl, die Streuung.
Mit geringerer Streuung sch√§tzen wir den wahren Fehler pr√§ziser.


```{r}
#| label: fig-cv-rmse
# fig-cap: "Streuung der Modellg√ºte (RMSE) bei nicht wiederholter und wiederholter CV"
#| layout-ncol: 2
#| fig-subcap: 
#|   - "Streuung der Modellg√ºte (RMSE) mit v=10 r=1 CV"
#|   - "Streuung der Modellg√ºte (RMSE) mit v=10 r=5 CV"
#| echo: false



p_rmse_cv1r
p_rmse_cv5r
```



Puh, das war recht theoretisch. Ja, das Leben des Data-Scientisten ist nicht immer einfach,
findet auch Grumpy Cat, s. @fig-grumpy1.

![Grumpy Cat und die Schwierigkeiten im Leben des Data-Scientisten](img/7h9qvw.jpg){#fig-grumpy1 width="50%"}

[Quelle: imgflip](https://imgflip.com/i/7h9qvw)


## Tuning


:::{#def-tuning}

>   üë®‚Äçüè´ Hey ChatGPT: Definiere Tuning mit maschinellen Lernen am Beispiel von knn und dem Parameter k.

>   ü§ñ KNN (K-nearest neighbors) ist ein Machine-Learning-Algorithmus, bei dem die Vorhersage eines neuen Datenpunkts auf der Basis der k n√§chsten Nachbarn im Trainingsdatensatz getroffen wird. Der Parameter k gibt an, wie viele Nachbarn zur Vorhersage verwendet werden sollen. Eine wichtige Aufgabe beim Tuning von KNN besteht darin, den optimalen Wert f√ºr den Parameter k zu finden.
Wenn k zu klein gew√§hlt wird, kann dies zu einer √úberanpassung an den Trainingsdatensatz f√ºhren, w√§hrend bei zu gro√ü gew√§hltem k das Modell unteranpassung aufweisen kann. Durch Tuning kann ein optimales k gefunden werden, das zu einer besseren Leistung des Modells auf einem Validierungsdatensatz f√ºhrt.
Es gibt verschiedene Methoden zum Tuning des k-Parameters. Eine M√∂glichkeit besteht darin, ein Rastersuchverfahren durchzuf√ºhren, bei dem eine vordefinierte Menge von k-Werten getestet wird, um dasjenige zu finden, das die beste Leistung auf einem Validierungsdatensatz aufweist. Eine andere M√∂glichkeit besteht darin, eine zuf√§llige Suche durchzuf√ºhren, indem man zuf√§llig verschiedene k-Werte ausw√§hlt und deren Leistung bewertet.
Das Ziel des Tunings ist es, den besten Wert f√ºr den Parameter k zu finden, um das KNN-Modell zu verbessern und die Vorhersagegenauigkeit auf einem Testdatensatz zu maximieren. Das Tuning sollte jedoch immer mit einem unabh√§ngigen Testdatensatz durchgef√ºhrt werden, um sicherzustellen, dass das Modell nicht √ºberangepasst wird und auf neuen Daten gut funktioniert.^[https://chat.openai.com/chat, 2023-04-06]$\square$
:::





### Tuning auszeichnen

In der Modellspezifikation des Modells k√∂nnen wir mit `tune()` *auszeichnen*,
welche Parameter wir tunen m√∂chten. 




```{r}
knn_model2 <-
  nearest_neighbor(
    mode = "regression",
    neighbors = tune()  # Wir tunen den Parameter "neighbors"
  ) 
```


Wir k√∂nnen dem Tuningparameter auch einen Namen (ID/Label) geben, z.B. "K":

```{r}
knn_model2a <-
  nearest_neighbor(
    mode = "regression",
    neighbors = tune("K")
  ) 
```


:::{.callout-note}
Tidymodels trennt generell das Spezifizieren vom Evaluieren:
Erst definieren wir ein Rezept und ein Modell, dann fitten wir es.
Das gilt auch f√ºr das Tunen: Erst weisen wir Parameter zum Tunen aus,
dann w√§hlen wir Tuningparameter und tunen.$\square
:::

### Tuning durchf√ºhren

Wir betrachten zwei zentrale Arten von Tuning: Grid Search vs. Iterative Search.


Im K-N√§chste-Nachbarn-Modell (Klassifikation) ist der vorhergesagt Wert, $\hat{y}$ f√ºr eine neue Beobachtung $x_0$ der Modus der $K$ n√§chsten Nachbarn.


Die Wahl von $K$ hat einen zentralen Einfluss auf die Vorhersagen und damit auf die Vorhersageg√ºte.
Allerdings wird $K$ nicht vom Modell gesch√§tzt.
Es liegt an den Nutzi,
diesen Wert zu w√§hlen.

Parameter dieser Art (die von den Nutzi zu bestimmen sind, nicht vom Algorithmus),
nennt man *Tuningparameter*.


Abbildung @fig-nnoverfit aus [diesem Kapitel von](https://www.tmwr.org/tuning.html#overfitting-bad) @silge_tidy_2022 stellt exemplarisch an den Vorhersagen eines neuronalen Netzes dar,
welchen gro√üen Einfluss die Wahl des Werts eines Tuningparameters auf die 
Vorhersagen eines Modells haben.


```{r nnoverfit, echo = FALSE, fig.cap = "Overfitting als Funktion der Modellparameter und insofern als Problem de Wahl der Tuningparameter"}
#| label: fig-nnoverfit
knitr::include_graphics("https://www.tmwr.org/figures/two-class-boundaries-1.png")
```


Aber wie w√§hlt man "gute" Werte der Tuningparater?
Zwei Ans√§tze, grob gesprochen, bieten sich an.

1. *Grid Search:* Probiere viele Werte aus und schaue, welcher der beste ist. Dabei musst du hoffen, dass du die Werte erwischt, die nicht nur im Train-, sondern auch im Test-Sample gut funktionieren werden.

2. *Iterative Search:* Wenn du einen Wert eines Tuningparameters hast, nutze diesen, um intelligenter einen neuen Wert eines Tuningparameters zu finden.


Der Unterschied beider Ans√§tze ist in @silge_tidy_2022 wie in @fig-tuning1 dargestellt.


```{r tuning1, echo=FALSE, fig.cap = "Links: Grid Search. Rechts: Iterative Search2"}
#| label: fig-tuning1 
knitr::include_graphics("https://www.tmwr.org/figures/tuning-strategies-1.png")
```


In `tidymodels` kann man mit `tune()` angeben, dass man einen bestimmten Parameter tunen m√∂chte. 
`tidymodels` f√ºhrt das dann ohne weiteres Federlesens f√ºr uns durch.





M√∂chte man wissen, 
welche und wie viele Tuningparameter tidymodels in einem Modell ber√ºcksichtigt,
kann man so aufrufen^[Alle Tuningparameter eines Modells sieht man so: `knn_model2 %>% 
  extract_parameter_set_dials()`]:

```{r}
knn_model2 %>% 
  extract_parameter_dials("neighbors")
```





Die Ausgabe informiert uns,
dass es nur einen Tuningparameter gibt in diesem Modell und
dass der Name (Label, ID) des Tuningparameters "K" ist.
Au√üerdem erfahren wir, dass der Tuningparmaeter die Anzahl der zu ber√ºcksichtigen Nachbarn bezeichent.
Der Tuningparameter ist numerisch; das sieht man an `nparam[+]`.
Tidymodels w√§hlt einen Range von 1 bis 15 Nachbarn.

:::{.callout-note}
Praktisch! Oft ist es nicht leicht zu wissen, was ein gutes Spektrum an Werten eines Tuningparameters ist. `tidymodels` bzw. `dials` macht es einfach: 
Es gibt uns einen Bereich plausibler Tuningwerte vor.$\square$
:::



Aktualisieren wir  unseren Workflow entsprechend:

```{r}
ames_wflow2 <-
  ames_wflow1 %>% 
  update_model(knn_model2)

ames_wflow2
```



Wir k√∂nnen auch Einfluss nehmen und angeben,
dass die Grenzen des Wertebereichs zwischen 1 und 50 liegen soll 
(f√ºr den Tuningparameter `neighbors`):


```{r}
knn_model3 <-
nearest_neighbor(
  mode = "classification",
  neighbors = tune(id = "K") %>% set_range(c(1, 50))
)
```

Den Wertebereich eines Pr√§diktors kann man aber auch mit `search_grid` bestimmen.


<!-- ```{r ames-update} -->
<!-- ames_set <- -->
<!--   extract_parameter_set_dials(ames_wflow) %>% -->
<!--   update(K = neighbors(c(1, 50))) -->

<!-- ames_set -->
<!-- ``` -->


<!-- ### Datenabh√§ngige Tuningparameter -->

<!-- Manche Tuningparameter kann man nur bestimmen, -->
<!-- wenn man den Datensatz kennt. -->
<!-- So ist die Anzahl der Pr√§diktoren, `mtry` in einem Random-Forest-Modell  -->
<!-- sinnvollerweise als Funktion der Pr√§diktorenzahl zu w√§hlen. -->
<!-- Der Workflow kennt aber den Datensatz nicht. -->
<!-- Daher muss der Workflow noch "finalisiert" oder "aktualisiert" werden, -->
<!-- um den Wertebereich (Unter- und Obergrenze) eines Tuningparameters zu bestimmen. -->






<!-- Wenn wir im Rezept aber z.B. die Anzahl der Pr√§diktoren ver√§ndert haben, -->
<!-- m√∂chten wir die Grenzen des Wertebereichs f√ºr `mtry` (oder andere Tuningparameter) vielleicht nicht h√§ndisch, "hartverdrahtet" selber bestimmen, -->
<!-- sondern lieber den Computer anweisen, und sinngem√§√ü sagen: -->
<!-- "Warte mal mit der Bestimmung der Werte der Tuningparameter, -->
<!-- bis du den Datensatz bzw. dessen Dimensionen kennst. Merk dir,  -->
<!-- dass du, wenn du den Datensatz kennst, die Werte des Tuningparameter noch √§ndern musst. Und tu das dann auch." Dazu sp√§ter mehr. -->


<!-- ```{r ames-finalize} -->
<!-- ames_set <- -->
<!--   workflow() %>%  -->
<!--   add_model(knn_model2) %>%  -->
<!--   add_recipe(ames_rec) %>%  -->
<!--   extract_parameter_set_dials() %>%  -->
<!--   finalize(ames_train) -->

<!-- ames_set -->
<!-- ``` -->


### Modelle mit Tuning berechnen

Nachdem wir die Tuningwerte bestimmt haben, 
k√∂nnen wir jetzt das Modell berechnen:
F√ºr jeden Wert des Tuningparameters wird ein Modell berechnet:

```{r ames-tune-grid}
ames_folds <- vfold_cv(ames_train, strata = "Sale_Price")

ames_grid_search <-
  tune_grid(
    object = ames_wflow2,
    resamples = ames_folds,
    grid = 5,  # 5 Tuningwerte insgesamt
    tuning_control = control_grid(save_workflow = TRUE)
  )
ames_grid_search
```

Im Default berechnet `tiymodels` 10 Kandidatenmodelle.

Mit `control_grid` kann man beim Tuning einige Schalter umlegen,
hier haben wir den Workflow an das Ergebnisobjekt angeh√§ngt.

Die Spalte `.metrics` beinhaltet die Modellg√ºte f√ºr jedes Kandidatenmodell.

```{r}
ames_grid_search %>% 
  collect_metrics()
```

Die Modellg√ºte in Abh√§ngigkeit der Tuningwerte k√∂nnen wir uns einfach visualisieren lassen:

```{r}
autoplot(ames_grid_search)
```


Auf Basis dieser Ergebnisse k√∂nnte es Sinn machen, 
noch gr√∂√üere Werte f√ºr $K$ zu √ºberpr√ºfen.

Tidymodels bietet verschiedene Optionen, 
um ein "Gitter" (`grid`) an Werten von einem oder (in vielen Modellen) mehreren Tuningparametern zu durchsuchen.
Eine M√∂glichkeit ist, ein Gitter mit *regelm√§√üigen* Abst√§nden der Werte zu erstellen, z.B. mit 5 Auspr√§gungen pro Tuningparameter:

```{r}
grid1 <- 
  grid_regular(
    neighbors(range = c(5L, 30L)),
    levels = 5
    )
grid1
```


```{r}
ames_grid_search2 <-
  tune_grid(
    object = ames_wflow2,
    resamples = ames_folds,  
    grid = grid1,
    control = control_grid(save_workflow = TRUE)
  )
ames_grid_search2
```


### Vorhersage im Test-Sample

Welches Modellkandidat war jetzt am besten?

```{r}
show_best(ames_grid_search2)
```



W√§hlen wir jetzt mal den besten Modellkandidaten aus (im Sinne des Optimierungskriteriusms, RMSE) und fitten damit das *gesamte* Train-Sample:

```{r}
fit_best_train <- fit_best(ames_grid_search2)
fit_best_train
```


Und mit den dann resultierenden Modellkoeffizienten sagen
wir das TestSample vorher:




```{r ames-last-fit}
fit_best_train %>% 
  predict(new_data = ames_test) %>% 
  head()
```

```{r}
fit_best_train %>% 
  predict(new_data = ames_test) %>% 
  head()
```


Mit `augment` kann man die Vorhersagen eines Modells zum Test-Datensatz hinzuf√ºgen:


```{r}
ames_test <- 
  augment(fit_best_train, new_data = ames_test)
```




Holen wir uns die Modellg√ºte:

```{r}
rsq(data = ames_test,
    truth = Sale_Price,
    estimate = .pred)
```








## Aufgaben
1. [tidymodels-penguins01](https://datenwerk.netlify.app/posts/tidymodels-penguins1/tidymodels-penguins01.html)
1. [tidymodels-penguins02](https://datenwerk.netlify.app/posts/tidymodels-penguins02/tidymodels-penguins02.html)
1. [tidymodels-penguins03](https://datenwerk.netlify.app/posts/tidymodels-penguins03/tidymodels-penguins03.html)
1. [tidymodels-penguins04](https://datenwerk.netlify.app/posts/tidymodels-penguins04/tidymodels-penguins04.html)
1. [tidymodels-penguins05](https://datenwerk.netlify.app/posts/tidymodels-penguins05/tidymodels-penguins05.html)


## Fallstudien

In @sec-fallstudien finden Sie eine ausf√ºhrliche Liste an Fallstudien.


- Arbeiten Sie sich so gut als m√∂glich durch [diese Analyse zum Verlauf von Covid-F√§llen](https://github.com/sebastiansauer/covid-icu)
- [Fallstudie zur Modellierung einer logististischen Regression mit tidymodels](https://onezero.blog/modelling-binary-logistic-regression-using-tidymodels-library-in-r-part-1/)
- [Fallstudie zu Vulkanausbr√ºchen (Resampling and kein Tuning)](https://juliasilge.com/blog/multinomial-volcano-eruptions/)
- [Fallstudie Himalaya (Resampling and kein Tuning)](https://juliasilge.com/blog/himalayan-climbing/)
- [Fallstudie Serie The Office: Lasso tunen](https://juliasilge.com/blog/lasso-the-office/)
- [Fallstudie B√§ume in San Francisco: Random Forest tunen](https://dev.to/juliasilge/tuning-random-forest-hyperparameters-in-r-with-tidytuesday-trees-data-4ilh)



##  Vertiefung

[Fields arranged by purity, xkcd 435](https://xkcd.com/435/)




