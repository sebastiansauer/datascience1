{
  "hash": "d8f7da87935f00e72979c15fed724dc6",
  "result": {
    "markdown": "## Feature Engineering\n\n\n\n\n\n\n## Lernsteuerung\n\n\n###  Lernziele\n\n- Sie wissen, dass \"Feature Engineering\" letztlich das Gleiche ist wie \"Datenaufbereitung\".\n- Sie können gängige Methoden des Feature Engineeering in Gründzügen erläutern.\n- Sie können gängige Methoden des Feature Engineeering in R für das Modellieren anwenden.\n\n\n\n### Hinweise\n\nDieses Kapitel basiert auf @kuhn_feature_2020.\n\n\n\n### R-Pakete\n\n\nIn diesem Kapitel werden folgende R-Pakete benötigt:\n\n\n::: {.cell hash='125-feature-engineeering_cache/html/unnamed-chunk-1_6d65f93e6338287c325519e3dcf476f9'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tictoc)  # Rechenzeit messen\n```\n:::\n\n\n\n\n\n## Transformation nominaler Variablen\n\n\nViele Lernalgorithmen verkraften keine nominalen Variablen.\nBeispiele sind lineare Modelle. \nWichtige Ausnahmen sind aber Entscheidungsbäume und Naive-Bayes-Modelle;\ndiese Lernalgorithmen können mit mit nominalskalierten Werten umgehen.\n\nBei vielen Lernalgorithmen ist es (oft) nötig, nominale Variablen zu dummysieren.\nIn Tidymodels ist dies mit `step_dummy` möglich.\n\nBevor man dummysiert, kann es sinnvoll sein, Faktorstufen, die nur im Test-Sample aber nicht im Train-Sample vorkomme,\nabzufangen.\nIn Tidymodels gibt es dazu `step_novel`.\n\nLiegt eine große Zahl an seltenen Faktorstufen vor, kann es Sinn machen mittels `step_other` diese Faktorstufen zusammenzufassen zu einer \"Other-Kategorie\" (auch dieser Schritt ist vor dem Dummysieren durchzuführen).\nÄhnliches gilt für den Fall von Variablen (fast) ohne Varianz. \n\nAlternativ kann man eine Methode verwenden, die man als Effekt- oder Likelihood-Enkodierung bezeichnet.\nHier wird für jede Faktorstufe ihr Betagewicht als neuer Prädiktorwert kodiert.\n\n\n\n## Transformation numerischer Variablen\n\n\nEine häufige Malaise mit numerischen Variablen ist Schiefe.\nSchiefe Variablen lassen sich häufig schlecht vorhersagen oder zum Vorhersagen nutzen.\nDer Grund ist, dass bei schiefen Variablen (per Definition) nur wenig Fälle einen großen Wertebereichs des Prädiktors bevölkern.\nDaher tut sich ein Modell schwer.\nTransformationen zu mehr Symmetrie (oder Normalverteilung) hin können daher nützlich sein.\nEin klassisches Beispiel einer solchen Transformation ist die Log-Transformation.\nAllerdings können bei der Log-Transformation nur Werte größer Null verarbeitet werden.\nEine Verallgemeinerung der Log-Transformation ist die Box-Cox-Transformation.\nEine Alternative zur Box-Cox-Transformation ist die Yeo-Johson-Transformation,\ndie den Vorteil hat, auch Werte die Null sind oder kleiner verarbeiten zu können.\n\nEin anderes Problem können hoch korrelierte (kollineare) Variablen darstellen.\nAbhilfe kann schaffen, eine von zwei hoch korrelierten Variablen zu entfernen.\nIn Tidymodels hilft hier `step_corr`.\n\n\n## Umgang mit fehlenden Werten\n\nViele bekannte Lernalgorithmen verkraften keine fehlenden Werte, z.B.\nglmnet, neuronale Netze oder SVM.\nManche können aber damit umgehen, etwa CART-Modelle (eine Implementierung von Entscheidungsbäumen, die in Tidymodels implementiert ist^[https://parsnip.tidymodels.org/reference/details_decision_tree_rpart.html]).\n\n\nWie so oft gibt es hier kein einfaches Standardrezept.\nInsbesondere hängt das zu wählende Vorgehen davon ab,\nwarum die Werte fehlen:\nIst es rein zufällig (MCAR) oder nicht (MAR, NMAR)?\n\nEin einfaches Vorgehen wäre, alle Fälle mit einem oder mehr fehlenden Werten zu löschen.\nNatürlich kann das schnell teuer mit Blick auf die Größe des Train-Samples werden.\nAuch das Löschen von Prädiktoren mit fehlenden Werten kann leicht unangenehm werden.\n\nAlternativ kann man fehlende Werte ersetzen (imputieren).\nMöchte man mit kNN imputieren, so kann man `step_impute_knn` imputieren,\ndabei ist aber Gowers Metrik zu bevorzugen.\n\n\n\n### Ausreisser entfernen\n\nEs kann oft sinnvoll sein, Ausreisser zu entfernen,\netwas wenn diese zuviel Einfluss haben auf die Parameter.\nIm Rahmen von Tidymodels gibt es ein spezialisiertes Paket [`tidy.outliers](https://brunocarlin.github.io/tidy.outliers/articles/integration_tidymodels.html),\ndas das Entfernen von Extremwerten im Rahmens eines Rezept unterstützt.\n\n\n## Feature Selection (Prädiktorenwahl)\n\nEinige Modelle haben intrinsische Feature-Selection-Fähigkeiten,\netwa LASSO.\nEin sehr einfaches Ansatz zur Auswahl von Prädiktoren ist es, \neinfache Korrelationen der Prädiktoren (ggf. dummyisiert) mit der Zielvariablen zu berechnen.\n\nWichtig für eine gute Auswahl von Prädiktoren ist,\ndass der Auswahlprozess im Resampling-Prozess integriert ist,\num Overfitting zu vermeiden.\n\nDas Paket [`recipesselector`](https://stevenpawley.github.io/recipeselectors/) stellt dafür eine Infrastruktur (innerhalb des Tidymodels-Rahmen) bereit.^[Hier ist ein Video dazu: <https://www.youtube.com/watch?v=1AKug0tgux8>.]\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}