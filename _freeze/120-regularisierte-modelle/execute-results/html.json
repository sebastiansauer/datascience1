{
  "hash": "ca62b874459da28f716d8ff9d541beab",
  "result": {
    "markdown": "# Regularisierte Modelle\n\n\n\n\n\n\n## Lernsteuerung\n\n\n###  Lernziele\n- \"Sie können Algorithmen für regularisierte lineare Modell erklären, d.h. Lasso- und Ridge-Regression\"\n- \"Sie wissen, anhand welche Tuningparamter man Overfitting bei diesen Algorithmen begrenzen kann\"\n- \"Sie können diese Verfahren in R berechnen\"\n  \n###  Literatur\n- \"Rhys, Kap. 11\"\n\n\n### Hinweise\n\n\n\"Rhys und ISLR sind eine gute Quelle zum Einstieg in das Thema.\n\n\n\n### R-Pakete\n\n\nIn diesem Kapitel werden folgende R-Pakete benötigt:\n\n\n::: {.cell hash='120-regularisierte-modelle_cache/html/libs-regul-mods_dc91e882af0b8b24beef04eef9f5b9c5'}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tictoc)  # Zeitmessung\n```\n:::\n\n\n\n## Regularisierung\n\n### Was ist Regularisierung?\n\n\nRegularisieren verweist auf \"regulär\"; \nlaut [Duden]() bedeutet das Wort so viel wie \"den Regeln, Bestimmungen, \nVorschriften entsprechend; vorschriftsmäßig, ordnungsgemäß, richtig\" oder \"üblich\".\n\nIm Englischen spricht man auch von \"penalized models\", \"bestrafte Modell\" und von \"shrinkage\",\nvon \"Schrumpfung\" im Zusammenhang mit dieer Art von Modellen.\n\nRegularisierung ist ein Meta-Algorithmus, also ein Verfahren, \nwas als zweiter Schritt \"auf\" verschiedene \nModelle angewendet werden kann - zumeist aber auf lineare Modelle, worauf \nwir uns im Folgenden konzentrieren.\n\nDas Ziel von Regularisierung ist es, Overfitting zu vermeiden,\nin dem die Komplexität eines Modells reduziert wird.\nDer Effekt von Regularisierung ist, \ndass die Varianz der Modelle verringert wird und damit das Overfitting.\nDer Preis ist, dass der Bias erhöht wird,\naber oft geht die Rechnung auf, dass der Gewinn größer ist als der Verlust.\n\nIm Kontext von linearen Modellen bedeutet das,\ndass die Koeffizienten ($\\beta$s) im Betrag verringert werden durch Regularisierung,\nalso in Richtung Null \"geschrumpft\" werden.\n\nDem liegt die Idee zugrunde,\ndass extreme Werte in den Koeffizienten vermutlich nicht \"echt\", sondern durch Rauschen\nfälschlich vorgegaukelt werden.\n\nDie bekanntesten Vertreter dieser Modellart sind *Ridge Regression*, $L2$, das *Lasso*, $L1$, sowie *Elastic Net*.\n\n\n### Ähnliche Verfahren\n\nEin ähnliches\nZiel wie der Regulaisierung liegt dem Pruning zugrunde, \ndem nachträglichen Beschneiden von Entscheidungsbäumen.\nIn beiden Fällen wird die Komplexität des Modells verringert,\nund damit die Varianz auf Kosten eines möglichen Anstiegs der Verzerrung (Bias)\ndes Modells. Unterm Strich hofft man, dass der Gewinn die Kosten übersteigt\nund somit der Fit im Test-Sample besser wird.\n\n\n\nEine Andere Art der Regularisierung wird durch die Verwendung von Bayes-Modellen erreicht:\nSetzt man einen konservativen Prior, etwa mit Mittelwert Null und kleiner Streuung,\nso werden die Posteriori-Koeffizienten gegen Null hin geschrumpft werden.\n\nMit Mehrebenen-Modellen (Multi Level Models) lässt sich ein ähnlicher Effekt erreichen.\n\n\n\n\n\n\n### Normale Regression (OLS)\n\n\n\nMan kann sich fragen, warum sollte man an der normalen Least-Square-Regression \n(OLS: Ordinary Least Square) weiter herumbasteln wollen,\nschließlich garantiert das [Gauss-Markov-Theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem), dass eine lineare Regression\nden besten linearen unverzerrten Schätzwert (BLUE, best linear unbiased estimator) stellt,\nvorausgesetzt die Voraussetzungen der Regression sind erfüllt.\n\nJa, die Schätzwerte (Vorhersagen) der Regression sind BLUE, schätzen also den wahren\nWert korrekt und maximal präzise. Das gilt (natürlich) nur, wenn die Voraussetzungen der Regression erfüllt\nsind, also vor allem, dass die Beziehung auch linear-additiv ist.\n\n\n\nZur Erinnerung, mit OLS minimiert man  man den quadrierten Fehler, $RSS$, Residual Sum of Square:\n\n\n$$RSS = \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)$$\n\nMan sucht also diejenigen Koeffizientenwerte $\\beta$ (Argumente der Loss-Funktion RSS),\ndie RSS minimieren:\n\n\n$$\\beta = \\underset {\\beta}{\\operatorname {arg\\,min(RSS)}}$$\n\n\nEs handelt sich hier um Schätzwerte, die meist mit dem Hütchen $\\hat{\\beta}$ ausgedrückt werden,\nhier aber zur einfacheren Notation weggelassen sind.\n\nAbb. @fig-ols visualisiert die Optimierung mit OLS [Quelle](https://www.crumplab.com/rstatsforpsych/regression.html).\nAn [gleicher Stelle](https://www.crumplab.com/rstatsforpsych/regression.html) findet sich\neine gute Darstellung zu den (mathematischen) Grundlagen der OLS-Regression.\n\n![Visualisierung der Minimierung der RSS durch OLS](img/regression_squares (1).gif){#fig-ols}\n\n\n\nÜbrigens nennt man Funktionen, die man minimiert mit Hilfe von Methoden des maschinellen Lernens \nmit dem Ziel die optimalen Koeffizienten (wie $\\beta$s) zu finden, auch *Loss Functions* (Kostenfunktion).\n\nDas Problem der Regression ist, dass die schöne Eigenschaft BLUE nur im *Train-Sample*, *nicht* (notwendig)\nim Test-Sample gilt.\n\n\n\n\n## Ridge Regression, L2\n\n\n### Strafterm\n\nRidge Regression ist sehr ähnlich zum OLS-Algorithmus,\nnur das ein \"Strafterm aufgebrummt\" wird, der $RSS$ erhöht.\n\n\nDer Gesamtterm, der optimiert wird, $L_{L2}$ (Loss Level 2) ist also\ndie Summe aus RSS und dem Strafterm:\n\n\n$$L_{L2} = RSS + \\text{Strafterm}$$\n\nDer Strafterm ist so aufgebaut,\ndass (im Absolutbetrag) größere Koeffizienten mehr zum Fehler beitragen,\nalso eine Funktion der (quadrierten) Summe der Absolutwerte der Koeffizienten:\n\n\n$$\\text{Strafterm} = \\lambda \\sum_{j=1}^p \\beta_j^2$$\n\n\nMan nennt den L2-Strafterm auch L2-Norm^[Streng genommen ist er eine Funktion der L2-Norm bzw. mit Lambda-Gewichtet und ohne die Wurzel, die zur Vektornorm gehört].\n\nDabei ist $\\lambda$ (lambda) ein Tuningparameter, \nder bestimmt, wie stark die Bestrafung ausfällt. Den Wert von $\\lambda$ lassen wir durch\nTuning bestimmen, wobei $\\lambda \\in \\mathbb{R}^+\\setminus\\{0\\}$. \nEs gilt: Je größer lambda, desto stärker die Schrumpfung der Koeffizienten gegen Null,\nda der gesamte zu minimierende Term, $L_{L2}$ entsprechend durch lambda vergrößert wird.\n\nDer Begriff \"L2\" beschreibt dass es sich um eine quadrierte Normierung handelt.\n\nDer Begriff \"Norm\" stammt aus der Vektoralgebra. Die L2-Norm eines Vektors $||v||$ mit $k$ Elementen ist so definiert [Quelle](https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261):\n\n$$||v|| = \\left(|{v_1}|^2+ |{v_2}|^2+ |{v_i}|^2+ \\ldots + |{v_k}|^2 \\right)^{1/2} $$\nwobei $|{v_i}|$ den Absolutwert (Betrag) meint de Elements $v_i$ meint.\nIm Falle von reellen Zahlen und Quadrierung braucht es hier die Absolutfunktion nicht.\n\nIm Falle von zwei Elementen vereinfacht sich obiger Ausdruck zu:\n\n\n$$||v|| = \\sqrt{\\left({v_1}^2+ {v_2}^2\\right)} $$\n\nDas ist nichts anderes als Pythagoras' Gesetz im euklidischen Raum.\n\nDer Effekt von $\\lambda \\sum_{j=1}^p \\beta_j^2$ ist wie gesagt, dass\ndie Koeffizienten in Richtung Null geschrumpft werden. Wenn $\\lambda = 0$,\nresultiert OLS. \nWenn $\\lambda \\rightarrow \\infty$, werden alle Koeffizienten auf Null geschätzt werden,\nAbb. @fig-l2-shrink verdeutlicht dies [@islr].\n\n\n\n![Links: Regressionskoeffizienten als Funktion von lambda. Rechts: L2-Norm der Ridge-Regression im Verhältnis zur OLS-Regression](img/6.4.png){#fig-l2-shrink}\n\n\n\n### Standardisierung\n\n\nDie Straftermformel sagt uns, dass die Ridge-Regression abhängig von der Skalierung\nder Prädiktoren ist.\nDaher sollten die Prädiktoren vor der Ridge-Regression zunächst auf $sd=1$ standardisiert werden.\nDa wir $\\beta_0$ nicht schrumpfen wollen, sondern nur die Koeffizienten der Prädiktoren\nbietet es sich an, die Prädiktoren dazu noch zu zentieren.\nKurz: Die z-Transformation bietet sich als Vorverarbeitung zur Ridge-Regression an.\n\n\n\n\n\n\n## Lasso, L1\n\n\n### Strafterm\n\nDer Strafterm in der \"Lasso-Variante\" der regularisierten Regression lautet so:\n\n$$\\text{Strafterm} = \\lambda \\sum_{j=1}^p |\\beta_j|,$$\n\n\nist also analog zur Ridge-Regression konzipiert.\n\nGenau wie bei der L2-Norm-Regularisierung ist ein \"guter\" Wert von lambda entscheidend.\nDieser Wert wird, wie bei der Ridge-Regression, durch Tuning bestimmt.\n\nDer Unterschied ist, dass die L1-Norm (Absolutwerte) und nicht die L2-Norm (Quadratwerte)\nverwendet werden.\n\nDie L1-Norm eines Vektors ist definiert durch $||\\beta||_1 = \\sum|\\beta_j|$.\n\n\n### Variablenselektion\n\nGenau wie die Ridge-Regression führt ein höhere lambda-Wert zu einer Regularisierung (Schrumpfung)\nder Koeffizienten.\nIm Unterschied zur Ridge-Regression hat das Lasso die Eigenschaft,\neinzelne Parameter auf *exakt* Null zu schrumpfen und damit faktisch als Prädiktor auszuschließen.\nAnders gesagt hat das Lasso die praktische Eigenschaft,\nVariablenselektion zu ermöglichen.\n\nAbb. @fig-lasso-l1 verdeutlicht den Effekt der Variablenselektion, vgl. @islr, Kap. 6.2.\nDie Ellipsen um $\\hat{beta}$ herum nent man Kontourlinien. Alle Punkte einer Kontourlinie\nhaben den gleiche RSS-Wert,\nstehen also für eine gleichwertige OLS-Lösung.\n\n\n![lambda in der Lasso-Regression](img/6.6.png){#fig-lasso-l1}\n\n\n\n\n\nWarum erlaubt die L1-Norm Variablenselektion,\ndie L2-Norm aber nicht?\nAbb. @fig-l1l2 verdeutlicht den Unterschied zwischen L1- und L2-Norm.\nEs ist eine Regression mit zwei Prädiktoren, also den zwei Koeffizienten $\\beta1, \\beta_2$ dargestellt.\n\n\n![Verlauf des Strafterms bei der L1-Norm (links) und der L2-Norm (rechts)](img/6.6.png){#fig-l1l2}\n\n\nBetrachten wir zunächst das rechte Teilbild für die L2-Norm aus Abb.  @fig-l1l2,\ndas in Abb. @fig-l2-penalty in den Fokus gerückt wird [@rhys].\n\n\n![Verlauf des Strafterms bei der L1-Norm (links) und der L2-Norm (rechts)](img/l2-penalty.png){#fig-l2-penalty}\n\n\nWenn lambda gleich Null ist, entspricht $L_{L2}$ genau der OLS-Lösung.\nVergrößert man lambda,\nso liegt $L_{L2}$ dem Schnittpunkt des OLS-Kreises mit dem zugehörigen lambda-Kreis.\nWie man sieht, führt eine Erhöhung von lambda zu einer Reduktion der Absolutwerte von $\\beta_1$ und $\\beta_2$.\nAllerdings werden, wie man im Diagramm sieht, auch bei hohen lambda-Werten die \nRegressionskoeffizienten nicht exakt Null sein.\n\nWarum lässt die L2-Norm für bestimmte lambda-Werte den charakteristischen Kreis entstehen?\nDie Antwort ist, dass die Lösungen für $\\beta_1^2 + \\beta_2^2=1$ (mit $\\lambda=1$) graphisch als Kreis dargestellt werden können.\n\n\n\n\n\n\nAnders ist die Situation bei der L1-Norm, dem Lasso, vgl. Abb. @fig-l1-penalty [@rhys].\n\n![Verlauf des Strafterms bei der L1-Norm](img/l1-penalty.png){#fig-l1-penalty}\n\n\n\n\nEine Erhöhung von \\lambda$ führt aufgrund der charakteristischen Kontourlinie zu einem Schnittpunkt (von OLS-Lösung und lambda-Wert), \nder - wenn lambda groß genug ist, stets auf einer der beiden Achsen liegt,\nalso zu einer Nullsetzung des Parameters führt.\n\nDamit kann man argumentieren,\ndass das Lasso implizit davon ausgeht,\ndass einige Koeffizienten in Wirklichkeit *exakt Null* sind,\ndie L2-Norm aber nicht.\n\n\n## L1 vs. L2\n\n\n### Wer ist stärker?\n\nMan kann nicht sagen, dass die L1- oder die L2-Norm strikt besser sei.\nEs kommt auf den Datensatz an.\nWenn man einen Datensatz hat, in dem es eingie wenige starke Prädiktoren gibt\nund viele sehr schwache (oder exakt irrelevante) Prädiktoren gibt,\ndann wird L1 tendenziell zu besseren Ergebnissen führen[@islr, S. 246].\nDas Lasso hat noch den Vorteil der Einfachheit, da\nweniger Prädiktoren im Modell verbleiben.\n\nRidge-Regression wird dann besser abschneiden (tendenziell),\nwenn die Prädiktoren etwa alle gleich stark sind.\n\n\n\n### Elastic Net als Kompromiss\n\nDas Elastic Net (EN) ist ein Kompromiss zwischen L1- und L2-Norm.\n$\\lambda$ wird auf einen Wert zwischen 1 und 2 eingestellt;\nauch hier wird der Wert für $\\lambda$ wieder per Tuning gefunden.\n\n$$L_{EN} = RSS + \\lambda\\left((1-\\alpha))\\cdot \\text{L2-Strafterm} + \\alpha \\cdot  \\text{L1-Strafterm}\\right)$$\n\n$\\alpha$ ist ein Tuningparameter, der einstellt, wie sehr wir uns Richtung L1- vs. L2-Norm bewegen.\nDamit wird sozusagen die \"Mischung\" eingestellt (von L1- vs. L2).\n\nSpezialfälle:\n\n- Wenn $\\alpha=0$ resultiert die Ridge-Regression (L1-Strafterm wird Null)\n- Wenn $\\alpha=1$ resultiert die Lasso-Regression (L2-Strafterm wird Null)\n\n\n\n\n## Aufgaben\n\n\n- \"[Fallstudie Serie The Office](https://juliasilge.com/blog/lasso-the-office/)\"\n- \"[Fallstudie NBER Papers](https://juliasilge.com/blog/nber-papers/)\"\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}